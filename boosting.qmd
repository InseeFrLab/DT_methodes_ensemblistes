---
title: "Le boosting"
author: |
  [Olivier Meslin](https://github.com/oliviermeslin)
  [Mélina Hillion](https://github.com/melinahillion)
format:
  typst:
    toc: true
    section-numbering: 1.1.1
    bibliography: references.bib
---

```{=typst} 
#import "@preview/mitex:0.2.4": *
#set math.equation(
  numbering: "(1)",
  supplement: none
  )
```


$
\DeclareMathOperator*{\argmin}{arg\,min}
\makeatletter
\let\ams@underbrace=\underbrace
\def\underbrace#1_#2{%
  \setbox0=\hbox{$\displaystyle#1$}%
  \ams@underbrace{#1}_{\parbox[t]{\the\wd0}{#2}}%
}
\makeatother
$

Le _boosting_ est une approche ensembliste qui ne traite pas les modèles de base séparément les uns des autres.

## Le *boosting*

### Introduction

Le fondement théorique du _boosting_ est un article de de 1990 (@shapire1990strength) qui a démontré théoriquement que, sous certaines conditions, il est possible de transformer un modèle peu performant en un modèle très performant. Plus précisément, un modèle ayant un pouvoir prédictif arbitrairement élevé  (appelé _strong learner_) peut être construit en combinant des modèles simples dont les prédictions ne sont que légèrement meilleures que le hasard (appelé _weak learners_). Le _boosting_ est donc une méthode qui combine une approche ensembliste reposant sur un grand nombre de modèles simples avec un entraînement séquentiel: chaque modèle simple (souvent des arbres de décision peu profonds) tâche d'améliorer la prédiction globale en corrigeant les erreurs des prédictions précédentes à chaque étape. 

S'il existe plusieurs variantes, les algorithmes de _boosting_ suivent une même logique :

- Un premier modèle simple et peu performant est entraîné sur les données.
- Un deuxième modèle est entraîné de façon à corriger les erreurs du premier modèle (par exemple en pondérant davantage les observations mal prédites);
- Ce processus est répété en ajoutant des modèles simples, chaque modèle corrigeant les erreurs commises par l'ensemble des modèles précédents;
- Tous ces modèles sont finalement combinés (souvent par une somme pondérée) pour obtenir un modèle complexe et performant.

D'un point de vue technique, il s'agit d'une **modélisation additive par étapes** qui revient à décomposer l'entraînement d'un modèle très complexe en une __séquence d'entraînements de petits modèles__, procédure beaucoup plus simple. 

METTRE ICI UNE FIGURE EN UNE DIMENSION, avec des points et des modèles en escalier qui s'affinent.


### Une première implémentation du _boosting_ : Adaboost

Dans les années 1990, de nombreux travaux ont tâché de proposer des implémentations du _boosting_ (@breiman1998rejoinder, @grove1998boosting) et ont comparé les mérites des différentes approches. Le modèle Adaboost (Adaptive Boosting, @freund1997decision) est la plus connue de ces implémentations pionnières. Adaboost pondère les erreurs commises à chaque itération en donnant plus d'importance aux observations mal prédites. Voici une esquisse du fonctionnement d'AdaBoost:

- Un premier modèle simple est entraîné sur un jeu d'entraînement dans lequel toutes les observations ont le même poids.
- A l'issue de cette première itération, les observations mal prédites reçoivent une pondération plus élevé que les observations bien prédites, et un deuxième modèle est entraîné sur ce jeu d'entraînement pondéré.
- Cette procédure est répétée en ajoutant de nouveaux modèles et en adaptant les pondérations. 
- Le modèle final est obtenu par une moyenne pondérée des modèles.


### L'invention du _boosting boosting_ : la _Gradient Boosting Machine_ de Friedman

A REPRENDRE DES SLIDES DE LA FORMATION BOOSTING

Imaginons qu'on veuille entraîner le modèle suivant:

$F\left(\mathbf{x}\right) = \sum_{m=1}^M \beta_m f\left(\mathbf{x}, \mathbf{\theta}_m\right)$


$\hat{F}$ est caractérisée par les paramètres $\{\beta_m, \mathbf{\theta}_m\}_{m=1}^{M}$ tels que
$\argmin_{\{\beta_m, \mathbf{\theta}_m\}_{m=1}^{M}} \sum_{i=1}^N L\left(y_i, \sum_{m=1}^M \beta_m f\left(\mathbf{x}_i, \mathbf{\theta}_m\right)\right)$


C'est un problème très compliqué dès que $M$ est élevé!


<!--
```{=typst} 
#import "@preview/algo:0.3.3": algo, i, d, comment, code

#algo(
  line-numbers: false,
  title: Modélisation additive par étape
)[
  if $n < 0$:#i\        // use #i to indent the following lines
    return null#d\      // use #d to dedent the following lines
  if $n = 0$ or $n = 1$:#i #comment[you can also]\
    return $n$#d #comment[add comments!]\
  return #smallcaps("Fib")$(n-1) +$ #smallcaps("Fib")$(n-2)$
]    
```
-->


1.  Commencer par $f_0\left(\mathbf{x}\right) = y_0$.
2.  Pour $m = 1, \dots, M:$
    (a) Entraîner le $m$-ième modèle:
    $$\left(\hat{\beta}_m, \hat{\theta}_m\right) = \argmin_{\beta, \mathbf{\theta}} \sum_{i=1}^N L\left(y_i, f_{m-1}\left(\mathbf{x}_i\right) + \beta b\left(\mathbf{x}_i, \mathbf{\theta}\right)\right)$$
    (b) Définir $f_m\left(\mathbf{x}\right) = f_{m-1}\left(\mathbf{x}\right) + \hat{\beta}_m b\left(\mathbf{x}_i, \mathbf{\hat{\theta}_m}\right)$

## La mécanique du _gradient boosting_

Depuis la publication de @friedman2001greedy, la méthode de _gradient boosting_ a connu de multiples développements et raffinements, parmi lesquels XGBoost (@chen2016xgboost), LightGBM (@ke2017lightgbm) et CatBoost (@prokhorenkova2018catboost). S'il existe quelques différences entre ces implémentations, elles partagent néanmoins la même mécanique d'ensemble, que la section qui suit va présenter en détail en s'appuyant sur l'implémentation proposée par XBGoost.[^Cette partie reprend la structure et les notations de la partie 2 de @chen2016xgboost.]

Choses importantes à mettre en avant:

- Toute la mécanique est indépendante de la fonction de perte choisie. En particulier, elle est applicable indifféremment à des problèmes de classification et de régression.
- Les poids sont calculés par une formule explicite, ce qui rend les calculs extrêmement rapides.
- Comment on interprète le gradient et la hessienne: cas avec une fonction de perte quadratique.
- Le boosting est fait pour overfitter; contrairement aux RF, il n'y a pas de limite à l'overfitting. Donc lutter contre le surapprentissage est un élément particulièrement important de l'usage des algorithmes de boosting.





### Le modèle à entraîner

On veut entraîner un modèle comprenant $K$ arbres de régression ou de classification: 

$$\hat{y}_{i} = \phi\left(\mathbf{x}_i\right) = \sum_{k=1}^{K} f_k\left(\mathbf{x}_i\right), f_k \epsilon \mathcal{F}$$

Chaque arbre $F$ est défini par trois paramètres: sa structure (une fonction $q: \mathbb{R}^m \rightarrow \{1, \dots, T\}$ qui à un vecteur d'inputs $x$ de dimension $m$ associe une feuille terminale de l'arbre), son nombre de feuilles terminales $T$ et les valeurs figurant sur ses feuilles terminales $\mathbf{w}\in \mathbb{R}^T$ (appelées poids ou _weights_).

Le modèle est entraîné avec une __fonction-objectif__ constituée d'une __fonction de perte__ $l$ et d'une __fonction de régularisation__ $\Omega$. La fonction de perte mesure la distance entre la prédiction $hat(y)$ et la vraie valeur $y$ et présente généralement les propriétés suivante: elle est convexe et dérivable deux fois, et atteint son minimum lorsque $hat(y) = y$. La fonction de régularisation pénalise la complexité du modèle. Dans le cas présent, elle pénalise les arbres avec un grand nombre de feuilles ($T$ élevé) et les arbres avec des poids élevés ($w_j$ élevés en valeur absolue).

```{=typst} 
#mitex(`
$$\mathcal{L}(\phi) = \underbrace{\sum_i l(\hat{y}_{i}, y_{i})}_{\substack{\text{Perte sur les} \\ \text{observations}}} + \underbrace{\sum_k \Omega(f_{k})}_{\substack{\text{Fonction de} \\ \text{régularisation}}}\hspace{5mm}\text{avec}\hspace{5mm}\Omega(f) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_i^2$$

`)
```

<!-- ```{=typst} 
#mitex(`
$$\mathcal{L}(\phi) = \underbrace{\sum_i l(\hat{y}_{i}, y_{i})}_{\substack{\text{Perte sur les} \\ \text{observations}}} + \underbrace{\sum_k \Omega(f_{k})}_{\substack{\text{Fonction de} \\ \text{régularisation}}}\hspace{5mm}\text{avec}\hspace{5mm}\Omega(f) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_i^2$$ + \alpha \sum_{j=1}^T |w_i|$$
`)
``` -->

<!-- $\mathcal{F} = \{f(\mathbf{x}) = w_{q(\mathbf{x})}\}(q:\mathbb{R}^m \rightarrow T,w \in \mathbb{R}^T)$ -->

<!-- $\hat{y}_i^{(0)} = 0\\ \hat{y}_i^{(1)} = f_1(x_i) = \hat{y}_i^{(0)} + f_1(x_i)\\ \hat{y}_i^{(2)} = f_1(x_i) + f_2(x_i)= \hat{y}_i^{(1)} + f_2(x_i)\\ ....\\ \hat{y}_i^{(t)} = \sum_{k=1}^t f_k(x_i)= \hat{y}_i^{(t-1)} + f_t(x_i)$ -->



### Isoler le $k$-ième arbre

La fonction-objectif introduite précédemment est très complexe et ne peut être utilisée directement pour entraîner le modèle, car il faudrait entraîner tous les arbres en même temps. On va donc reformuler donc cette fonction objectif de façon à isoler le $t$-ième arbre, qui pourra ensuite être entraîné seul, une fois que les $t-1$ arbres précédents auront été entraînés. Pour cela, on note $\hat{y}_i^{(t)}$ la prédiction à l'issue de l'étape $t$: $\hat{y}_i^{(t)} = \sum_{k=1}^t f_k(\mathbf{x}_i)$, et on écrit la fonction-objectif `#mi("\mathcal{L}^{(t)}")`{=typst} au moment de l'entraînement du $t$-ième arbre: 

```{=typst} 
#mitex(`
$$\begin{align*}
\mathcal{L}^{(t)} 
&= \sum_{i=1}^{n} l(y_i, \hat{y}_{i}^{(t)})                         & + \sum_{k=1}^t\Omega(f_k) \\
&= \sum_{i=1}^{n} l(y_i, \hat{y}_{i}^{(t-1)} + f_{t}(\mathbf{x}_i)) & + \sum_{k=1}^{t-1}\Omega(f_k) + \Omega(f_k)
\end{align*}$$
`)
```

### Faire apparaître le gradient

Une fois isolé le $k$-ième arbre, on fait un développement limité d'ordre 2 de $l(y_i, \hat{y}_{i}^{(t-1)} + f_{k}(\mathbf{x}_i))$ au voisinage de $\hat{y}_{i}^{(t-1)}$: 

```{=typst} 
#mitex(`
$$\mathcal{L}^{(t)} \approx \sum_{i=1}^{n} [\underbrace{l(y_i, \hat{y}_{i}^{(t-1)})}_{\text{(A)}} + g_i f_k(\mathbf{x}_i)+ \frac{1}{2} h_i f^2_t(\mathbf{x}_i)] + \underbrace{\sum_{k=1}^{t-1}\Omega(f_k)}_{\text{(B)}} + \Omega(f_t)$$
`)
```

```{=typst}
#mitex(`
$$\text{avec}\hspace{4mm} 
g_i = \frac{\partial l(y_i, \hat{y}_i^{(t-1)})}{\partial\hat{y}_i^{(t-1)}} \hspace{4mm}\text{et}\hspace{4mm}
h_i = \frac{\partial^2 l(y_i, \hat{y}_i^{(t-1)})}{\partial \left(\hat{y}_i^{(t-1)}\right)^2}$$
`)
```

Il est important de noter qu'à l'issue de l'entraînement des $t-1$ arbres précédents, les termes (A) et (C) sont constants, et ne sont pas modifiés par l'entraînement du $t$-ième arbre. 
<!-- Autrement dit, la seule façon d'améliorer le modèle sera de trouver un $t$-ième arbre $f_t$ qui minimise la fonction-objectif `#mi("\mathcal{L}^{(t)}")`{=typst} ainsi réécrite.  -->
On peut donc retirer les termes constants pour obtenir la fonction-objectif simplifiée qui sera utilisée pour l'entraînement du $t$-ième arbre.

```{=typst} 
#mitex(`
$$\mathcal{\tilde{L}}^{(t)} = \sum_{i=1}^{n} [g_i f_t(\mathbf{x}_i)+ \frac{1}{2} h_i f^2_t(\mathbf{x}_i)] + \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_i^2$$
`)
```

### Calculer les poids optimaux

A partir de l'expression précédente, il est possible de faire apparaître les poids $w_i$ du $t$-ième arbre. Pour une structure d'arbre donnée ($q: \mathbb{R}^m \rightarrow \{1, \dots, T\}$), on définit $I_j = \{ i | q(\mathbf{x}_i) = j \}$ l'ensemble des observations situées sur la feuille $j$ puis on réorganise $\mathcal{\tilde{L}}^{(t)}$:

```{=typst} 
#mitex(`
$$\begin{align*}
\mathcal{\tilde{L}}^{(t)}
  &= \sum_{j=1}^{T} \sum_{i\in I_j} \Bigl[g_i f_t(\mathbf{x}_i) &+ \frac{1}{2} h_i f^2_t(\mathbf{x}_i)\Bigr] &+ \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_i^2 \\
  &= \sum_{j=1}^{T} \sum_{i\in I_j} \Bigl[g_i w_j &+ \frac{1}{2} h_i w_j^2\Bigl] &+ \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_i^2 \\
  &= \sum^T_{j=1} \Bigl[w_j\Bigl(\sum_{i\in I_j} g_i\Bigr) &+ \frac{1}{2} w_j^2 \Bigl(\sum_{i\in I_j} h_i + \lambda\Bigr) \Bigr] &+ \gamma T  \label{eq:dudu}
\end{align*}$$
`)
```

Dans la dernière expression, on voit que la fonction de perte simplifiée se reformule comme une combinaison quadratique des poids $w_i$, dans laquelle les dérivées première et seconde de la fonction de perte interviennent sous forme de pondérations. Tout l'enjeu de l'entraînement devient donc de trouver les poids optimaux $w_j$ qui minimiseront cette fonction de perte, compte tenu de ces opérations.

Il se trouve que le calculs de ces poids optimaux est très simple: pour une structure d'arbre donnée ($q: \mathbb{R}^m \rightarrow \{1, \dots, T\}$), le poids optimal `#mi("w_j^{\ast}")`{=typst} de la feuille $j$ est donné par la formule: 

```{=typst} 
$ w_j^* = -frac(sum_(i in I_j) g_i, sum_(i in I_j) h_i + lambda) $ <w_j_optimal>
```

Par conséquent, la valeur optimale de la fonction objectif pour l'arbre $q$ est égale à

```{=typst} 
#mitex(`
$$\mathcal{\tilde{L}}^{(t)}(q) = -\frac{1}{2} \sum_{j=1}^T \frac{\left(\sum_{i\in I_j} g_i\right)^2}{\sum_{i\in I_j} h_i+\lambda} + \gamma T$$
`) <fct_obj_optimal>
```

### Construire le $k$-ième arbre


Dans la mesure où l'équation `@fct_obj_optimal`{=typst} permet de mesurer la qualité globale d'un arbre (et donc de comparer des arbres entre eux), on pourrait penser qu'elle est suffisante pour choisir directement le $k$-ième arbre: il suffirait d'énumérer les arbres possibles, de calculer la qualité de chacun d'entre eux, et de retenir le meilleur. Bien que cette approche soit possible théoriquement, elle est inemployable en pratique car le nombre d'arbres possibles est extrêmement élevé. Par conséquent, le $k$-ième arbre n'est pas défini en une fois, mais construit de façon gloutonne REFERENCE A LA PARTIE CART/RF.

Pour chaque noeud de l'arbre, on essaie d'ajouter un _split_ et on utilise la formule `@fct_obj_optimal`{=typst} pour évaluer le gain induit par ce _split_ marginal. Imaginons qu'on envisage de décomposer la feuille $I$ en deux nouvelles feuilles $I_L$ et $I_R$ (avec $I = I_L \cup I_R$), selon une condition logique reposant sur une variable et une valeur de cette variable (exemple: $x_6 > 11$). Par application de la formule @fct_obj_optimal, le gain potentiel induit par ce _split_ est égal à:

```{=typst} 
#mitex(`
$$\mathcal{L}_{\text{split}} = \frac{1}{2} \left[\frac{\left(\sum_{i\in I_L} g_i\right)^2}{\sum_{i\in I_L} h_i+\lambda}+\frac{\left(\sum_{i\in I_R} g_i\right)^2}{\sum_{i\in I_R} h_i+\lambda}-\frac{\left(\sum_{i\in I} g_i\right)^2}{\sum_{i\in I} h_i+\lambda}\right] - \gamma$$
`) <fct_eval_split>
```
Cette dernière formule est au coeur de la mécanique du _gradient boosting_: à chaque étape de l'entraînement d'un arbre, elle permet de comparer entre eux les _splits_ possibles et de choisir le _split_ qui réduit le plus la fonction de perte.

### Choisir les _splits_

ENUMERATION DES VARIABLES ET DES VALEURS




<!-- 
## Encore des équations


```{=typst} 
#mitex(`
$obj^{(t)} = \sum_{i=1}^n (y_{i} - (\hat{y}_{i}^{(t-1)} + f_t(x_i)))^2 + \sum_{i=1}^t\Omega(f_i) \\ = \sum_{i=1}^n [2(\hat{y}_i^{(t-1)} - y_i)f_t(x_i) + f_t(x_i)^2] + \Omega(f_t) + constant$
`)
```

```{=typst} 
#mitex(`
$obj^{(t)} = \sum_{i=1}^{n} [l(y_i, \hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2} h_{i} f_{t}^2(x_i)] + \Omega(f_t) + constant$
`)
```


## Toujours des équations



```{=typst} 
#mitex(`
$\sum_{i=1}^n [g_{i} f_{t}(x_i) + \frac{1}{2} h_{i} f_{t}^2(x_i)] + \Omega(f_t)$
`)
```


```{=typst} 
#mitex(`
$f_t(x) = w_{q(x)}, w \in R^{T}, q:R^d\rightarrow \{1,2,...,T\}$
`)
```


## Ya encore des équations

```{=typst} 
#mitex(`
$\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2 obj^{(t)} \approx \sum_{i=1}^n [g_i w_{q(x_i)} + \frac{1}{2} h_i w_{q(x_i)}^2] + \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2\\ = \sum^T_{j=1} [(\sum_{i\in I_j} g_i) w_j + \frac{1}{2} (\sum_{i\in I_j} h_i + \lambda) w_j^2 ] + \gamma T$
`)
```


```{=typst} 
#mitex(`
$obj^{(t)} = \sum^T_{j=1} [G_jw_j + \frac{1}{2} (H_j+\lambda) w_j^2] +\gamma T$
`)
```


```{=typst} 
#mitex(`
$G_j = \sum_{i\in I_j} g_i \text{~~et~~} H_j = \sum_{i\in I_j} h_i$
`)
```


```{=typst} 
#mitex(`
$w_j^{\ast} = -\frac{G_j}{H_j+\lambda}\\ \text{obj}^{\ast} = -\frac{1}{2} \sum_{j=1}^T \frac{G_j^2}{H_j+\lambda} + \gamma T$
`)
```


```{=typst} 
#mitex(`
$Gain = \frac{1}{2} \left[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\right] - \gamma$
`)
```

 -->




