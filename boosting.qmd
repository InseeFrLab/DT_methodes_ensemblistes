---
title: "Le boosting"
author: |
  [Olivier Meslin](https://github.com/oliviermeslin)
  [Mélina Hillion](https://github.com/melinahillion)
format:
  typst:
    toc: true
    section-numbering: 1.1.1
    bibliography: references.bib
---
  
$
\DeclareMathOperator*{\argmin}{arg\,min}
\makeatletter
\let\ams@underbrace=\underbrace
\def\underbrace#1_#2{%
  \setbox0=\hbox{$\displaystyle#1$}%
  \ams@underbrace{#1}_{\parbox[t]{\the\wd0}{#2}}%
}
\makeatother
$

Le _boosting_ est une approche ensembliste qui ne traite pas les modèles de base séparément les uns des autres.


Imaginons qu'on veuille entraîner le modèle suivant:

$F\left(\mathbf{x}\right) = \sum_{m=1}^M \beta_m f\left(\mathbf{x}, \mathbf{\theta}_m\right)$


$\hat{F}$ est caractérisée par les paramètres $\{\beta_m, \mathbf{\theta}_m\}_{m=1}^{M}$ tels que
$\argmin_{\{\beta_m, \mathbf{\theta}_m\}_{m=1}^{M}} \sum_{i=1}^N L\left(y_i, \sum_{m=1}^M \beta_m f\left(\mathbf{x}_i, \mathbf{\theta}_m\right)\right)$


C'est un problème très compliqué dès que $M$ est élevé!

## Le *boosting*

Le *boosting* combine une approche ensembliste avec une **modélisation additive par étapes** (*forward stagewise additive modeling*). La modélisation additive par étapes consite à décomposer l'entraînement d'un modèle très complexe en une __séquence d'entraînements de petits modèles__, procédure beaucoup plus simple. 

D'autre part, chacun de ces modèles simples est entraîné de façon à améliorer la prédiction par rapport à 

. . .

- Chaque _base learner_ essaie de [corriger les erreurs du modèle issu des étapes précédentes]{.blue2}.


## Modélisation additive par étapes {.small90}

Un point important est que __la modélisation additive par étapes simplifie considérablement l'entraînement du modèle__: - On commence par entraîner un seul _base learner_;
- On ajoute au modèle un deuxième _base learner_ sans modifier les paramètres du premier _base learner_ déjà entraîné;
- On entraîne ce deuxième _base learner_ de façon à améliorer le plus possible le modèle global (la somme des deux _base learners_).
- On ajoute un troisième _base learner_ et on l'entraîne...

<!--
```{=typst} 
#import "@preview/algo:0.3.3": algo, i, d, comment, code

#algo(
  line-numbers: false,
  title: Modélisation additive par étape
)[
  if $n < 0$:#i\        // use #i to indent the following lines
    return null#d\      // use #d to dedent the following lines
  if $n = 0$ or $n = 1$:#i #comment[you can also]\
    return $n$#d #comment[add comments!]\
  return #smallcaps("Fib")$(n-1) +$ #smallcaps("Fib")$(n-2)$
]    
```
-->


1.  Commencer par $f_0\left(\mathbf{x}\right) = 0$.
2.  Pour $m = 1, \dots, M:$
    (a) Entraîner le $m$-ième modèle:
    $$\left(\hat{\beta}_m, \hat{\theta}_m\right) = \argmin_{\beta, \mathbf{\theta}} \sum_{i=1}^N L\left(y_i, f_{m-1}\left(\mathbf{x}_i\right) + \beta b\left(\mathbf{x}_i, \mathbf{\theta}\right)\right)$$
    (b) Définir $f_m\left(\mathbf{x}\right) = f_{m-1}\left(\mathbf{x}\right) + \hat{\beta}_m b\left(\mathbf{x}_i, \mathbf{\hat{\theta}_m}\right)$

## La mécanique du _gradient boosting_

Depuis la publication de @friedman2001, la méthode de _gradient boosting_ a connu de multiples implémentations, parmi lesquelles celle proposée par Friedman lui-même, XGBoost (@chen2016xgboost), LightGBM (@ke2017lightgbm) et CatBoost (@prokhorenkova2018catboost). S'il existe quelques différences entre ces implémentations, elles partagent néanmoins la même mécanique d'ensemble, que la section qui suit va présenter en détail en s'appuyant sur l'implémentation proposée par XBGoost.[^Cette partie reprend la structure et les notations de la partie 2 de @chen2016xgboost.]

Quatre étapes:

- Présentation du modèle qu'on veut entraîner;
- Reformulation du modèle pour faire apparaître les gradients;
- Calcul des poids optimaux et de la qualité d'un arbre dont la structure est donnée;
- Définition d'une méthode et de critères permettant de construire un arbre.


### Le modèle à veut entraîner

On veut entraîner un [modèle]{.orange} comprenant $K$ arbres de régression: $$\hat{y}_{i} = \phi\left(\mathbf{x}_i\right) = \sum_{k=1}^{K} f_k\left(\mathbf{x}_i\right), f_k \epsilon \mathcal{F}$$

- Chaque arbre est défini par trois [paramètres]{.orange}: sa structure (fonction $q: \mathbb{R}^m \rightarrow \{1, \dots, T\}$), son nombre de feuilles terminales $T$ et ses poids $\mathbf{w}\in \mathbb{R}^T$.

- On entraîne le modèle avec une [fonction-objectif]{.orange} constituée d'une [fonction de perte]{.orange} dérivable et convexe $l$ et d'une [fonction de régularisation]{.orange} $\Omega$.$$\mathcal{L}(\phi) = \underbrace{\sum_i l(\hat{y}_{i}, y_{i})}_{\substack{\text{Perte sur les} \\ \text{observations}}} + \underbrace{\sum_k \Omega(f_{k})}_{\substack{\text{Fonction de} \\ \text{régularisation}}}\hspace{5mm}\text{avec}\hspace{5mm}\Omega(f) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_i^2$$

- Notation: $\hat{y}_i^{(t)}$ désigne la prédiction à lissue de l'étape $t$: $\hat{y}_i^{(t)} = \sum_{k=1}^t f_k(\mathbf{x}_i)$.

<!-- $\mathcal{F} = \{f(\mathbf{x}) = w_{q(\mathbf{x})}\}(q:\mathbb{R}^m \rightarrow T,w \in \mathbb{R}^T)$ -->

<!-- $\hat{y}_i^{(0)} = 0\\ \hat{y}_i^{(1)} = f_1(x_i) = \hat{y}_i^{(0)} + f_1(x_i)\\ \hat{y}_i^{(2)} = f_1(x_i) + f_2(x_i)= \hat{y}_i^{(1)} + f_2(x_i)\\ ....\\ \hat{y}_i^{(t)} = \sum_{k=1}^t f_k(x_i)= \hat{y}_i^{(t-1)} + f_t(x_i)$ -->

::::
:::

## Faire apparaître le gradient {.small90}

::: {.small80}
:::: {.incremental}

- On écrit la fonction-objectif au moment de l'entraînement du $t$-ième arbre: $$\begin{aligned}\mathcal{L}^{(t)} 
&= \sum_{i=1}^{n} l(y_i, \hat{y}_{i}^{(t)}) + \sum_{k=1}^t\Omega(f_k) \\
&= \sum_{i=1}^{n} l\left(y_i, \hat{y}_{i}^{(t-1)} + f_{t}(\mathbf{x}_i)\right) + \Omega(f_t) + constant
\end{aligned}$$

- On fait un développement limité d'ordre 2 de $l(y_i, \hat{y}_{i}^{(t-1)} + f_{t}(\mathbf{x}_i))$ au voisinage de $f_{t}(\mathbf{x}_i)$: $$\mathcal{L}^{(t)} \approx \sum_{i=1}^{n} [l(y_i, \hat{y}_{i}^{(t-1)}) + g_i f_t(\mathbf{x}_i)+ \frac{1}{2} h_i f^2_t(\mathbf{x}_i)] + \Omega(f_t)$$ avec $$g_i = \frac{\partial l(y_i, \hat{y}_i^{(t-1)})}{\partial\hat{y}_i^{(t-1)}} \hspace{4mm}\text{et}\hspace{4mm} h_i = \frac{\partial^2 l(y_i, \hat{y}_i^{(t-1)})}{{\partial \hat{y}_i^{(t-1)}}^2}$$



::::
:::

## Faire apparaître les poids $w_j$ {.small90}

::: {.small80}
:::: {.incremental}

- On peut simplifier $\mathcal{\tilde{L}}^{(t)}$ en enlevant les termes $l(y_i, \hat{y}_{i}^{(t-1)})$ car ils sont constants et en remplaçant $\Omega(f)$ par sa définition: 
$$\mathcal{\tilde{L}}^{(t)} = \sum_{i=1}^{n} [g_i f_t(\mathbf{x}_i)+ \frac{1}{2} h_i f^2_t(\mathbf{x}_i)] + \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_i^2$$

- On définit $I_j = \{ i | q(\mathbf{x}_i) = j \}$ l'ensemble des observations situées sur la feuille $j$ et on réorganise $\mathcal{\tilde{L}}^{(t)}$ pour faire apparaître les poids $w_i$ du $t$-ième arbre:
$$\begin{alignat*}{4}
\mathcal{\tilde{L}}^{(t)}
  &= \sum_{j=1}^{T} \sum_{i\in I_j} \Bigl[g_i f_t(\mathbf{x}_i) &&+ \frac{1}{2} h_i f^2_t(\mathbf{x}_i)\Bigr] &&+ \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_i^2 \\
  &= \sum_{j=1}^{T} \sum_{i\in I_j} \Bigl[g_i w_j &&+ \frac{1}{2} h_i w_j^2\Bigl] &&+ \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_i^2 \\
  &= \sum^T_{j=1} \Bigl[w_j\Bigl(\sum_{i\in I_j} g_i\Bigr) &&+ \frac{1}{2} w_j^2 \Bigl(\sum_{i\in I_j} h_i + \lambda\Bigr) \Bigr] &&+ \gamma T
\end{alignat*}$$

:::
::::

## Calculer les poids optimaux $w_j$ {.small90}

::: {.small90}
:::: {.incremental}

- Pour une structure d'arbre donnée ($q: \mathbb{R}^m \rightarrow \{1, \dots, T\}$), on peut facilement calculer les poids optimaux et la valeur optimale de la fonction-objectif:
$$w_j^{\ast} = -\frac{\sum_{i\in I_j} g_i}{\sum_{i\in I_j} h_i+\lambda}$$
$$\mathcal{\tilde{L}}^{(t)}(q) = -\frac{1}{2} \sum_{j=1}^T \frac{\left(\sum_{i\in I_j} g_i\right)^2}{\sum_{i\in I_j} h_i+\lambda} + \gamma T$$

- [__Ces deux formules sont très utiles!__]{.blue2}

:::
::::

## Évaluer la qualité d'un arbre et d'un _split_ {.small90}

::: {.small80}
:::: {.incremental}

- On peut utiliser la formule donnant la valeur optimale de la fonction-objectif pour comparer la qualité de deux arbres.

- On également peut utiliser cette formule pour évaluer le gain (réduction de la fonction-objectif) induit par un _split_ marginal.

- Imaginons qu'on décompose la feuille $I$ en deux nouvelles feuilles $I_L$ et $I_R$ (avec $I = I_L \cup I_R$). Le gain potentiel par ce _split_ est:
$$\mathcal{L}_{\text{split}} = \frac{1}{2} \left[\frac{\left(\sum_{i\in I_L} g_i\right)^2}{\sum_{i\in I_L} h_i+\lambda}+\frac{\left(\sum_{i\in I_R} g_i\right)^2}{\sum_{i\in I_R} h_i+\lambda}-\frac{\left(\sum_{i\in I} g_i\right)^2}{\sum_{i\in I} h_i+\lambda}\right] - \gamma$$

- Cette dernière formule est essentielle: elle permet de comparer les _splits_ possibles lorsqu'on construit un arbre.

:::
::::

## Comment construire un arbre

- Bien dire qu'on est en approche _greedy_ (glouton/morfal/bourrin).
- Décrire les différentes façons de rechercher les _splits_.
- Parler à la fin du _pruning_.



## Encore des équations


$obj^{(t)} = \sum_{i=1}^n (y_{i} - (\hat{y}_{i}^{(t-1)} + f_t(x_i)))^2 + \sum_{i=1}^t\Omega(f_i) \\ = \sum_{i=1}^n [2(\hat{y}_i^{(t-1)} - y_i)f_t(x_i) + f_t(x_i)^2] + \Omega(f_t) + constant$

$obj^{(t)} = \sum_{i=1}^{n} [l(y_i, \hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2} h_{i} f_{t}^2(x_i)] + \Omega(f_t) + constant$

## Toujours des équations



$\sum_{i=1}^n [g_{i} f_{t}(x_i) + \frac{1}{2} h_{i} f_{t}^2(x_i)] + \Omega(f_t)$

$f_t(x) = w_{q(x)}, w \in R^{T}, q:R^d\rightarrow \{1,2,...,T\}$

## Ya encore des équations

$\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2 obj^{(t)} \approx \sum_{i=1}^n [g_i w_{q(x_i)} + \frac{1}{2} h_i w_{q(x_i)}^2] + \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2\\ = \sum^T_{j=1} [(\sum_{i\in I_j} g_i) w_j + \frac{1}{2} (\sum_{i\in I_j} h_i + \lambda) w_j^2 ] + \gamma T$

$obj^{(t)} = \sum^T_{j=1} [G_jw_j + \frac{1}{2} (H_j+\lambda) w_j^2] +\gamma T$

$G_j = \sum_{i\in I_j} g_i\\ H_j = \sum_{i\in I_j} h_i$

$w_j^{\ast} = -\frac{G_j}{H_j+\lambda}\\ \text{obj}^{\ast} = -\frac{1}{2} \sum_{j=1}^T \frac{G_j^2}{H_j+\lambda} + \gamma T$

$Gain = \frac{1}{2} \left[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\right] - \gamma$





