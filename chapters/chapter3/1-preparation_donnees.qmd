
## Préparation des données {#sec-preprocessing}

Les méthodes ensemblistes à base d'arbres ne requièrent pas le même travail préparation des données que les méthodes économétriques traditionnelles. En particulier, certaines étapes indispensables à l'économétrie cessent d'être nécessaires. En revanche, d'autres, notamment le traitement des valeurs manquantes et des variables catégorielles. Cette section gagne à être lue après ou en parallèle de la section @sec-sujets-avances qui détaille les approches possibles.

### Préparation des variables explicatives

#### Quels sont les traitements inutiles?

Deux traitements usuels en économétrie ne sont pas nécessaires pour utiliser des méthodes ensemblistes: 

- Il est inutile de normaliser ou de standardiser les variables numériques car c'est l'_ordre_ défini par les valeurs qui est essentiel, pas les valeurs numériques elles-mêmes (voir la section @sec-CART). Pour la même raison, il est inutile de modifier ou supprimer les valeurs extrêmes.

- Il n'est pas indispensable de supprimer les variables corrélées car les méthodes ensemblistes sont robustes à la multicollinéarité. Ceci dit, réduire le nombre de variables peut accélérer légèrement l'entraînement des modèles.

#### Comment traiter les variables catégorielles?

Le __traitement des variables catégorielles__ est un sujet plus complexe, car plusieurs approches sont possibles et certaines implémentations peuvent prendre en charge les variables catégorielles sans préparation particulière. Il est recommandé de lire la section @sec-categorical-variables-encoding qui détaille les approches possibles. L'approche à privilégier _in fine_ dépendra de trois facteurs: l'implémentation utilisée, l'existence d'un ordre des catégories, et le nombre total de catégories. Voici quelques recommandations générales:

  - __il est nettement préférable de convertir les variables catégorielles en variables numériques lorsqu'elles comportent un ordre naturel__ (exemples: âge, niveau de revenu, niveau de diplôme, niveau de densité urbaine...). Cette approche a l'avantage de simplifier considérablement l'utilisation des variables catégorielles, en les traitant comme des variables numériques classiques. Par exemple, l'âge exprimé sous forme de tranche d'âge ($[0; 9]$, $[10; 19]$, $[20; 29]$, etc.) peut aisément être converti en variable numérique: la valeur 0 sera associée à la modalité $[0; 9]$, la valeur 1 à la modalité $[10; 19]$. L'encodeur `OrdinalEncoder` de `scikit-learn` permet d'automatiser cette tâche de façon efficace. De la même façon, il est possible de convertir une date en variable numérique exprimée en jours écoulés depuis une date de référence.

  - dans le cas des variables catégorielles non ordonnées (exemple: secteur d'activité, PCS, département...), __il est préférable de tester les approches intégrées aux implémentations des méthodes ensemblistes__ avant se lancer dans la construction d'une approche _ad hoc_.

  - __le _one-hot-encoding_ ne doit pas être utilisé pour les variables catégorielles qui présentent un nombre élevé de modalités__ (par exemple au-delà de 10 modalités).

#### Comment traiter les valeurs manquantes?

A COMPLETER

(voir la partie @sec-valeurs-manquantes);

- En revanche, il peut être utile de les corriger si elles sont clairement erronées.

- **Nettoyage des données**
  - Gestion des valeurs manquantes : Imputation ou suppression si nécessaire (même si certaines implémentations de Random Forest gèrent les valeurs manquantes, il est préférable de les traiter pour éviter les biais)[5].

#### Est-il utile de créer des variables additionnelles?

Il n'est pas toujours simple de savoir s'il est nécessaire d'ajouter des variables additionnelles dans les données d'entraînement des algorithmes ensemblistes. Il arrive d'ailleurs qu'on affirme que créer des variables additionnelles ne présente pas d'intérêt, dans la mesure où ces algorithmes sont capables de modéliser des relations complexes, non linéaires et faisant intervenir des interactions arbitraires entre variables. Cette affirmation n'est que partiellement vraie, comme le montrent les paragraphes suivants.

Une chose est sûre: __il est inutile d'ajouter aux données des variables numériques issues d'une transformation _monotone_ d'une variable existante__. En effet, si une variable additionnelle $x_2$ est issue d'une transformation monotone de la variable $x_1$ déjà présente dans les données, alors les sous-régions qui peuvent être définies par une règle de décision basée sur $x_2$ sont identiques à celles qui peuvent être définies avec $x_1$. Par conséquent, $x_2$ ne permet pas d'affiner le partitionnement au-delà de ce qui était déjà possible avec $x_1$. Cela signifie en pratique qu'il faut éviter d'inclure des variables telles que le carré ou le cube d'une variable déjà présente dans les données.

Inversement, __une variable additionnelle peut s'avérer utile si elle est issue d'une transformation _non monotone_ d'une variable, ou d'une transformation quelconque de deux ou plusieurs variables.__ Voici deux exemples qui en illustrent l'intérêt:

  - __Transformation _non monotone_ d'une variable__: imaginons qu'on veuille prédire le prix moyen des glaces à Paris uniquement avec la date, exprimée sous forme numérique en jours écoulés depuis une date de référence. Il est probable que ce prix présente une saisonnalité marquée (prix élevé en été, faible en hiver). On peut alors envisager d'ajouter une variable issue d'une transformation sinusoïdale de la date; cette variable prendra une valeur élevée à certaines périodes et plus faible à d'autres, ce qui aidera à capter la saisonnalité.

  - __Transformation quelconque de deux ou plusieurs variables__: imaginons qu'on veuille prédire la probabilité de faillite d'une entreprise à partir de seulement deux variables: son chiffre d'affaires et de son excédent brut d'exploitation (EBE). Ajouter le carré de l'EBE est inutile car il s'agit d'une transformation monotone de cette variable: les entreprises ayant un EBE faible ont un EBE carré faible, et ainsi de suite. En revanche, il est probablement utile d'ajouter le taux de marge (défini comme le ratio entre chiffre d'affaires et EBE), car un taux de marge faible peut aider à repérer des sous-groupes d'entreprises en difficulté, et ce indépendamment de leur taille. 

Il faut noter que dans ces deux exemples, un algorithme ensembliste parviendrait probablement à capter la saisonnalité du prix des glaces ou à repérer les entreprises à faible taux de marge même en l'absence de variables additionnelles. Mais il faut bien comprendre que ce résultat s'obtiendrait au prix d'un grand nombre de _splits_ aboutissant à des arbres profonds et complexes. Si elles ne sont donc effectivement pas strictement indispensables pour que les algorithmes soient performants, des variables additionnelles bien choisies ont néanmoins pour effet de faciliter et d'accélérer la construction du modèle.


### Préparation de la variable-cible

Les méthodes ensemblistes sont sensibles aux valeurs prises par la variable-cible. La préparation de la variable-cible obéit donc à des règles différentes de celles applicables aux variables explicatives:

- Il faut choisir soigneusement les transformations appliquées à la variable-cible: on obtient des modèles très différents selon qu'on entraîne un algorithme avec pour variable-cible le prix de vente des logements, le prix au mètre carré ou le logarithme du prix au mètre carré.

- Il faut repérer et traiter les valeurs extrêmes, aberrantes ou erronées prises par la variable-cible, soit en les corrigeant, soit en supprimant les observations concernés. Ce point est particulièrement important lorsqu'on veut entraîner un algorithme de _gradient boosting_.



### Train-test

Pas indispensable pour RF, mais souhaitable. Indispensable pour GB.


## Evaluation des performances du modèle et optimisation des hyper-paramètres

### Estimation de l'erreur par validation croisée

La validation croisée est une méthode d'évaluation couramment utilisée en apprentissage automatique pour estimer la capacité d'un modèle à généraliser les prédictions à de nouvelles données. Bien que l'évaluation par l'erreur _Out-of-Bag_ (OOB) soit généralement suffisante pour les forêts aléatoires, la validation croisée permet d'obtenir une évaluation plus robuste, car moins sensible à l'échantillon d'entraînement, notamment sur des jeux de données de petite taille.

Concrètement, le jeu de donné est divisé en $k$ sous-ensembles, un modèle est entraîné sur $k-1$ sous-ensembles et testé sur le sous-ensemble restant. L'opération est répétée $k$ fois de manière à ce que chaque observation apparaisse au moins une fois dans l'échantillon test. L'erreur est ensuite moyennée sur l'ensemble des échantillons test.


**Procédure de validation croisée**:

La validation croisée la plus courante est la validation croisée en k sous-échantillons (_k-fold cross-validation_):

- **Division des données** : Le jeu de données est divisé en k sous-échantillons égaux, appelés folds. Typiquement, k est choisi entre 5 et 10, mais il peut être ajusté en fonction de la taille des données.

- **Entraînement et test** : Le modèle est entraîné sur k - 1 sous-échantillons et testé sur le sous-échantillon restant. Cette opération est répétée k fois, chaque sous-échantillon jouant à tour de rôle le rôle de jeu de test.

- **Calcul de la performance** : Les k performances obtenues (par exemple, l'erreur quadratique moyenne pour une régression, ou l'accuracy (_exactitude_)  pour une classification) sont moyennées pour obtenir une estimation finale de la performance du modèle.


**Avantages de la validation croisée**:

- **Utilisation optimale des données** : En particulier lorsque les données sont limitées, la validation croisée maximise l'utilisation de l'ensemble des données en permettant à chaque échantillon de contribuer à la fois à l'entraînement et au test.

- **Réduction de la variance** : En utilisant plusieurs divisions des données, on obtient une estimation de la performance moins sensible aux particularités d'une seule division.

Bien que plus coûteuse en termes de calcul, la validation croisée est souvent préférée lorsque les données sont limitées ou lorsque l'on souhaite évaluer différents modèles ou hyperparamètres avec précision.


**Leave-One-Out Cross-Validation (LOOCV)** : 
Il s'agit d'un cas particulier où le nombre de sous-échantillons est égal à la taille du jeu de données. En d'autres termes, chaque échantillon est utilisé une fois comme jeu de test, et tous les autres échantillons pour l'entraînement. LOOCV fournit une estimation très précise de la performance, mais est très coûteuse en temps de calcul, surtout pour de grands jeux de données.


### Choix des hyper-paramètres du modèle

L'estimation Out-of-Bag (OOB) et la validation croisée sont deux méthodes clés pour optimiser les hyper-paramètres d'une forêt aléatoire. Les deux approches permettent de comparer les performances obtenues pour différentes combinaisons d'hyper-paramètres et de sélectionner celles qui maximisent les performances prédictives, l'OOB étant souvent plus rapide et moins coûteuse, tandis que la validation croisée est plus fiable dans des situations où le surapprentissage est un risque important (@probst2019hyperparameters). 

Il convient de définir une stratégie d'optimisation des hyperparamètres pour ne pas perdre de temps à tester trop de jeux d'hyperparamètres. Plusieurs stratégies existent pour y parvenir, les principales sont exposées dans la section @sec-guide-rf. Les implémentations des forêts aléatoires disponibles en `R` et en Python permettent d'optimiser aisément les principaux hyper-paramètres des forêts aléatoires.

#### Méthodes de recherche exhaustives

- **Recherche sur grille** (Grid Search): Cette approche simple explore toutes les combinaisons possibles d'hyperparamètres définis sur une grille. Les paramètres continus doivent être discrétisés au préalable. La méthode est exhaustive mais coûteuse en calcul, surtout pour un grand nombre d'hyperparamètres.

- **Recherche aléatoire** (Random Search): Plus efficace que la recherche sur grille, cette méthode échantillonne aléatoirement les valeurs des hyperparamètres dans un espace défini. Bergstra et Bengio (2012) ont démontré sa supériorité pour les réseaux neuronaux, et elle est également pertinente pour les forêts aléatoires. La distribution d'échantillonnage est souvent uniforme.


#### Optimisation séquentielle/itérative basée sur un modèle (SMBO)

La méthode SMBO (Sequential model-based optimization) est une approche plus efficace que les précédentes car elle s'appuie sur les résultats des évaluations déjà effectuées pour guider la recherche des prochains hyper-paramètres à tester (@probst2019hyperparameters). 

Voici les étapes clés de cette méthode:

- Définition du problème: On spécifie une mesure d'évaluation (ex: AUC pour la classification, MSE pour la régression), une stratégie d'évaluation (ex: validation croisée k-fold), et l'espace des hyperparamètres à explorer.

- Initialisation: échantillonner aléatoirement des points dans l'espace des hyperparamètres et évaluer leurs performances.

- Boucle itérative :
  - Construction d'un modèle de substitution (surrogate model): un modèle de régression (ex: krigeage ou une forêt aléatoire) est ajusté aux données déjà observées. Ce modèle prédit la performance en fonction des hyperparamètres.
  - Sélection d'un nouvel hyperparamètre: un critère basé sur le modèle de substitution sélectionne le prochain ensemble d'hyperparamètres à évaluer. Ce critère vise à explorer des régions prometteuses de l'espace des hyperparamètres qui n'ont pas encore été suffisamment explorées.
  - Évaluer les points proposés et les ajouter à l'ensemble déjà exploré: la performance du nouvel ensemble d'hyperparamètres est évaluée et ajoutée à l'ensemble des données d'apprentissage du modèle de substitution afin d'orienter les recherches vers de nouveaux hyper-paramètres prometteurs.
