# Aperçu des méthodes ensemblistes

__Principe__: cette partie propose une présentation intuitive des méthodes ensemblistes, à destination notamment des _managers_ sans bagage en _machine learning_. Elle ne contient aucune formalisation mathématique.

## Que sont les méthodes ensemblistes?

Les approches ensemblistes désignent un ensemble d'algorithmes de _machine learning_ supervisé développés depuis le début des années 1990, c'est-à-dire des méthodes statistiques permettant de prédire une variable-cible $y$ à partir d'un ensemble de variables $\mathbf{X}$ (_features_). Elles peuvent par exemple être utilisées pour prédire le salaire d'un salarié, la probabilité de réponse dans une enquête, le niveau de diplôme... Ces approches se définissent par un point commun: plutôt que de tenter de construire d'emblée un unique modèle très complexe et très performant, elles visent à obtenir un modèle très performant en combinant intelligemment un ensemble de modèles simples et peu performants, dits "apprenants faibles" (_weak learner_ ou _base learner_). Le choix de ces modèles de base (des arbres de décision dans la plupart des cas) et la manière dont leurs prédictions sont combinées sont des facteurs déterminants pour la performance de ces approches. Le présent document se concentre sur trois approches ensemblistes à base d'arbres de décisions qui sont fréquemment utilisées: le _bagging_, la forêt aléatoire (_random forest_) et le _boosting_.

On distingue deux grandes familles de méthodes ensemblistes, selon qu'elles s'appuient sur des modèles de base entrainés en parallèle indépendamment les uns des autres, ou au contraire de façon séquentielle. Lorsque les modèles sont _entrainés en parallèle_, chaque modèle de base est entraîné en utilisant soit un échantillon aléatoire des données d'entraînement, soit un sous-ensemble aléatoires des variables disponibles, et le plus souvent une combinaison des deux, auquel cas on parle de forêt aléatoire (_random forest_). Les implémentations les plus courantes des forêts aléatoires sont les _packages_ `ranger` en `R` et `scikit-learn` en Python. Lorsque les modèles de base sont _entrainés de manière séquentielle_, chaque modèle de base vise à améliorer la prédiction proposée par l'ensemble des modèles de base précédents. Les implémentations les plus courantes du _boosting_ sont actuellement `XGBoost`, `CatBoost` et `LightGBM`.

## Pourquoi utiliser des méthodes ensemblistes?

Les méthodes ensemblistes sont particulièrement bien adaptées à de nombreux cas d'usage de la statistique publique, pour deux raisons. D'une, elles sont conçues pour s'appliquer à des _données tabulaires_ (enregistrements en lignes, variables en colonnes), structure de données omniprésente dans la statistique publique. D'autre part, elles peuvent être mobilisées dans toutes les situations où le statisticien mobilise une régression linéaire ou une régression logistisque (imputation, repondération...).

Les méthodes ensemblistes présentent trois avantages par rapport aux méthodes économétriques traditionnelles (régression linéaire et régression logistique):

- Elles ont une __puissance prédictive supérieure__: alors que les méthodes traditionnelles supposent fréquemment l'existence d'une relation linéaire ou log-linéaire entre $y$ et $\mathbf{X}$, les méthodes ensemblistes ne font quasiment aucune hypothèse sur la relation entre $y$ et $\mathbf{X}$, et se contentent d'approximer le mieux possible cette relation à partir des données disponibles. En particulier, les modèles ensemblistes peuvent facilement modéliser des __non-linéarités__ de la relation entre $y$ et $\mathbf{X}$ et des __interactions__ entre variables explicatives _sans avoir à les spécifier explicitement_ au préalable, alors que les méthodes traditionnelles supposent fréquemment l'existence d'une relation linéaire ou log-linéaire entre $y$ et $\mathbf{X}$.

- Elles nécessitent __moins de préparation des données__: elles ne requièrent pas de normalisation des variables explicatives et peuvent s'accommoder des valeurs manquantes (selon des techniques variables selon les algorithmes).

- Elles sont généralement __moins sensibles aux valeurs extrêmes et à l'hétéroscédasticité__ des variables explicatives que les approches traditionnelles.

Elles présentent par ailleurs deux inconvénients rapport aux méthodes économétriques traditionnelles. Premièrement, bien qu'il existe désormais de multiples approches permettent d'interpétrer partiellement les modèles ensemblistes, leur interprétabilité reste globalement moindre que celle d'une régression linéaire ou logistique. Deuxièmement, les modèles ensemblistes sont plus complexes que les approches traditionnelles, et leurs hyperparamètres doivent faire l'objet d'une optimisation, par exemple au travers d'une validation croisée. Ce processus d'optimisation est généralement plus complexe et plus long que l'estimation d'une régression linéaire ou logistique. En revanche, les méthodes ensemblistes sont relativement simples à prendre en main, et ne requièrent pas nécessairement une puissance de calcul importante.

::: {.callout-note title="Et par rapport au _deep learning_?"}
Si les approches de _deep learning_ sont sans conteste très performantes pour le traitement du langage naturel et le traitement d'image, leur supériorité n'est pas établie pour les applications reposant sur des données tabulaires. Les comparaisons disponibles dans la littérature concluent en effet que les méthodes ensemblistes à base d'arbres sont soit plus performantes que les approches de _deep learning_ (@grinsztajn2022tree, @shwartz2022tabular), soit font jeu égal avec elles (@mcelfresh2024neural). Ces études ont identifié trois avantages des méthodes ensemblistes: elles sont peu sensibles aux variables explicatives non pertinentes, robustes aux valeurs extrêmes des variables explicatives, et capables d'approximer des fonctions très irrégulières. De plus, dans la pratique les méthodes ensemblistes sont souvent plus rapides à entraîner et moins gourmandes en ressources informatiques, et l'optimisation des hyperparamètres s'avère souvent moins complexe (@shwartz2022tabular).
:::

## Comment fonctionnent les méthodes ensemblistes?

### Le modèle de base: l'arbre de classification et de régression

#### Qu'est-ce qu'un arbre CART?

Le modèle de base des méthodes ensemblistes est le plus souvent un arbre de classification et de régression (CART, @breiman1984cart). Un arbre CART est un algorithme prédictif assez simple avec trois caractéristiques essentielles:

- L'arbre partitionne l'espace des variables explicatives $X$ en régions (appelées feuilles ou _leaves_) homogènes au sens d'une certaine mesure de l'hétérogénéité;
- Chaque région est définie par un ensemble de conditions, appelées régles de décision (_splitting rules_), qui portent sur les valeurs des variables explicatives (par exemple, une région peut être définie par la condition: $age > 40 & statut = 'Cadre'$);
- Une fois l'arbre construit, les prédictions de l'arbre pour chaque région se déduisent des données d'entraînement de façon intuitive: il s'agira de la classe la plus fréquente parmi les observations situées dans cette région dans le cas d'une classification, et de la moyenne des observations situées dans cette région dans le cas d'une régression.

La structure de cet algorithme a trois conséquences importantes:

-  **l'arbre final est une fonction constante par morceaux**: la prédiction est identique pour toutes les observations situées dans la même région, et ne varie que d'une région à l'autre. 
- aucune hypothèse sur le lien entre $X$ et $y$ forme fonctionnelle précise;
- présence d'interactions entre variables.

Illustration, et représentation graphique (sous forme d'arbre et de graphique).

#### Comment construit-on un arbre?

Si son principe est simple, la construction d'un arbre de décision se heurte à trois difficultés pratiques.

Première difficulté: comment trouver le partitionnement optimal en un temps raisonnable? Les arbres CART proposent une solution efficace à ce problème en faisant deux hypothèses simplificatrices. D'une part, la procédure de construction de l'arbre ne s'intéresse qu'à des critères de décision binaires très simples, mobilisant à chaque fois une seule variable et un seul seuil (exemples: $age > 40?$, $diplome = 'Licence'$...). Autrement dit, les critères complexes mobilisant des combinaisons de variables et de seuils sont exclus _a priori_. D'autre part, la construction de l'arbre se fait de façon itérative, une règle de décision à la fois: la procédure se contente à chaque étape de chercher la règle de décision qui réduit le plus l'hétérogénéité des groupes, conditionnellement aux régles de décision qui ont été choisies au préalable. Cette procédure ne garantit donc pas que l'arbre final soit optimal, mais elle permet d'obtenir rapidement un arbre raisonnablement performant.

Deuxième difficulté: comment mesurer l'homogénéité des régions?

Troisième difficulté: à quel moment faut-il s'arrêter?


#### Avantages et limites des arbres CART

Les arbres CART présentent plusieurs avantages: leur principe est simple, ils sont aisément interprétables et peuvent faire l'objet de représentations graphiques intuitives. Par ailleurs, la flexibilité offerte par le partitionnement récursif assure que les arbres obtenus reflètent les corrélations observées dans les données d'entraînement. 

Ils souffrent néanmoins de deux limites. D'une part, les arbres CART ont souvent un __pouvoir prédictif faible__ qui en limite l'usage. D'autre part, ils sont __peu robustes et instables__. Ainsi, un léger changement dans les données (par exemple l'ajout ou la suppression de quelques observations) peut entraîner des modifications significatives dans la structure de l'arbre et dans la définition des feuilles. De plus, les prédictions des arbres CART sont sensibles à de petites fluctuations des données: celles-ci peuvent aboutir à ce qu'une partie des observations change brutalement de feuille et donc de valeur prédite.

Toute les méthodes ensemblistes présentées ci-dessous (_bagging_, _random forests_ et _boosting_) combinent un grand nombre d'arbres de décision pour en surmonter les deux limites: il s'agit d'obtenir un modèle dont le pouvoir prédictif est élevé et dont les prédictions sont stables. 

### Le _bagging_ (Bootstrap Aggregating)

Présenter le _bagging_ en reprenant des éléments du chapitre 10 de https://bradleyboehmke.github.io/HOML.
Mettre une description de l'algorithme en pseudo-code?

- Présentation avec la figure en SVG;
- Illustration avec un cas d'usage de classification en deux dimensions.

![Représentation schématique d'un algorithme de _bagging_](/figures/bagging.svg){#fig-bagging}

Le _bagging_, ou _Bootstrap Aggregating_ (@breiman1996bagging), est une méthode ensembliste qui comporte trois étapes principales:

- __Tirage de sous-échantillons aléatoires__: À partir du jeu de données initial, plusieurs sous-échantillons sont générés par échantillonnage aléatoire avec remise (_bootstrapping_). Chaque sous-échantillon a la même taille que le jeu de données original, mais peut contenir des observations répétées, tandis que d'autres peuvent être omises.

- __Entraînement parallèle__: Un arbre est entraîné sur chaque sous-échantillon de manière indépendante. Cette technique permet un gain d'efficacité et un meilleur contrôle du surapprentissage (overfitting).

- __Agrégation des prédictions__: Les prédictions des modèles sont combinées pour produire le résultat final. En classification, la prédiction finale est souvent déterminée par un vote majoritaire, tandis qu'en régression, elle correspond généralement à la moyenne des prédictions.

Cette technique permet de diversifier les données d'entraînement en créant des échantillons variés, ce qui aide à réduire la variance et à améliorer la robustesse du modèle.

En combinant les prédictions de plusieurs modèles, le _bagging_ renforce la stabilité et la performance globale de l'algorithme, notamment en réduisant la variance des prédictions.

Avantages: pouvoir prédictif, entraînement hautement parallélisable.
Inconvénients: malgré l'échantillonnage des données, les arbres ont souvent une structure similaire car les variables hautement prédictives restent approximativement les mêmes dans les différents sous-échantillons. Ce phénomène de corrélation entre arbres est le principal frein à la puissance prédictive du _bagging_ et explique pourquoi il est très peu utilisé en pratique aujourd'hui. 


Le bagging est particulièrement efficace pour réduire la variance des modèles, ce qui les rend moins vulnérables au surapprentissage. Cette caractéristique est particulièrement utile dans les situations où la robustesse et la capacité de généralisation des modèles sont cruciales. De plus, comme le bagging repose sur des processus indépendants, l'exécution est plus plus rapide dans des environnements distribués.

Cependant, bien que chaque modèle de base soit construit indépendamment sur des sous-échantillons distincts, les variables utilisées pour générer ces modèles ne sont pas forcément indépendantes d'un modèle à l'autre. Dans le cas du bagging appliqué aux arbres de décision, cela conduit souvent à des arbres ayant une structure similaire. 

Les forêts aléatoires apportent une amélioration à cette approche en réduisant cette corrélation entre les arbres, ce qui permet d'augmenter la précision de l'ensemble du modèle.


    
### Les _random forests_

Expliquer que les _random forests_ sont une amélioration du _bagging_, en reprenant des éléments du chapitre 11 de https://bradleyboehmke.github.io/HOML/

![Représentation schématique d'un algorithme de forêt aléatoire](/figures/rf.svg){#fig-rf}


<!-- https://neptune.ai/blog/ensemble-learning-guide -->
<!-- https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/ -->

- Présentation avec la figure en SVG;
- Difficile d'illustrer avec un exemple (car on ne peut pas vraiment représenter le _feature sampling_);
- Bien insister sur les avantages des RF: 1/ faible nombre d'hyperparamètres; 2/ faible sensibilité aux hyperparamètres; 3/ limite intrinsèque à l'overfitting.

### Le _boosting_

Reprendre des éléments du chapitre 12 de https://bradleyboehmke.github.io/HOML/ et des éléments de la formation boosting.

Le *boosting* combine l'[**approche ensembliste**]{.orange} avec une [**modélisation additive par étapes**]{.orange} (*forward stagewise additive modeling*).

- Présentation;
- Avantage du boosting: performances particulièrement élevées.
- Inconvénients: 1/ nombre élevé d'hyperparamètres; 2/ sensibilité des performances aux hyperparamètres; 3/ risque élevé d'overfitting.

- Préciser qu'il est possible d'utiliser du subsampling par lignes et colonnes pour un algoithme de boosting. Ce point est abordé plus en détail dans la partie sur les hyperparamètres.

![Représentation schématique d'un algorithme de _boosting_](/figures/gb.svg){#fig-gb}








Un arbre CART (Classification And Regression Tree) est construit en utilisant une approche hiérarchique pour diviser un ensemble de données en sous-groupes de plus en plus homogènes. Intuitivement, voici comment cela se passe :

1. **Choix de la meilleure coupure** :  
   - L’arbre commence à la racine, c’est-à-dire l’ensemble complet des données.  
   - À chaque étape, on cherche la variable et la valeur de seuil qui divisent le mieux les données en deux groupes selon un critère spécifique (comme l'entropie, l'indice de Gini pour la classification, ou la variance pour la régression).  
   - L'objectif est de minimiser l’hétérogénéité (ou maximiser l’homogénéité) au sein des groupes créés par la division.

2. **Division récursive** :  
   - Une fois la meilleure coupure trouvée, les données sont séparées en deux sous-groupes : un groupe pour les observations qui satisfont la condition de la coupure, et l'autre pour celles qui ne la satisfont pas.  
   - Ce processus est répété récursivement sur chaque sous-groupe, formant ainsi de nouveaux "nœuds" dans l’arbre.

3. **Arrêt de la croissance de l’arbre** :  
   - L’arbre ne continue pas à se développer indéfiniment. La division s’arrête lorsque l’un des critères de fin est atteint, par exemple :  
     - Un nombre minimal d’observations dans un nœud.  
     - Une amélioration trop faible dans le critère de division.  
     - Une profondeur maximale spécifiée.

4. **Assignation des prédictions** :  
   - Une fois l’arbre construit, chaque feuille (nœud terminal) contient un sous-ensemble de données.  
   - Pour la classification, la classe prédominante dans une feuille est assignée comme prédiction pour toutes les observations appartenant à cette feuille.  
   - Pour la régression, la moyenne (ou médiane) des valeurs dans une feuille est utilisée comme prédiction.

**Exemple intuitif** :  
Imaginez que vous essayez de deviner si une personne préfère le café ou le thé. Vous commencez par poser une question générale, comme "Préfères-tu les boissons chaudes ?" Selon la réponse, vous posez d'autres questions plus spécifiques (comme "Ajoutes-tu du lait ?" ou "Aimes-tu les boissons amères ?"), jusqu’à ce que vous puissiez deviner leur préférence avec un haut degré de certitude.

En résumé, construire un arbre CART revient à poser des questions successives qui divisent les données de manière optimale pour parvenir à une prédiction claire et précise.