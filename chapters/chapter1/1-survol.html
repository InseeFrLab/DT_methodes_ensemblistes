<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.37">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>survol – Introduction aux méthodes ensemblistes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/chapter1/2-comparaison_GB_RF.html" rel="next">
<link href="../../chapters/chapter1/0-intro.qmd" rel="prev">
<link href="../../images/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-29e2c20b02301cfff04dc8050bf30c7e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-3e293e72bd4276b15eaf903c56998264.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/chapter1/0-intro.qmd">Survol des méthodes ensemblistes</a></li><li class="breadcrumb-item"><a href="../../chapters/chapter1/1-survol.html">Aperçu des méthodes ensemblistes</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Introduction aux méthodes ensemblistes</a> 
        <div class="sidebar-tools-main">
    <a href="../.././pdf/dt_methodes_ensemblistes.pdf" title="NMFS Open Science" class="quarto-navigation-tool px-1" aria-label="NMFS Open Science"><i class="bi bi-file-pdf-fill"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction aux méthodes ensemblistes</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/chapter1/0-intro.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Survol des méthodes ensemblistes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter1/1-survol.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Aperçu des méthodes ensemblistes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter1/2-comparaison_GB_RF.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Comparaison entre forêts aléatoires et <em>gradient boosting</em></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/chapter2/0-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Présentation formelle des algorithmes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter2/1-CART.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">La brique élémentaire: l’arbre de décision</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter2/2-bagging.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Le bagging</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter2/3-random_forest.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">La forêt aléatoire</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter2/4-boosting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Le <em>boosting</em></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/chapter3/0-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Comment bien utiliser les algorithmes?</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter3/1-preparation_donnees.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Préparation des données</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter3/2-guide_usage_RF.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Guide d’entraînement des forêts aléatoires</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#aperçu-des-méthodes-ensemblistes" id="toc-aperçu-des-méthodes-ensemblistes" class="nav-link active" data-scroll-target="#aperçu-des-méthodes-ensemblistes">Aperçu des méthodes ensemblistes</a>
  <ul class="collapse">
  <li><a href="#que-sont-les-méthodes-ensemblistes" id="toc-que-sont-les-méthodes-ensemblistes" class="nav-link" data-scroll-target="#que-sont-les-méthodes-ensemblistes">Que sont les méthodes ensemblistes?</a></li>
  <li><a href="#pourquoi-utiliser-des-méthodes-ensemblistes" id="toc-pourquoi-utiliser-des-méthodes-ensemblistes" class="nav-link" data-scroll-target="#pourquoi-utiliser-des-méthodes-ensemblistes">Pourquoi utiliser des méthodes ensemblistes?</a></li>
  <li><a href="#comment-fonctionnent-les-méthodes-ensemblistes" id="toc-comment-fonctionnent-les-méthodes-ensemblistes" class="nav-link" data-scroll-target="#comment-fonctionnent-les-méthodes-ensemblistes">Comment fonctionnent les méthodes ensemblistes?</a>
  <ul class="collapse">
  <li><a href="#sec-cart-intuition" id="toc-sec-cart-intuition" class="nav-link" data-scroll-target="#sec-cart-intuition">Le modèle de base: l’arbre de classification et de régression</a></li>
  <li><a href="#sec-rf-intuition" id="toc-sec-rf-intuition" class="nav-link" data-scroll-target="#sec-rf-intuition">Le <em>bagging</em> (Bootstrap Aggregating) et les forêts aléatoires</a></li>
  <li><a href="#sec-gb-intuition" id="toc-sec-gb-intuition" class="nav-link" data-scroll-target="#sec-gb-intuition">Le <em>gradient boosting</em></a></li>
  </ul></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/oliviermeslin/DT_methodes_ensemblistes/edit/main/chapters/chapter1/1-survol.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/oliviermeslin/DT_methodes_ensemblistes/blob/main/chapters/chapter1/1-survol.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li><li><a href="https://github.com/oliviermeslin/DT_methodes_ensemblistes/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/chapter1/0-intro.qmd">Survol des méthodes ensemblistes</a></li><li class="breadcrumb-item"><a href="../../chapters/chapter1/1-survol.html">Aperçu des méthodes ensemblistes</a></li></ol></nav></header>




<section id="aperçu-des-méthodes-ensemblistes" class="level1">
<h1>Aperçu des méthodes ensemblistes</h1>
<p><strong>Principe</strong>: cette partie propose une présentation intuitive des méthodes ensemblistes à destination des lecteurs souhaitant un aperçu du fonctionnement et des cas d’utilisation de ces méthodes. Elle ne contient aucun formalisme mathématique.</p>
<section id="que-sont-les-méthodes-ensemblistes" class="level2">
<h2 class="anchored" data-anchor-id="que-sont-les-méthodes-ensemblistes">Que sont les méthodes ensemblistes?</h2>
<p>Les approches ensemblistes désignent un ensemble d’algorithmes de <em>machine learning</em> supervisé développés depuis le début des années 1990, c’est-à-dire des méthodes statistiques permettant de prédire une variable-cible <span class="math inline">\(y\)</span> (appelée <em>target</em>) à partir d’un ensemble de variables <span class="math inline">\(\mathbf{X}\)</span> (appelées <em>features</em>). Elles peuvent par exemple être utilisées pour prédire le salaire d’un salarié, la probabilité de réponse dans une enquête, le niveau de diplôme… Au-delà de leur diversité, ces approches se définissent par un point commun: plutôt que de tenter de construire d’emblée un unique modèle très complexe et très performant, elles visent à obtenir un modèle très performant en combinant intelligemment un ensemble de modèles peu performants, appelés “apprenants faibles” (<em>weak learner</em> ou <em>base learner</em>). Le choix de ces modèles de base (des arbres de décision dans la plupart des cas) et la manière dont leurs prédictions sont combinées sont des facteurs déterminants pour la performance de ces approches. Le présent document se concentre sur les méthodes ensemblistes à base d’arbres de décisions qui sont parmi les plus utilisées en pratique.</p>
<p>On distingue <strong>deux grandes familles de méthodes ensemblistes</strong> à base d’arbres de décisions, selon qu’elles s’appuient sur des modèles de base entraînés en parallèle indépendamment les uns des autres, ou au contraire entraînés de façon séquentielle. Lorsque les modèles sont <em>entrainés en parallèle, indépendamment les uns des autres</em>, on parle de <em>bagging</em> ou de forêt aléatoire (<em>random forest</em>). Les implémentations les plus courantes des forêts aléatoires sont les <em>packages</em> <code>ranger</code> en <code>R</code> et <code>scikit-learn</code> en Python. Lorsque les modèles de base sont <em>entraînés de manière séquentielle</em>, chaque modèle de base visant à améliorer la prédiction proposée par l’ensemble des modèles de base précédents, on parle de <em>boosting</em>. Ce document aborde essentiellement le <em>gradient boosting</em>, qui est l’approche de <em>boosting</em> la plus utilisée actuellement. Les implémentations les plus courantes du <em>gradient boosting</em> sont actuellement <code>XGBoost</code>, <code>CatBoost</code> et <code>LightGBM</code>.</p>
</section>
<section id="pourquoi-utiliser-des-méthodes-ensemblistes" class="level2">
<h2 class="anchored" data-anchor-id="pourquoi-utiliser-des-méthodes-ensemblistes">Pourquoi utiliser des méthodes ensemblistes?</h2>
<p>Les méthodes ensemblistes sont particulièrement bien adaptées à de nombreux cas d’usage de la statistique publique, pour deux raisons. D’une, elles sont conçues pour s’appliquer à des <em>données tabulaires</em> (enregistrements en lignes, variables en colonnes), structure de données omniprésente dans la statistique publique. D’autre part, elles peuvent être mobilisées dans toutes les situations où le statisticien mobilise une régression linéaire ou une régression logistisque (imputation, repondération…).</p>
<p>Les méthodes ensemblistes présentent trois avantages par rapport aux méthodes économétriques traditionnelles (régression linéaire et régression logistique):</p>
<ul>
<li><p>Elles ont une <strong>puissance prédictive supérieure</strong>: alors que les méthodes traditionnelles supposent fréquemment l’existence d’une relation linéaire ou log-linéaire entre <span class="math inline">\(y\)</span> et <span class="math inline">\(\mathbf{X}\)</span>, les méthodes ensemblistes ne font quasiment aucune hypothèse sur la relation entre <span class="math inline">\(y\)</span> et <span class="math inline">\(\mathbf{X}\)</span>, et se contentent d’approximer le mieux possible cette relation à partir des données disponibles. En particulier, les modèles ensemblistes peuvent facilement modéliser des <strong>non-linéarités</strong> de la relation entre <span class="math inline">\(y\)</span> et <span class="math inline">\(\mathbf{X}\)</span> et des <strong>interactions</strong> entre variables explicatives <em>sans avoir à les spécifier explicitement</em> au préalable, alors que les méthodes traditionnelles supposent fréquemment l’existence d’une relation linéaire ou log-linéaire entre <span class="math inline">\(y\)</span> et <span class="math inline">\(\mathbf{X}\)</span>.</p></li>
<li><p>Elles nécessitent <strong>moins de préparation des données</strong>: elles ne requièrent pas de normalisation des variables explicatives et peuvent s’accommoder des valeurs manquantes (selon des techniques variables selon les algorithmes).</p></li>
<li><p>Elles sont généralement <strong>moins sensibles aux valeurs extrêmes et à l’hétéroscédasticité</strong> des variables explicatives que les approches traditionnelles.</p></li>
</ul>
<p>Elles présentent par ailleurs deux inconvénients rapport aux méthodes économétriques traditionnelles. Premièrement, bien qu’il existe désormais de multiples approches permettent d’interpétrer partiellement les modèles ensemblistes, leur interprétabilité reste moindre que celle d’une régression linéaire ou logistique. Deuxièmement, les modèles ensemblistes sont plus complexes que les approches traditionnelles, et leurs hyperparamètres doivent faire l’objet d’une optimisation, par exemple au travers d’une validation croisée. Ce processus d’optimisation est généralement plus complexe et plus long que l’estimation d’une régression linéaire ou logistique. En revanche, les méthodes ensemblistes sont relativement simples à prendre en main, et ne requièrent pas nécessairement une puissance de calcul importante.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Et par rapport au _deep learning_?">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Et par rapport au <em>deep learning</em>?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Si les approches de <em>deep learning</em> sont sans conteste très performantes pour le traitement du langage naturel, des images et du son, leur supériorité n’est pas établie pour les applications reposant sur des données tabulaires. Les comparaisons disponibles dans la littérature concluent en effet que les méthodes ensemblistes à base d’arbres sont soit plus performantes que les approches de <em>deep learning</em> (<span class="citation" data-cites="grinsztajn2022tree">Grinsztajn, Oyallon, and Varoquaux (<a href="#ref-grinsztajn2022tree" role="doc-biblioref">2022</a>)</span>, <span class="citation" data-cites="shwartz2022tabular">Shwartz-Ziv and Armon (<a href="#ref-shwartz2022tabular" role="doc-biblioref">2022</a>)</span>), soit font jeu égal avec elles (<span class="citation" data-cites="mcelfresh2024neural">McElfresh et al. (<a href="#ref-mcelfresh2024neural" role="doc-biblioref">2024</a>)</span>). Ces études ont identifié trois avantages des méthodes ensemblistes: elles sont peu sensibles aux variables explicatives non pertinentes, robustes aux valeurs extrêmes des variables explicatives, et capables d’approximer des fonctions très irrégulières. De plus, dans la pratique les méthodes ensemblistes sont souvent plus rapides à entraîner et moins gourmandes en ressources informatiques, et l’optimisation des hyperparamètres s’avère souvent moins complexe (<span class="citation" data-cites="shwartz2022tabular">Shwartz-Ziv and Armon (<a href="#ref-shwartz2022tabular" role="doc-biblioref">2022</a>)</span>).</p>
</div>
</div>
</section>
<section id="comment-fonctionnent-les-méthodes-ensemblistes" class="level2">
<h2 class="anchored" data-anchor-id="comment-fonctionnent-les-méthodes-ensemblistes">Comment fonctionnent les méthodes ensemblistes?</h2>
<p>Ce paragraphe présente d’abord le modèle de base sur lesquelles sont construites les méthodes ensemblistes à base d’arbres: l’arbre de classification et de régression (<a href="#sec-cart-intuition" class="quarto-xref">Section&nbsp;1.3.1</a>). Elle introduit ensuite les deux grandes familles de méthodes ensemblistes couvertes dans ce document: le <em>bagging</em> et les forêts aléatoires (<a href="#sec-rf-intuition" class="quarto-xref">Section&nbsp;1.3.2</a>), puis le <em>gradient boosting</em> (<a href="#sec-gb-intuition" class="quarto-xref">Section&nbsp;1.3.3</a>).</p>
<section id="sec-cart-intuition" class="level3">
<h3 class="anchored" data-anchor-id="sec-cart-intuition">Le modèle de base: l’arbre de classification et de régression</h3>
<section id="quest-ce-quun-arbre-cart" class="level4">
<h4 class="anchored" data-anchor-id="quest-ce-quun-arbre-cart">Qu’est-ce qu’un arbre CART?</h4>
<p>Le modèle de base des méthodes ensemblistes est le plus souvent un arbre de classification et de régression (CART, <span class="citation" data-cites="breiman1984cart">Breiman et al. (<a href="#ref-breiman1984cart" role="doc-biblioref">1984</a>)</span>). Un arbre CART est un algorithme prédictif assez simple avec trois caractéristiques essentielles:</p>
<ul>
<li>L’arbre partitionne l’espace des variables explicatives <span class="math inline">\(X\)</span> en régions (appelées feuilles ou <em>leaves</em>) les plus homogènes possible, au sens d’une certaine mesure de l’hétérogénéité;</li>
<li>Chaque région est définie par un ensemble de conditions, appelées régles de décision (<em>splitting rules</em>), qui portent sur les valeurs des variables explicatives (par exemple, une région peut être définie par la condition: <span class="math inline">\(age &gt; 40 \text{ et } statut = 'Cadre'\)</span>);</li>
<li>Une fois l’arbre construit, les prédictions de l’arbre pour chaque région se déduisent des données d’entraînement de façon intuitive: il s’agira de la classe la plus fréquente parmi les observations situées dans cette région dans le cas d’une classification, et de la moyenne des observations situées dans cette région dans le cas d’une régression.</li>
</ul>
<p>La structure de cet algorithme a deux conséquences importantes:</p>
<ul>
<li>L’algorithme CART ne fait <strong>aucune hypothèse <em>a priori</em> sur la relation entre <span class="math inline">\(X\)</span> et <span class="math inline">\(y\)</span></strong> et se laisse au contraire guider par les données. Par exemple, on ne suppose pas qu’il existe une relation linéaire de type <span class="math inline">\(y = \mathbf{X \beta}\)</span>.</li>
<li><strong>L’arbre final est une fonction constante par morceaux</strong>: la prédiction est identique pour toutes les observations situées dans la même région, et ne varie que d’une région à l’autre.</li>
</ul>
<p>Illustration, et représentation graphique (sous forme d’arbre et de graphique).</p>
<!-- #### Comment construit-on un arbre?

Si son principe est simple, la construction d'un arbre de décision se heurte à trois difficultés pratiques.

Première difficulté: comment trouver le partitionnement optimal en un temps raisonnable? Les arbres CART proposent une solution efficace à ce problème en faisant deux hypothèses simplificatrices. D'une part, la procédure de construction de l'arbre ne s'intéresse qu'à des critères de décision binaires très simples, mobilisant à chaque fois une seule variable et un seul seuil (exemples: $age > 40?$, $diplome = 'Licence'$...). Autrement dit, les critères complexes mobilisant des combinaisons de variables et de seuils sont exclus _a priori_. D'autre part, la construction de l'arbre se fait de façon itérative, une règle de décision à la fois: la procédure se contente à chaque étape de chercher la règle de décision qui réduit le plus l'hétérogénéité des groupes, conditionnellement aux règles de décision qui ont été choisies au préalable. Cette procédure ne garantit donc pas que l'arbre final soit optimal, mais elle permet d'obtenir rapidement un arbre raisonnablement performant.

Deuxième difficulté: comment mesurer l'homogénéité des régions? => mesure d'hétérogénéité

Troisième difficulté: à quel moment faut-il s'arrêter? => critère d'arrêt, profondeur max, pruning -->
</section>
<section id="avantages-et-limites-des-arbres-cart" class="level4">
<h4 class="anchored" data-anchor-id="avantages-et-limites-des-arbres-cart">Avantages et limites des arbres CART</h4>
<p>Les arbres CART présentent plusieurs avantages: leur principe est simple, ils sont aisément interprétables et peuvent faire l’objet de représentations graphiques intuitives. Par ailleurs, la flexibilité offerte par le partitionnement récursif assure que les arbres obtenus reflètent les corrélations observées dans les données d’entraînement.</p>
<p>Ils souffrent néanmoins de deux limites. D’une part, les arbres CART ont souvent un <strong>pouvoir prédictif faible</strong> qui en limite l’usage. D’autre part, ils sont <strong>peu robustes et instables</strong>: on dit qu’ils présentent une <strong>variance élevée</strong>. Ainsi, un léger changement dans les données (par exemple l’ajout ou la suppression de quelques observations) peut entraîner des modifications significatives dans la structure de l’arbre et dans la définition des feuilles. Les arbres CART sont notamment sensibles aux valeurs extrêmes, aux points aberrants et au bruit statistique. De plus, les prédictions des arbres CART sont sensibles à de petites fluctuations des données: celles-ci peuvent aboutir à ce qu’une partie des observations change brutalement de feuille et donc de valeur prédite.</p>
<p>Les deux familles de méthodes ensemblistes présentées ci-dessous (<em>bagging</em>, <em>random forests</em> et <em>gradient boosting</em>) combinent un grand nombre d’arbres de décision pour en surmonter les deux limites: il s’agit d’obtenir un modèle dont le pouvoir prédictif est élevé et dont les prédictions sont stables. La différence essentielle entre ces deux familles portent sur la façon dont les arbres sont entraînés.</p>
</section>
</section>
<section id="sec-rf-intuition" class="level3">
<h3 class="anchored" data-anchor-id="sec-rf-intuition">Le <em>bagging</em> (Bootstrap Aggregating) et les forêts aléatoires</h3>
<p>Le <em>bagging</em> (Bootstrap Aggregating) et les forêts aléatoires constituent une famille de méthodes ensemblistes dont le point commun est de combiner des modèles de bases qui ont été entraînés indépendamment les uns des autres.</p>
<section id="le-bagging" class="level4">
<h4 class="anchored" data-anchor-id="le-bagging">Le <em>bagging</em></h4>
<p>Le <em>bagging</em>, ou <em>Bootstrap Aggregating</em> (<span class="citation" data-cites="breiman1996bagging">Breiman (<a href="#ref-breiman1996bagging" role="doc-biblioref">1996</a>)</span>), est une méthode ensembliste qui comporte trois étapes principales:</p>
<ul>
<li><strong>Tirage de sous-échantillons aléatoires</strong>: À partir du jeu de données initial, plusieurs sous-échantillons sont générés par échantillonnage aléatoire avec remise (<em>bootstrapping</em>). Chaque sous-échantillon a la même taille que le jeu de données original, mais peut contenir des observations répétées, tandis que d’autres peuvent être omises.</li>
<li><strong>Entraînement parallèle</strong>: Un arbre est entraîné sur chaque sous-échantillon de manière indépendante. Ces arbres sont habituellement assez complexes et profonds.</li>
<li><strong>Agrégation des prédictions</strong>: Les prédictions des modèles sont combinées pour produire le résultat final. En classification, la prédiction finale est souvent déterminée par un vote majoritaire, tandis qu’en régression, elle correspond généralement à la moyenne des prédictions.</li>
</ul>
<div id="fig-bagging" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bagging-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../figures/bagging.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bagging-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Représentation schématique d’un algorithme de <em>bagging</em>
</figcaption>
</figure>
</div>
<p>La figure <a href="#fig-bagging" class="quarto-xref">Figure&nbsp;1</a> propose une représentation schématique du <em>bagging</em>: tout d’abord, on tire des sous-échantillons aléatoires des données d’entraînement. Ensuite, un arbre est entraîné sur chaque sous-échantillon. Enfin, les arbres sont combinés de façon à obtenir la prédiction finale.</p>
<ul>
<li>Illustration avec un cas d’usage de classification en deux dimensions.</li>
</ul>
<p>L’intuition qui explique l’efficacité du <em>bagging</em> est la suivante: en diversifiant les données d’entraînement par le tirage d’échantillons aléatoires, on obtient un grand nombre d’arbres différents les uns des autres et qui, pris dans leur ensemble, constituent un modèle plus prédictif et plus stable que chaque arbre pris isolément. Une image fréquemment employée pour décrire le <em>bagging</em> est celle d’un collège de juges. Chaque juge a sa propre façon de juger, qui est imparfaite et qui dépend des cas qu’il a déjà rencontrés. Il peut donc rendre une décision complètement erronée dans telle ou telle situation, rendant son verdict instable et peu fiable. Mais si le verdict repose sur l’opinion majoritaire d’un ensemble de juges différents les uns des autres, il est probable que le jugement sera plus robuste et plus fiable.</p>
<p>Le <em>bagging</em> présente donc deux avantages par rapport aux arbres CART: un pouvoir prédictif plus élevé et des prédictions plus stables. L’inconvénient du <em>bagging</em> réside dans la corrélation des arbres entre eux: malgré l’échantillonnage des données, les arbres ont souvent une structure similaire car les relations entre variables restent à peu près les mêmes dans les différents sous-échantillons. Ce phénomène de corrélation entre arbres est le principal frein à la puissance prédictive du <em>bagging</em>, et c’est pour surmonter (ou au moins minimiser) ce problème que les forêts aléatoires ont été mises au point. Le pouvoir prédictif plus élevé des forêts aléatoires explique pourquoi le <em>bagging</em> est très peu utilisé en pratique aujourd’hui.</p>
</section>
<section id="les-random-forests" class="level4">
<h4 class="anchored" data-anchor-id="les-random-forests">Les <em>random forests</em></h4>
<p>Les forêts aléatoires (<em>random forests</em>, <span class="citation" data-cites="breiman2001random">Breiman (<a href="#ref-breiman2001random" role="doc-biblioref">2001</a>)</span>) sont une variante du <em>bagging</em> qui vise à produire des modèles très performants en conciliant deux objectifs: maximiser le pouvoir prédictif des arbres pris isolément, et minimiser la corrélation entre ces arbres (le problème inhérent au <em>bagging</em>). Pour atteindre ce second objectif, la forêt aléatoire introduit une nouvelle source de variation aléatoire dans la construction des arbres: au moment de choisir une règle de décision pour diviser une région en deux sous-régions, la procédure d’entraînement ne considère qu’<strong>un sous-ensemble de variables sélectionnées aléatoirement</strong>, et non toutes les variables. Cette randomisation supplémentaire a pour effet mécanique d’aboutir à des arbres plus diversifiés (parce que des arbres différents ne peuvent pas mobiliser les mêmes variables au même moment) et donc de <strong>réduire la corrélation entre arbres</strong>, ce qui permet d’améliorer la performance et la stabilité du modèle agrégé. Un enjeu important de l’entraînement d’une forêt aléatoire est l’arbitrage entre puissance prédictive des arbres et corrélation entre arbres.</p>
<div id="fig-rf" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../figures/rf.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Représentation schématique d’un algorithme de forêt aléatoire
</figcaption>
</figure>
</div>
<p>La figure <a href="#fig-rf" class="quarto-xref">Figure&nbsp;2</a> propose une représentation schématique d’une forêt aléatoire. La logique d’ensemble est identique à celle du <em>bagging</em>: combinés de façon à obtenir la prédiction finale. La seule différence est que la liste des variables utilisables pour construire des règles de décision varie à chaque étape de l’entraînement. Cette restriction de la liste des variables considérées permet de réduire l’utilisation des variables les plus prédictives et de mieux mobiliser l’information disponible dans les variables peu corrélées avec <span class="math inline">\(y\)</span>.</p>
<p>Contrairement au <em>bagging</em>, les forêts aléatoires sont un algorithme qui est très largement employé pour plusieurs raisons: les forêts aléatoires ont un faible nombre d’hyperparamètres, sont généralement peu sensibles aux valeurs de ces hyperparamètres et proposent de bonnes performances avec les valeurs par défaut. Les forêts aléatoires sont toutefois sujettes au problème de surapprentissage (voir encadré), bien que dans une mesure moindre que le <em>gradient boosting</em>.</p>
<p>Les forêts aléatoires présentent également un avantage de taille: <strong>il est possible d’évaluer la qualité d’une forêt aléatoire en utilisant les données sur lesquelles elle a été entraînée</strong>, sans avoir besoin d’un jeu de test séparé. En effet, lors de la construction de chaque arbre, l’échantillonnage aléatoire implique que certaines observations ne sont pas utilisées pour entraîner cet arbre; ces observations sont dites <em>out-of-bag</em>. On peut donc construire pour chaque observation une prédiction qui agrège uniquement les arbres pour lesquels cette observation est <em>out-of-bag</em>; cette prédiction n’est pas affectée par le surapprentissage. De cette façon, il est possible d’évaluer correctement la performance de la forêt aléatoire.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Qu'est-ce que le surapprentissage?">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Qu’est-ce que le surapprentissage?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Le surapprentissage (<em>overfitting</em>) est un phénomène fréquent en <em>machine learning</em> où un modèle apprend non seulement les relations sous-jacentes entre la variable cible et les variables explicatives, mais également le bruit présent dans les données d’entraînement. En capturant ces fluctuations aléatoires plutôt que les tendances générales, le modèle affiche une performance excellente mais trompeuse sur les données d’entraînement, et s’avère médiocre sur des données nouvelles ou de test, car il ne parvient pas à généraliser efficacement.</p>
</div>
</div>
<!-- https://neptune.ai/blog/ensemble-learning-guide -->
<!-- https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/ -->
</section>
</section>
<section id="sec-gb-intuition" class="level3">
<h3 class="anchored" data-anchor-id="sec-gb-intuition">Le <em>gradient boosting</em></h3>
<p>Alors que les forêts aléatoires construisent un ensemble d’arbres complexes et indépendants les uns des autres, le <em>gradient boosting</em> adopte une autre approche, dans laquelle les arbres sont peu complexes et entraînés de façon séquentielle, chaque arbre essayant d’améliorer la prédiction proposée par l’ensemble des arbres précédents. Bien qu’elles ressemblent fortement aux forêts aléatoires en apparence, il est important de noter que les approches de <em>boosting</em> reposent sur des fondements théoriques très différents. La logique du <em>gradient boosting</em> est illustrée par la figure <a href="#fig-gb" class="quarto-xref">Figure&nbsp;3</a>:</p>
<div id="fig-gb" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gb-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../figures/gb.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gb-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Représentation schématique d’un algorithme de <em>gradient boosting</em>
</figcaption>
</figure>
</div>
<ul>
<li>Un premier modèle simple et peu performant est entraîné sur les données.</li>
<li>Un deuxième modèle est entraîné de façon à corriger les erreurs du premier modèle (par exemple en pondérant davantage les observations mal prédites);</li>
<li>Ce processus est répété en ajoutant des modèles simples, chaque modèle corrigeant les erreurs commises par l’ensemble des modèles précédents;</li>
<li>Tous ces modèles sont finalement combinés (souvent par une somme pondérée) pour obtenir un modèle complexe et performant.</li>
</ul>
<p>Il s’avère que le <em>gradient boosting</em> offre des performances prédictives particulièrement élevées. Toutefois, cet avantage incontestable ne doit pas masquer les sérieux inconvénients de cette approche: les algorithmes de <em>gradient boosting</em> comprennent un nombre élevé d’hyperparamètres et sont plus sensibles que les forêts aléatoires aux valeurs de ces hyperparamètres. Par ailleurs, ces algorithmes se caractérisent par un risque élevé de surapprentissage, et sont assez sensibles au bruit statistique et aux éventuelles erreurs sur <span class="math inline">\(y\)</span>. Par conséquent, l’usage de ces algorithmes est plus délicat, et l’optimisation de leurs hyperparamètres est une étape importante qui peut prendre un certain temps et demande une bonne connaissance des algorithmes.</p>
<!-- ::: {.callout-note title="Qu'est-ce qu'on fait de cette partie?"}

Un arbre CART (Classification And Regression Tree) est construit en utilisant une approche hiérarchique pour diviser un ensemble de données en sous-groupes de plus en plus homogènes. Intuitivement, voici comment cela se passe :

1. **Choix de la meilleure coupure** :  
   - L’arbre commence à la racine, c’est-à-dire l’ensemble complet des données.  
   - À chaque étape, on cherche la variable et la valeur de seuil qui divisent le mieux les données en deux groupes selon un critère spécifique (comme l'entropie, l'indice de Gini pour la classification, ou la variance pour la régression).  
   - L'objectif est de minimiser l’hétérogénéité (ou maximiser l’homogénéité) au sein des groupes créés par la division.

2. **Division récursive** :  
   - Une fois la meilleure coupure trouvée, les données sont séparées en deux sous-groupes : un groupe pour les observations qui satisfont la condition de la coupure, et l'autre pour celles qui ne la satisfont pas.  
   - Ce processus est répété récursivement sur chaque sous-groupe, formant ainsi de nouveaux "nœuds" dans l’arbre.

3. **Arrêt de la croissance de l’arbre** :  
   - L’arbre ne continue pas à se développer indéfiniment. La division s’arrête lorsque l’un des critères de fin est atteint, par exemple :  
     - Un nombre minimal d’observations dans un nœud.  
     - Une amélioration trop faible dans le critère de division.  
     - Une profondeur maximale spécifiée.

4. **Assignation des prédictions** :  
   - Une fois l’arbre construit, chaque feuille (nœud terminal) contient un sous-ensemble de données.  
   - Pour la classification, la classe prédominante dans une feuille est assignée comme prédiction pour toutes les observations appartenant à cette feuille.  
   - Pour la régression, la moyenne (ou médiane) des valeurs dans une feuille est utilisée comme prédiction.

**Exemple intuitif** :  
Imaginez que vous essayez de deviner si une personne préfère le café ou le thé. Vous commencez par poser une question générale, comme "Préfères-tu les boissons chaudes ?" Selon la réponse, vous posez d'autres questions plus spécifiques (comme "Ajoutes-tu du lait ?" ou "Aimes-tu les boissons amères ?"), jusqu’à ce que vous puissiez deviner leur préférence avec un haut degré de certitude.

En résumé, construire un arbre CART revient à poser des questions successives qui divisent les données de manière optimale pour parvenir à une prédiction claire et précise.

::: -->



</section>
</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-breiman1996bagging" class="csl-entry" role="listitem">
Breiman, Leo. 1996. <span>“Bagging Predictors.”</span> <em>Machine Learning</em> 24: 123–40.
</div>
<div id="ref-breiman2001random" class="csl-entry" role="listitem">
———. 2001. <span>“Random Forests.”</span> <em>Machine Learning</em> 45: 5–32.
</div>
<div id="ref-breiman1984cart" class="csl-entry" role="listitem">
Breiman, Leo, Jerome Friedman, Richard Olshen, and Charles Stone. 1984. <span>“Cart.”</span> <em>Classification and Regression Trees</em>.
</div>
<div id="ref-grinsztajn2022tree" class="csl-entry" role="listitem">
Grinsztajn, Léo, Edouard Oyallon, and Gaël Varoquaux. 2022. <span>“Why Do Tree-Based Models Still Outperform Deep Learning on Typical Tabular Data?”</span> <em>Advances in Neural Information Processing Systems</em> 35: 507–20.
</div>
<div id="ref-mcelfresh2024neural" class="csl-entry" role="listitem">
McElfresh, Duncan, Sujay Khandagale, Jonathan Valverde, Vishak Prasad C, Ganesh Ramakrishnan, Micah Goldblum, and Colin White. 2024. <span>“When Do Neural Nets Outperform Boosted Trees on Tabular Data?”</span> <em>Advances in Neural Information Processing Systems</em> 36.
</div>
<div id="ref-shwartz2022tabular" class="csl-entry" role="listitem">
Shwartz-Ziv, Ravid, and Amitai Armon. 2022. <span>“Tabular Data: Deep Learning Is Not All You Need.”</span> <em>Information Fusion</em> 81: 84–90.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/github\.com\/oliviermeslin\/DT_methodes_ensemblistes");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../chapters/chapter1/0-intro.qmd" class="pagination-link" aria-label="Survol des méthodes ensemblistes">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Survol des méthodes ensemblistes</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/chapter1/2-comparaison_GB_RF.html" class="pagination-link" aria-label="Comparaison entre forêts aléatoires et _gradient boosting_">
        <span class="nav-page-text">Comparaison entre forêts aléatoires et <em>gradient boosting</em></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© CC-1.0</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/oliviermeslin/DT_methodes_ensemblistes/edit/main/chapters/chapter1/1-survol.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/oliviermeslin/DT_methodes_ensemblistes/blob/main/chapters/chapter1/1-survol.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li><li><a href="https://github.com/oliviermeslin/DT_methodes_ensemblistes/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This page is built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>