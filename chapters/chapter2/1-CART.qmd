# La brique élémentaire: l'arbre de décision

Les arbres de décision sont des outils puissants en apprentissage automatique, utilisés notamment pour des tâches de classification et de régression. Le principe de cet algorithme consiste à diviser l'espace des caractéristiques en sous-régions homogènes à l'aide de règles simples, puis de former pour chaque sous-ensemble une prédiction à partir des observations présentes dans cette sous-région. Malgré leur simplicité apparente, les arbres de décision sont capable de saisir des relations complexes et non linéaires entre les variables (ou _caractéristiques_) d'un jeu de données.

## Le principe fondamental: partitionner pour prédire

Imaginez que vous souhaitiez prédire le prix d'une maison en fonction de sa superficie et de son nombre de pièces, à partir d'un ensemble de transactions pour lesquelles le prix est connu. L'espace des caractéristiques (superficie et nombre de pièces) est vaste, et les prix des maisons (la _réponse_ à prédire) sont très variables. L'idée centrale des arbres de décision est de diviser cet espace en zones plus petites, au sein desquelles les maisons ayant des surfaces et un nombre de pièces similaire ont des prix proches, et d'attribuer une prédiction identique à toutes les maisons situées dans la même zone.

### Les défis du partitionnement optimal

L'objectif principal est de trouver la partition de l'espace des caractéristiques qui offre les meilleures prédictions possibles. Cependant, cet objectif se heurte à plusieurs difficultés, et la complexité du problème augmente rapidement avec le nombre de caractéristiques et la taille de l'échantillon:

- **Infinité des découpages possibles**: Il existe une infinité de façons de diviser l'espace des caractéristiques;

- **Complexité de la paramétrisation**: Il est difficile de représenter tous ces découpages avec un nombre limité de paramètres;

- **Optimisation complexe**: Même avec une paramétrisation, trouver le meilleur découpage nécessite une optimisation complexe, souvent irréaliste en pratique.

### Les solutions apportées par les arbres de décision

Pour surmonter ces difficultés, les algorithmes d'arbres de décision, et notamment la plus célèbre, l'algorithme CART (Classification And Regression Tree, @breiman1984cart), procèdent à trois simplifications cruciales:

   1. **Optimisation gloutonne (__greedy optimization__)**: plutôt que de rechercher d'emblée un partitionnement optimal, les arbres de décision partitionnent l'espace selon une approche séquentielle. A chaque étape, l'arbre choisit la meilleure division possible d'une région en deux sous-régions, _indépendamment des étapes précédentes ou suivantes_. Ce processus est répété pour chaque sous-région, ce qui permet d'affiner progressivement le partitionnement de l'espace, jusqu'à ce qu'un critère d'arrêt soit atteint. Cette méthode dite "gloutonne" (_greedy_) s'avère très efficace, car elle décompose un problème d'optimisation complexe en une succession de problèmes plus simples et plus rapides à résoudre. Le résultat obtenu n'est pas nécessairement un optimum global, mais il s'en approche raisonnablement et surtout rapidement.

   2. **Simplification des règles de partitionnement**: au lieu d'explorer tous les règles de décision possibles, les arbres de décision se restreignent à des règles de décision très simples, appelés **découpages binaires** (_binary splits_): à chaque étape, l'algorithme divise chaque région de l'espace en deux sous-régions à l'aide d'une règle de décision (_decision rule_) qui ne fait appel qu'à __une seule caractéristique__ (ou _variable_) et à __un seul seuil__ (ou _critère_) pour cette segmentation. Cela revient à poser une question simple telle que: "La valeur de la caractéristique $X$ dépasse-t-elle le seuil $x$ ?" Par exemple: "La superficie de la maison est-elle supérieure à 100 m² ?". Les deux réponses possibles ("Oui" ou "Non") définissent deux nouvelles sous-régions distinctes de l'espace, chacune correspondant à un sous-ensemble de données plus homogènes.

   3. **Simplicité des prédictions locales**: une fois le partitionnement réalisé, une prédiction est calculée pour chaque région à partir des observations des données d'entraînement présentes dans cette région. Il s'agit souvent de la moyenne des valeurs cibles dans cette région (régression) ou de la classe majoritaire (classification). Un point essentiel est que la prédiction est constante au sein de chaque région.

<!-- En raison de leur nature **non-continue** et **non-différentiable**, il est impossible d'utiliser des méthodes d'optimisation classiques reposant sur le calcul de gradients. -->


### Terminologie et structure d'un arbre de décision

Cet algorithme est appelé __arbre de décision__ (_decision tree_) en raison provient de la structure arborescente en forme d'arbre inversé qui apparaît lorsqu'on en fait une représentation graphique. Plus généralement, les principaux éléments qui composent les arbres de décision sont désignés par des termes issus du champ lexical des arbres:

- **Nœud Racine (_Root Node_)**: Le nœud-racine est le point de départ de l'arbre de décision, il est situé au sommet de l'arbre. Il contient l'ensemble des données d'entraînement avant tout partitionnement. À ce niveau, l'algorithme cherche la caractéristique la plus discriminante, c'est-à-dire celle qui permet de diviser les données en deux régions de manière à minimiser un certain critère d'hétérogénéité (comme l'indice de Gini pour la classification ou la variance pour la régression).

- **Nœuds Internes (_Internal Nodes_)**: Les nœuds internes sont les points intermédiaires où l'algorithme CART applique des règles de décision pour diviser les données en sous-régions plus petites. Chaque nœud interne se définit par une règle de décision basée sur une variable et un seuil (par exemple, "La superficie de la maison est-elle supérieure à 100 m² ?"). À chaque étape, une seule caractéristique (la superficie) et un seul seuil (supérieur à 100) sont utilisés pour opérer au partitionnement des données.

- **Branches (_Branches_)**:  Les branches sont les connexions entre les nœuds et représentent le chemin suivies par les données. Chaque branche correspond à une décision binaire, "Oui" ou "Non", qui oriente les observations vers une nouvelle subdivision de l'espace des caractéristiques.
  
- **Nœuds Terminaux ou Feuilles (_Leaf Nodes_,  _Terminal Nodes_ ou _Leaves_)**: Les nœuds terminaux, situés à l'extrémité des branches, sont les points où le processus de division s'arrête. Ils fournissent la prédiction finale. Dans un problème de classification, la prédiction d'une feuille est soit la classe majoritaire parmi les observations de la feuille (par exemple, "Oui" ou "Non"), soit une probabilité d'appartenir à chaque classe. Dans un problème de régression, la prédiction d'une feuille est une valeur numérique, souvent la moyenne des observations de la feuille.
    
_Figure illustrative_: Une représentation visuelle de la structure de l'arbre peut être utile ici pour illustrer les concepts de nœuds, branches et feuilles.




### Illustration

Supposons que nous souhaitions prédire le prix d'une maison en fonction de sa superficie et de son nombre de pièces. Un arbre de décision pourrait procéder ainsi:

1. **Première division**: "La superficie de la maison est-elle supérieure à 100 m² ?"
   - Oui: Aller à la branche de gauche.
   - Non: Aller à la branche de droite.
2. **Deuxième division (branche de gauche)**: "Le nombre de pièces est-il supérieur à 4 ?"
   - Oui: Prix élevé (par exemple, plus de 300 000 €).
   - Non: Prix moyen (par exemple, entre 200 000 € et 300 000 €).
3. **Deuxième division (branche de droite)**: "Le nombre de pièces est-il supérieur à 2 ?"
   - Oui: Prix moyen (par exemple, entre 150 000 € et 200 000 €).
   - Non: Prix bas (par exemple, moins de 150 000 €).

Cet arbre utilise des règles simples pour diviser l'espace des caractéristiques (superficie et nombre de pièces) en sous-groupes homogènes et fournir une prédiction (estimer le prix d'une maison).

_Figure illustrative_


## La construction d'un arbre de décision par l'algorithme CART

### Définir une fonction d'impureté adaptée au problème

La **fonction d'impureté** quantifie l'hétérogénéité des observations au sein d'un nœud par rapport à la variable cible (classe pour la classification, ou valeur continue pour la régression). Plus précisément, une mesure d'impureté est conçue pour croître avec la dispersion dans un nœud: plus un nœud est homogène, plus son impureté est faible. Un nœud est dit **pur** lorsque toutes les observations qu'il contient appartiennent à la même classe (classification) ou présentent des valeurs similaires voire identiques (régression). Le choix de la fonction d'impureté dépend du type de problème (voir ci-dessous).

<!-- - **Classification**: L'**indice de Gini** ou l'**entropie** sont très souvent utilisées pour évaluer la dispersion des classes dans chaque nœud. 

- **Régression**: La **somme des erreurs quadratiques** (SSE) est souvent utilisée pour mesurer la variance des valeurs cibles dans chaque nœud.  -->

**La fonction d'impureté est un élément essentiel de la construction des arbres de décision.** En effet, c'est elle qui est utilisée pour comparer entre elles les règles de décision possibles. À chaque étape de la croissance de l'arbre (_tree growing_), l'algorithme sélectionne la règle de décision qui réduit le plus l'impureté, afin de définir des nœuds les plus homogènes possibles. Il est à noter que l'arbre final dépend de la fonction d'impureté utilisée: si pour un problème donné on recommence l'entraînement avec une autre fonction d'impureté, on obtient généralement un arbre différent (car les règles de décision retenues à chaque nœud ne sont plus les mêmes).

#### Mesures d'impureté pour les problèmes de classification

Dans un problème de classification où l'on souhaite classifier des observations parmi $K$ classes, une **mesure d'impureté** $I(t)$ est une fonction qui quantifie l'hétérogénéité des classes dans un nœud donnée. Les mesures d'impureté usuelles détaillées ci-dessous partagent les deux propriétés suivantes:

- **Pureté maximale**: lorsque toutes les observations du nœud appartiennent à une seule classe, c'est-à-dire que la proportion $p_k = 1$ pour une classe $k$ et $p_j = 0$ pour toutes les autres classes $j \neq k$, l'impureté est minimale et $I(t) = 0$. Cela indique que le nœud est **entièrement pur**, ou homogène.

- **Impureté maximale**: lorsque les observations sont réparties de manière uniforme entre toutes les classes, c'est-à-dire que la proportion $p_k = \frac{1}{K}$ pour chaque classe $k$, l'impureté atteint son maximum. Cette situation reflète une **impureté élevée**, car le nœud est très hétérogène et contient une forte incertitude sur la classe des observations.

Il existe trois mesures d'impureté couramment utilisées en classification:


**1. L'indice de Gini**

L'**indice de Gini** mesure la probabilité qu'un individu sélectionné au hasard dans un nœud soit mal classé si on lui attribue une classe au hasard, en fonction de la distribution des classes dans ce nœud. Pour un nœud $t$ contenant $K$ classes, l'indice de Gini $G(t)$ est donné par

$$ G(t) = 1 - \sum_{k=1}^{K} p_k^2 $$

où $p_k$ est la proportion d'observations appartenant à la classe $k$ dans le nœud $t$.

**Critère de choix**:
L'indice de Gini est très souvent utilisé parce qu'il est simple à calculer et capture bien l'homogénéité des classes au sein d'un nœud. Il privilégie les partitions où une classe domine fortement dans chaque sous-ensemble.

**2. L'entropie (ou entropie de Shannon)**

L'**entropie** est une autre mesure de l'impureté utilisée dans les arbres de décision. Elle mesure la quantité d'incertitude ou de désordre dans un nœud, en s'appuyant sur la théorie de l'information.

Pour un nœud $t$ contenant $K$ classes, l'entropie $E(t)$ est définie par:

$$
E(t) = - \sum_{k=1}^{K} p_k \log(p_k)
$$

où $p_k$ est la proportion d'observations de la classe $k$ dans le nœud $t$.

<!-- **Propriété**:

- Comme pour l'indice de Gini, si toutes les observations d'un nœud appartiennent à la même classe, l'entropie est nulle ($E(t) = 0$), indiquant un nœud pur.

- L'entropie atteint son maximum lorsque les observations sont uniformément réparties entre les classes, reflétant une grande incertitude dans la classification.-->

**Critère de choix**:
L'entropie a tendance à être plus sensible aux changements dans les distributions des classes que l'indice de Gini, car elle attribue un poids plus élevé aux événements rares (valeurs de $p_k$ très faibles). Elle est souvent utilisée lorsque l'erreur de classification des classes minoritaires est particulièrement importante.

**3. Taux d'erreur**

Le **taux d'erreur** est une autre mesure de l'impureté parfois utilisée dans les arbres de décision. Il représente la proportion d'observations mal classées dans un nœud.

Pour un nœud $t$, le taux d'erreur $\text{TE}(t)$ est donné par:

$$
\text{TE}(t) = 1 - \max(p_k)
$$

où $\max(p_k)$ est la proportion d'observations appartenant à la classe majoritaire dans le nœud.

<!-- **Propriété**:

- Si toutes les observations d'un nœud appartiennent à la même classe, le taux d'erreur est nul ($\text{TE}(t) = 0$), indiquant un nœud pur.

- Le taux d'erreur atteint son maximum lorsque les observations sont uniformément réparties entre les classes, reflétant une grande incertitude dans la classification.-->

**Critère de choix**:
Bien que le taux d'erreur soit simple à comprendre, il est moins souvent utilisé dans la construction des arbres de décision parce qu'il est moins sensible que l'indice de Gini ou l'entropie aux petits changements dans la distribution des classes.

#### Mesures d'impureté pour les problèmes de régression

<!-- Dans un problème de classification où l'on souhaite classifier des observations parmi $K$ classes, une **mesure d'impureté** $I(t)$ est une fonction qui quantifie l'hétérogénéité des classes dans un nœud donnée. Les mesures d'impureté usuelles détaillées ci-dessous partagent les deux propriétés suivantes: -->

Dans les problèmes de régression, l'objectif est de partitionner les données de manière à réduire au maximum la variabilité des valeurs au sein de chaque sous-région. Pour mesurer cette variabilité, la fonction d'impureté la plus couramment employée est la somme des erreurs quadratiques (SSE). Elle évalue l'impureté d'une région en quantifiant à quel point les valeurs de cette région s'écartent de la moyenne locale. Pour un nœud $t$, contenant $N$ observations avec des valeurs $y_i$, la SSE est donnée par:

$$
\text{SSE}(t) = \sum_{i=1}^{N} (y_i - \hat{y})^2
$$

où $\hat{y}$ est la moyenne des valeurs $y_i$ dans le nœud.

Cette fonction d'impureté a des propriétés similaires à celles présentées pour la classification: si toutes les valeurs de $y_i$ dans un nœud sont proches de la moyenne $\hat{y}$, la SSE sera faible, indiquant une homogénéité élevée dans le nœud. Inversement, une SSE élevée indique une grande variabilité dans les valeurs, donc un nœud impur. Une limite de cette mesure d'impureté est qu'elle est particulièrement sensible aux écarts élevés entre les valeurs observées et la moyenne prédite, et donc aux valeurs extrêmes. En cherchant à minimiser la SSE, les modèles visent à former des nœuds dans lesquels les valeurs des observations sont aussi proches que possible de la moyenne locale. 


### Identifier la partition maximisant la réduction de l'impureté

Une fois la mesure d'impureté définie, l'algorithme CART construit séquentiellement le partitionnement de l'espace des caractéristiques en examinant les règles de décision possibles. À chaque nœud, et pour chaque caractéristique, il cherche à identifier le **seuil optimal**, c'est-à-dire le seuil qui minimise le plus efficacement l'impureté des deux sous-ensembles générés. L'algorithme compare ensuite toutes les divisions potentielles (caractéristiques et seuils optimaux associés à chaque nœud) et sélectionne celle qui entraîne la réduction maximale de l'impureté. 

Prenons l'exemple d'une caractéristique continue, telle que la superficie d'une maison:

- Si l'algorithme teste la règle "Superficie > 100 m²", il calcule la fonction de perte pour les deux sous-ensembles générés par cette règle ("Oui" et "Non").

- Ce processus est répété pour différentes valeurs seuils afin de trouver la partition qui minimise le plus efficacement l’impureté au sein des sous-ensembles.


### Réitérer le processus jusqu'à atteindre un critère d'arrêt

L'algorithme CART poursuit le partitionnement de l'espace des caractéristiques en appliquant de manière récursive les mêmes étapes: identification de la caractéristique et du seuil optimal pour chaque nœud, puis sélection du partitionnement binaire qui maximise la réduction de l'impureté. Ce processus est répété jusqu'à ce qu'un **critère d'arrêt** soit atteint, par exemple:

- **Profondeur maximale de l'arbre**: Limiter le nombre de divisions successives pour éviter un arbre trop complexe.
- **Nombre minimum d'observations par feuille**: Empêcher la création de feuilles contenant très peu d'observations, ce qui réduirait la capacité du modèle à généraliser.
- **Réduction minimale de l'impureté à chaque étape**


### Elagage (_pruning_)


### Prédire

Une fois l'arbre construit, la prédiction pour une nouvelle observation s'effectue en suivant les branches de l'arbre, en partant du nœud racine jusqu'à un nœud terminal (ou feuille). À chaque nœud interne, une décision est prise en fonction des valeurs des caractéristiques de l'observation, ce qui détermine la direction à suivre vers l'un des sous-ensembles. Ce cheminement se poursuit jusqu'à ce que l'observation atteigne une feuille, où la prédiction finale est effectuée.

- En **classification**, la classe attribuée est celle majoritaire dans la feuille atteinte.
- En **régression**, la valeur prédite est généralement la moyenne des valeurs cibles des observations dans la feuille.


### Critères de qualité et ajustements 

Pour améliorer la performance de l'arbre, on peut ajuster les hyperparamètres tels que la profondeur maximale ou le nombre minimum d'observations dans une feuille. De plus, des techniques comme la **prédiction avec arbres multiples** (bagging, forêts aléatoires) permettent de surmonter les limites des arbres individuels, souvent sujets au surapprentissage.


## Avantages et limites de cette approche

### Avantages

- **Interprétabilité**: Les arbres de décision sont faciles à comprendre et à visualiser.
- **Simplicité**: Pas besoin de transformations complexes des données.
- **Flexibilité**: Ils peuvent gérer des caractéristiques numériques et catégorielles, ainsi que les valeurs manquantes.
- **Gestion des interactions**: Modèles non paramétriques, pas d'hypothèses sur les lois par les variables. Ils capturent naturellement les interactions entre les caractéristiques.

### Limites

- **Surapprentissage**: Les arbres trop profonds peuvent surapprendre les données d'entraînement.
- **Optimisation locale**: L'approche gloutonne peut conduire à des solutions sous-optimales globalement (optimum local).
- **Stabilité**: De petits changements dans les données peuvent entraîner des changements significatifs dans la structure de l'arbre (manque de robustesse).




