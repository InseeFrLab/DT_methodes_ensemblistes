## Sujets avancés {#sec-sujets-avances}

### Le traitement des variables manquantes {#sec-missing-values}

A COMPLETER

### Le traitement des variables continues: l'utilisation des histogrammes {#sec-continuous-variables-hist}

__L'algorithme de détermination des critères de partition (_split-finding algorithm_) est un enjeu de performance essentiel dans les méthodes ensemblistes.__ En effet, l'algorithme le plus simple qui consiste à énumérer tous les critères de partition possibles (en balayant toutes les valeurs de toutes les variables) s'avère très coûteux à utiliser dès lors que les données contiennent soit un grand nombre de variables, soit des variables continues prenant un grand nombre de valeurs. C'est pourquoi cet algorithme a fait l'objet de multiples améliorations et optimisations visant à réduire leur coût computationnel sans dégrader la qualité des critères de partition.

L'utilisation d'histogrammes (_histogram-based algorithms_) est une approche efficace qui permet de réduire de manière significative le coût computationnel lié à la recherche des _splits_ optimaux en discrétisant les variables continues. Elle est proposée par toutes les implémentations courantes du _gradient boosting_ (XGBoost, LightGBM, CatBoost et scikit-learn). Elle comprend deux caractéristiques principales:

- __Discrétisation__: avant le début de l'entraînement, chaque variable continue est discrétisée en un nombre limité d'intervalles (_bins_), construits le plus souvent à partir de ses quantiles. Ce processus est appelé *binning*. Par exemple, une variable continue uniformément distribuée de 0 à 100 peut être divisée en dix intervalles ($[0, 10), [10, 20), \dots, [90, 100)$). Le nombre maximal de _bins_ est un hyperparamètre qui peut parfois jouer un rôle important.
<!-- - __Construction de l'histogramme__: après la discrétisation, un histogramme est construit pour chaque variable continue; il résume combien d'observations appartiennent à chaque _bin_, ainsi que la somme des gradients et la somme des hessiennes de ces observations (il s'agit des quantités $\sum_{i \in \text{bin}} g_i$ et $\sum_{i \in \text{bin}} h_i$ mentionnées dans l'équation @eq-w-j-optimal. -->
- __Énumération restreinte__: l'algorithme de détermination des critères de partition ne considère que les bornes des intervalles précédemment définies (10, 20, 30, etc. dans l'exemple précédent) et non l'ensemble des valeurs prises par les variables continues. Cette modification se traduit par une nette accélération de l'entraînement, dans la mesure où le nombre de _bins_ est en général beaucoup plus faible que le nombre de valeurs uniques des variables continues.
<!-- - __Mise à jour de l'histogramme__: G et H sont mis à jour après chaque arbre. -->

### Le traitement des variables catégorielles {#sec-categorical-variables-encoding}

<!-- IL NE FAUT PAS SUPPRIMER CE COMMENTAIRE. -->
<!-- Faire en sorte que la police Font Awesome soit chargée -->
<!-- {{< fa circle-xmark >}} -->
<!-- IL NE FAUT PAS SUPPRIMER CE COMMENTAIRE. -->

#### Une diversité d'approches

A l'origine, les arbres CART ont été conçus pour partitionner l'espace en utilisant exclusivement des variables numériques, et ne pouvaient pas mobiliser les variables catégorielles sans un retraitement préalable. Plusieurs __méthodes d'encodage__ (_one-hot-encoding_, _ordinal encoding_, _target encoding_) ont donc été développées pour surmonter ce problème en transformant les variables catégorielles en variables numériques; elles ne sont d'ailleurs pas spécifiques aux méthodes ensemblistes à base d'arbres. Depuis le milieu des années 2010, une nouvelle approche efficace et spécifique aux méthodes ensemblistes a été introduite dans les implémentations du _gradient boosting_ (_native support for categorical features_). Ce paragraphe présente ces différentes approches des variables catégorielles, et précise quelle approche est proposée par les différentes implémentations.

Les trois approches d'encodage les plus courantes sont les suivantes:

- Le **_one-hot encoding_** consiste à transformer une variable catégorielle en une série de variables binaires qui représentent chacune une modalité de la variable; pour chaque observation, seule la colonne correspondant à la modalité de la variable catégorielle aura la valeur 1, et toutes les autres auront la valeur 0. Cette approche permet de représenter des catégories de manière numérique de façon très simple et sans leur attribuer un ordre. Toutefois, le _one-hot encoding_ augmente fortement la dimensionnalité des données (ce qui ralentit l'entraînement et augmente les besoins en mémoire), et est inutilisable lorsque les variables catégorielles présentent un nombre de modalités[^note_OHE].

- L'**_ordinal encoding_** consiste à attribuer un entier unique à chaque modalité d'une variable catégorielle. Par exemple, la catégorie "Sans diplôme" sera encodée par la valeur 0, la catégorie "Baccalauréat ou moins" sera encodée par 1, etc. Simple à mettre en œuvre, cette approche permet de remplacer la variable catégorielle par une unique variable numérique et est donc utile pour traiter les variables présentant un grand nombre de modalités, pour lesquelles le _one-hot encoding_ est impraticable. Elle est particulièrement adaptée aux variables catégorielles qui sont naturellement ordonnées (exemples: niveau de diplôme, catégorie d'âge, étage d'un appartement...). En revanche, cette approche est peu adaptée aux variables non ordonnées (exemples: secteur d'activité, département, pays...) car elle introduit un ordre fictif qui peut perturber les modèles qui interprètent les entiers comme des valeurs ordonnées.

- Le **_target encoding_** consiste à remplacer chaque modalité d'une variable catégorielle par la moyenne de la variable cible pour cette modalité. Cette approche est notamment proposée par CatBoost[^note_catboost] et par scikit-learn[^note_scikit]. Comme l'_ordinal encoding_, cette approche permet d'obtenir une unique variable numérique et est donc utile pour traiter les variables présentant un grand nombre de modalités. Par ailleurs, le _target encoding_ fonctionne bien avec la méthode des histogrammes décrite précédemment, dans la mesure où les valeurs encodées sont par construction ordonnées en fonction de leur association avec la variable cible. Toutefois, il est important d'utiliser le _target encoding_ en lissant la moyenne ou en recourant à une validation croisée car il est sujet au surapprentissage. De plus, les implémentations existantes réalisent l'encodage une seule fois avant l'entraînement au niveau de l'ensemble des données d'entraînement, pas au niveau de chaque _split_; l'ordre des modalités qui résulte de l'encodage peut être peu pertinent sur certaines parties des données d'entraînement si celles-ci présentent un haut degré d'hétérogénéité.

La dernière approche, appelée __support natif des variables catégorielles__ (**_native support for categorical features_**) n'est pas un encodage et est une spécificité des implémentations du _gradient boosting_. Cette approche a été introduite par [LightGBM](https://lightgbm.readthedocs.io/en/latest/Features.html#optimal-split-for-categorical-features), puis reprise par [XGBoost](https://xgboost.readthedocs.io/en/latest/tutorials/categorical.html) et [scikit-learn](https://scikit-learn.org/stable/modules/ensemble.html#categorical-features-support). Dans cette approche, l'objectif est de déterminer le meilleur _split_ directement à partir des modalités d'une variable catégorielle, en les séparant en deux sous-ensembles (par exemple : {'A', 'B', 'C'} et {'D', 'E', 'F'} pour une variable comportant six modalités). La difficulté est qu'il est souvent impossible en pratique de trouver le partitionnement optimal par une énumération exhaustive des partitions possibles, car il existe $2^{k-1} - 1$ partitions possibles pour une variable à $k$ modalités. C'est pourquoi le support natif des variables catégorielles repose sur une autre méthode plus efficace dont l'optimalité a été démontrée par @fisher1958grouping: à chaque split, les modalités sont triées selon $-\frac{\sum_{i} g_i}{\sum_{i} h_i}$ [^note_native], puis le meilleur _split_ est choisi en testant les différentes divisions possibles de cette liste triée. Par exemple, si pour un _split_ donné les modalités sont triées dans l'ordre ABDFEC, l'algorithme examinera les $k-1$ _splits_ A|BDFEC, AB|DFEC, ABD|FEC, etc. Cette approche peut être considérée comme une variante optimisée du _target encoding_, avec deux différences notables: l'encodage des modalités se fait à partir du gradient et de la hessienne de la fonction de perte (et non à partir de $y$), et cet encodage a lieu à chaque _split_ et non une seule fois avant l'entraînement. Par ailleurs, cette approche peut rencontrer des difficultés quand les variables catégorielles comprennent un nombre très élevé des modalités (au-delà de quelques centaines).

<!-- 
https://stats.stackexchange.com/questions/501391/interpretation-of-gradient-and-hessian-for-categorical-variables-in-gradient-boo
$ \frac{\sum_{i=1}^{n} 1_{x_{i j}=x_{i k}} g_{i}}{\sum_{i=1}^{n} 1_{x_{i j}=x_{i k}} h_{i}} $
 -->

[^note_OHE]: Il est bien sûr possible de n'encoder que les modalités les plus fréquentes, et de regrouper toutes les autres dans une seule variable binaire.
[^note_catboost]: Le _target encoding_ utilisé par CatBoost est présenté en détail dans @prokhorenkova2018catboost et sur ce [billet de blog](https://blog.dataiku.com/how-do-gradient-boosting-algorithms-handle-categorical-variables).
[^note_scikit]: Voir cet [exemple](https://scikit-learn.org/dev/auto_examples/preprocessing/plot_target_encoder.html#sphx-glr-auto-examples-preprocessing-plot-target-encoder-py) dans la documentation de scikit-learn.
[^note_native]: Il s'agit de l'[équation donnant le poids optimal](4-boosting.qmd#eq-w-j-optimal) d'une feuille terminale, avec $\lambda = 0$.


#### Comparaison des différents approches

Déterminer quelle approche des variables catégorielles est adaptée dans une situation donnée n'est pas toujours aisé. Les cas d'usage des différentes approches peuvent être résumés comme ceci:

- le _one-hot-encoding_ est adapté uniquement aux variables catégorielles comprenant peu de modalités; 
- l'_ordinal encoding_ est adapté aux variables catégorielles qui sont naturellement ordonnées, et ce quel que soit le nombre de modalités (car la variable catégorielle est implicitement convertie en variable numérique);
- le _target encoding_ est adapté aux variables catégorielles comprenant un grand nombre de modalités et non naturellement ordonnées;
- le support natif des variables catégorielles est adapté à tous les types de variables catégorielles, à l'exception de celles qui comprennent un nombre très élevé des modalités (au-delà de quelques centaines).

Comparé aux autres approches, le support natif des variables catégorielles comporte plusieurs avantages: il permet de réduire le nombre de _splits_, d'obtenir des arbres plus simples et d'augmenter l'efficacité computationnelle de l'entraînement des arbres. Ces avantages apparaissent clairement avec un exemple[^note_scikit2]. Imaginons qu'on s'intéresse à une variable catégorielle prenant les modalités ABCDEF, et que sur une feuille donnée de l'arbre, le meilleur partitionnement sur cette variable soit ACF - BDE. Une approche de _one-hot-encoding_ aura besoin de trois _splits_ pour approximer ce partitionnement: un premier pour séparer A et BCDEF, un deuxième pour séparer C et BDEF et un troisième pour séparer F et BCDE (l'ordre des _splits_ pouvant différer). Une approche d'_ordinal encoding_ aura besoin de quatre _splits_: un _split_ pour isoler A, un _split_ pour isoler F, et deux splits pour isoler C (car C est au milieu de l'ordre des modalités). Enfin, une approche de _target encoding_ aura besoin d'un nombre variable de _splits_, qui dépend de l'ordre des modalités dans l'_encoding_: si le _target encoding_ a trié les modalités dans l'ordre ACFBDE, alors un unique _split_ suffira; si l'ordre est très différent, alors il faudra davantage de _splits_. Inversement, le support natif des variables catégorielles identifiera immédiatement le partitionnement optimal, et ne fera qu'un seul _split_, ce qui simplifie la structure de l'arbre et accélère l'entraînement.

<!-- Ainsi, comparées au support natif des variables catégorielles, les approches traditionnelles (_one-hot-encoding_, _ordinal encoding_ et _target encoding_) ont trois limites:

- Elles aboutissent à des arbres profonds, avec de multiples _splits_ déséquilibrés (une modalité séparée de toutes les autres);
- Les partitionnements obtenus ne sont pas forcément les plus pertinents;
- La construction des arbres est plus lente et plus coûteuse sur le plan computationnel, en raison du nombre élevé de _splits_. -->

[^note_scikit2]: Cet exemple s'appuie sur la [documentation de `scikit-learn`](https://scikit-learn.org/dev/auto_examples/ensemble/plot_gradient_boosting_categorical.html).


#### Approches intégrées aux implémentations des méthodes ensemblistes 

Certaines implémentations des méthodes ensemblistes prennent en charge directement certaines des quatre approches présentées ci-dessus, auquel cas il est possible d'entraîner le modèle sur des données contenant des variables catégorielles sans _preprocessing_ particulier; dans les autres cas, il faut préparer les données en amont de l'entraînement et de la prédiction, par exemple en utilisant un `ColumnTransformer` de `scikit-learn` ([`OneHotEncoder()`](https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.OneHotEncoder.html), [`TargetEncoder()`](https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.TargetEncoder.html), [`OrdinalEncoder()`](https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)). Le tableau et les notes ci-dessous résument quelles approches sont proposées par chaque implémentations des méthodes ensemblistes.

::: {.content-visible when-format="typst"}


```{=typst}
#import "@preview/fontawesome:0.5.0": *

// Define shorthands for Fontawesome icons
#let present() = {
  text(fa-icon("circle-check", fill: rgb("#2C9B2D")), size: 14pt)
}

#let absent() = {
  fa-icon("circle-xmark", fill: red, size: 14pt)
}

#let unknown() = {
  fa-icon("circle-question")
}


#present()

#absent()

#figure(
  table(
    columns: (8fr, 2fr, 2fr, 2fr, 2fr, 2fr,),
    align: center + horizon,
    table.header(
      [Approche],                                 [`ranger`],   [`scikit-learn`], [`XGBoost`],  [`LightGBM`], [`CatBoost`]
    ),
    [ _One-hot-encoding_],                        [#absent()],  [#absent()],      [#present()], [#present()], [#present()], 
    [ _Ordinal encoding_],                        [#present()], [#absent()],      [#absent()],  [#absent()],  [#absent()], 
    [ _Target encoding_],                         [#present()], [#absent()],      [#absent()],  [#absent()],  [#present()], 
    [ Support natif des variables catégorielles], [#absent()],  [#present()],     [#present()], [#present()], [#absent()], 
  ),
    caption: [_Preprocessing_ des variables catégorielles dans les implémentations des méthodes ensemblistes],
) <tbl-preprocessing-cat>
```
:::


::: {.content-visible when-format="html"}

:::: {#tbl-preprocessing tbl-colwidths="[30,10,10,10,10]"}

| Approche                                   | `ranger`   | `scikit-learn` | `XGBoost`  | `LightGBM` | `CatBoost`  |
| :----------------------------------------- | :--------: | :------------: | :--------: | :--------: | :---------: |
| _One-hot-encoding_                         | <i class="fa fa-circle-xmark" style="color: #FF0000; font-size: 24px;"></i> | <i class="fa fa-circle-xmark" style="color: #FF0000; font-size: 24px;"></i> | <i class="fa fa-circle-check" style="color: #2C9B2D; font-size: 24px;"></i> | <i class="fa fa-circle-check" style="color: #2C9B2D; font-size: 24px;"></i> | <i class="fa fa-circle-check" style="color: #2C9B2D; font-size: 24px;"></i> |
| _Ordinal encoding_                         | <i class="fa fa-circle-check" style="color: #2C9B2D; font-size: 24px;"></i> | <i class="fa fa-circle-xmark" style="color: #FF0000; font-size: 24px;"></i> | <i class="fa fa-circle-xmark" style="color: #FF0000; font-size: 24px;"></i> | <i class="fa fa-circle-xmark" style="color: #FF0000; font-size: 24px;"></i> | <i class="fa fa-circle-xmark" style="color: #FF0000; font-size: 24px;"></i> |
| _Target encoding_                          | <i class="fa fa-circle-check" style="color: #2C9B2D; font-size: 24px;"></i> | <i class="fa fa-circle-xmark" style="color: #FF0000; font-size: 24px;"></i> | <i class="fa fa-circle-xmark" style="color: #FF0000; font-size: 24px;"></i> | <i class="fa fa-circle-xmark" style="color: #FF0000; font-size: 24px;"></i> | <i class="fa fa-circle-check" style="color: #2C9B2D; font-size: 24px;"></i> |
| Support natif des variables catégorielles  | <i class="fa fa-circle-xmark" style="color: #FF0000; font-size: 24px;"></i> | <i class="fa fa-circle-check" style="color: #2C9B2D; font-size: 24px;"></i> | <i class="fa fa-circle-check" style="color: #2C9B2D; font-size: 24px;"></i> | <i class="fa fa-circle-check" style="color: #2C9B2D; font-size: 24px;"></i> | <i class="fa fa-circle-xmark" style="color: #FF0000; font-size: 24px;"></i> |


_Preprocessing_ des variables catégorielles dans les implémentations des méthodes ensemblistes

::::

:::

- `ranger`: le traitement des variables catégorielles (`factors`) est contrôlé par l'argument `respect.unordered.factors`, qui peut prendre trois valeurs: `ignore` (_ordinal encoding_),  `order` (_target encoding_) et `partition` (essayer toutes les combinaisons possibles). L'usage de cet argument est détaillé dans la documentation de la fonction `ranger()`. L'usage de la modalité `partition` n'est pas recommandé. En revanche, `ranger` ne prend pas en charge le _one-hot-encoding_.

- `LightGBM`: cette implémentation utilise par défaut le _one-hot-encoding_ pour les variables catégorielles comprenant 4 modalités ou moins, et le support natif des variables catégorielles pour les autres. Ce seuil peut être modifié via le paramètre `max_cat_to_onehot`. En revanche, `LightGBM` ne prend pas en charge ni l'_ordinal encoding_ ni le _target encoding_.

- `XGBoost`: le traitement des variables catégorielles est contrôlé par l'argument `enable_categorical`. Si `enable_categorical = True` (en Python), alors `XGBoost` applique le support natif des variables catégorielles. Par ailleurs, le paramètre `max_cat_to_onehot` permet d'utiliser le _one-hot-encoding_ pour les variables catégorielles comprenant moins de `max_cat_to_onehot` modalités, et le support natif pour les autres (voir la [documentation](https://xgboost.readthedocs.io/en/stable/tutorials/categorical.html)). En revanche, `XGBoost` ne prend pas en charge ni l'_ordinal encoding_ ni le _target encoding_.

- `CatBoost`: cette implémentation est celle qui propose le plus d'options relatives aux variables catégorielles. Par défaut, elle utilise le _one-hot-encoding_ pour les variables catégorielles comprenant peu de modalités, et une variante de _target encoding_ aux autres. Le seuil par défaut varie selon le type de tâche et peut être modifié via le paramètre `one_hot_max_size`. La documentation de `CatBoost` propose un [_notebook_](https://github.com/catboost/catboost/blob/master/catboost/tutorials/categorical_features/categorical_features_parameters.ipynb) qui détaille les différents hyperparamètres.

- `scikit-learn`: l'implémentation du _gradient boosting_ ([`HistGradientBoostingClassifier`](https://scikit-learn.org/dev/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html), [`HistGradientBoostingRegressor`](https://scikit-learn.org/dev/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html)) propose le support natif des variables catégorielles. Cette implémentation ne propose pas de _one-hot-encoding_ pour les variables catégorielles comprenant peu de modalités, et ne prend pas en charge ni l'_ordinal encoding_ ni le _target encoding_.
