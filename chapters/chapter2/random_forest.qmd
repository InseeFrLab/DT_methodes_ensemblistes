# La forêt aléatoire

La forêt aléatoire (_random forests_) est une méthode ensembliste qui consiste à agréger plusieurs arbres de décision pour améliorer la précision et la robustesse des prédictions du modèle final. Cette méthode s’appuie sur la technique du bagging, qui consiste à entraîner chaque arbre sur un échantillon (_bootstrap_) tiré au hasard à partir du jeu de données initial. Toutefois, la forêt aléatoire va plus loin en introduisant un degré supplémentaire de randomisation : pour chaque division lors de la construction d'un arbre, elle **sélectionne aléatoirement** un sous-ensemble de variables sur lequel sera fondé le critère de séparation. Cette randomisation supplémentaire **réduit la corrélation** entre les arbres, ce qui permet de renforcer la performance globale du modèle agrégé.


## Principe de la forêt aléatoire

Les forêts aléatoires reposent sur trois éléments essentiels :

- **L'échantillonnage bootstrap**: Chaque arbre est construit à partir d'un échantillon bootstrap, un échantillon aléatoire tiré avec remise du jeu de données d'entraînement. 

- **La sélection aléatoire de caractéristiques (_variables_)** : Lors de la construction de chaque arbre, et à chaque nœud de celui-ci, un sous-ensemble aléatoire de caractéristiques est sélectionné. La meilleure division est ensuite choisie parmi ces caractéristiques aléatoires. 

- **L'agrégation des prédictions** : Comme pour le bagging, les prédictions de tous les arbres sont agrégées, en procédant généralement à la moyenne (ou à la médiane) des prédictions dans le cas de la régression, et au vote majoritaire (ou à la moyenne des probabilités prédites pour chaque classe) dans le cas de la classification, afin d'obtenir des prédictions plus précises et généralisables.


## Pourquoi (et dans quelles situations) la random forest fonctionne










