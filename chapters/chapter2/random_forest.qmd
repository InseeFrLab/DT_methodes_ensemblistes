# La forêt aléatoire

La forêt aléatoire (_random forests_) est une méthode ensembliste qui consiste à agréger plusieurs arbres de décision pour améliorer la précision et la robustesse des prédictions du modèle final. Cette méthode s’appuie sur la technique du bagging, qui consiste à entraîner chaque arbre sur un échantillon (_bootstrap_) tiré au hasard à partir du jeu de données initial. Toutefois, la forêt aléatoire introduit un degré supplémentaire de randomisation : pour chaque arbre et pour chaque nouvelle division, un sous-ensemble de variables sur lequel sera fondé le critère de séparation est **sélectionné aléatoirement**. Cette randomisation supplémentaire **réduit la corrélation** entre les arbres, ce qui permet de renforcer la performance globale du modèle agrégé.

## Principe de la forêt aléatoire

Les forêts aléatoires reposent sur plusieurs éléments essentiels :

- **Les arbres CART**: Les modèles élémentaires sont des arbres CART non élagués, c'est-à-dire autorisés à pousser jusqu'à l'atteinte d'un critère d'arrêt défini en amont.

- **L'échantillonnage bootstrap**: Chaque arbre est construit à partir d'un échantillon aléatoire tiré avec remise du jeu de données d'entraînement.

- **La sélection aléatoire de caractéristiques (_variables_)** : Lors de la construction de chaque arbre, et à chaque nœud de celui-ci, un sous-ensemble aléatoire de caractéristiques est sélectionné. La meilleure division est ensuite choisie parmi ces caractéristiques aléatoires.

- **L'agrégation des prédictions** : Comme pour le bagging, les prédictions de tous les arbres sont agrégées. On procède généralement à la moyenne (ou à la médiane) des prédictions dans le cas de la régression, et au vote majoritaire (ou à la moyenne des probabilités prédites pour chaque classe) dans le cas de la classification.

## Pourquoi (et dans quelles situations) la random forest fonctionne

Les travaux de Breiman (2001) ont mis en évidence les propriétés théoriques des forêts aléatoires et ont permis de mieux comprendre pourquoi elles sont particulièrement robustes et performantes.

### Convergence et absence de surapprentissage

Les Random Forests ont la particularité de converger à mesure que le nombre d'arbres augmente. Cela implique  que le modèle ne souffre pas de surapprentissage (également appelé overfitting)  lorsqu'un grand nombre d'arbres est ajouté. Cette stabilité est garantie par la Loi des Grands Nombres, qui stipule que l'erreur de généralisation de l'ensemble se stabilise au fur et à mesure que le nombre d'arbres augmente. Cela rend la Random Forest particulièrement robuste.

### Facteurs déterminants la diminution de l'erreur de généralisation

L'erreur de généralisation des Random Forests est influencée par deux facteurs principaux :

- La force des arbres individuels : La force d'un arbre dépend de sa capacité à faire des prédictions correctes. Pour que l'ensemble soit performant, chaque arbre doit être suffisamment précis.

- La corrélation entre les arbres : Une  corrélation faible entre les arbres  améliore les performances globales, car des arbres fortement corrélés auront tendance à faire des erreurs similaires. La randomisation des caractéristiques à chaque nœud contribue à réduire cette corrélation, ce qui améliore la précision globale.


## Estimation de l'erreur Out-of-Bag (OOB)

Les échantillons Out-of-Bag (OOB) peuvent être utilisés pour estimer l'erreur de généralisation des forêts aléatoires. Pour chaque arbre, l'échantillon OOB est constitué des observations qui n'ont pas été utilisées dans sa construction. Environ un tiers des observations n'est pas inclus dans chaque échantillon bootstrap en raison du tirage aléatoire avec remise, et peuvent être utilisées pour tester la performance de l'arbre. L'estimation OOB offre une mesure fiable de l'erreur de généralisation, comparable à celle obtenue par validation croisée. Elle présente toutefois l'avantage de ne pas nécessiter un jeu de validation distinct du jeu d'entraînement.



