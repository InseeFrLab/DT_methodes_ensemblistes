## Le *boosting*

### Introduction

Le fondement théorique du *boosting* est un article de de 1990 (@shapire1990strength) qui a démontré théoriquement que, sous certaines conditions, il est possible de transformer un modèle prédictif peu performant en un modèle prédictif très performant. Plus précisément, cet article prouve que s'il est possible de construire un modèle simple dont les prédictions ne sont que légèrement meilleures que le hasard (appelé *weak learner*), alors il est possible de construire un modèle ayant un pouvoir prédictif arbitrairement élevé (appelé *strong learner*) en améliorant progressivement ce modèle simple. Le *boosting* est donc une méthode qui combine une approche ensembliste reposant sur un grand nombre de modèles simples avec un entraînement séquentiel: chaque modèle simple (souvent des arbres de décision peu profonds) tâche d'améliorer la prédiction globale en corrigeant les erreurs des prédictions précédentes à chaque étape. Bien qu'une approche de *boosting* puisse en théorie mobiliser différentes classes de *weak learners*, en pratique les *weak learners* utilisés par les algorithmes de *boosting* sont presque toujours des arbres de décision.

<!-- S'il existe plusieurs variantes, tous les algorithmes de *boosting* suivent la même logique :

-   Un premier modèle simple et peu performant est entraîné sur les données.
-   Un deuxième modèle est entraîné de façon à corriger les erreurs du premier modèle (par exemple en pondérant davantage les observations mal prédites), puis combiné avec le premier modèle;
-   Ce processus est répété en ajoutant des modèles simples, chaque modèle corrigeant les erreurs commises par l'ensemble des modèles précédents;
-   Le modèle final est la combinaison de l'ensemble des modèles simples. -->

En termes plus techniques, les différentes variantes du *boosting* partagent toutes trois caractéristiques communes:

-   Ils visent à **trouver une approximation** $\hat{F}$ d'une fonction inconnue $F^{\ast}: \mathbf{x} \mapsto y$ à partir d'un ensemble d'entraînement $(y_i, \mathbf{x_i})_{i= 1,\dots,n}$;
-   Ils supposent que la fonction $F^{\ast}$ peut être approchée par une **somme pondérée de modèles simples** $f$ de paramètres $\theta$:

$ F\left(\mathbf{x}\right) = \sum_{m=1}^M \beta_m f\left(\mathbf{x}, \mathbf{\theta}_m\right) $

-   Ils reposent sur une **modélisation additive par étapes** (_forward stagewise additive modeling_), qui décompose l'entraînement de ce modèle complexe en une **séquence d'entraînements de petits modèles**. Chaque étape de l'entraînement cherche le modèle simple $f$ qui améliore la puissance prédictive du modèle complet, sans modifier les modèles précédents, puis l'ajoute de façon incrémentale à ces derniers:

$ F_m(\mathbf{x}) = F_{m-1}(\mathbf{x}) + \hat{\beta}_m f(\mathbf{x}_i, \mathbf{\hat{\theta}_m}) $


<!-- Imaginons qu'on veuille entraîner le modèle suivant:

$F\left(\mathbf{x}\right) = \sum_{m=1}^M \beta_m f\left(\mathbf{x}, \mathbf{\theta}_m\right)$


$\hat{F}$ est caractérisée par les paramètres $\{\beta_m, \mathbf{\theta}_m\}_{m=1}^{M}$ tels que
$\argmin_{\{\beta_m, \mathbf{\theta}_m\}_{m=1}^{M}} \sum_{i=1}^N L\left(y_i, \sum_{m=1}^M \beta_m f\left(\mathbf{x}_i, \mathbf{\theta}_m\right)\right)$


C'est un problème très compliqué dès que $M$ est élevé! -->

METTRE ICI UNE FIGURE EN UNE DIMENSION, avec des points et des modèles en escalier qui s'affinent.

### Les premières approches du *boosting*

#### Le *boosting* par repondération: Adaboost

Dans les années 1990, de nombreux travaux ont tâché de proposer des mise en application du *boosting* (@breiman1998rejoinder, @grove1998boosting) et ont comparé les mérites des différentes approches. Deux approches ressortent particulièrement de cette littérature: Adaboost (Adaptive Boosting, @freund1997decision) et la *Gradient Boosting Machine* (@friedman2001greedy). Ces deux approches reposent sur des principes très différents.

Le principe d'Adaboost consiste à pondérer les erreurs commises à chaque itération en donnant plus d'importance aux observations mal prédites, de façon à obliger les modèles simples à se concentrer sur les observations les plus difficiles à prédire. Voici une esquisse du fonctionnement d'AdaBoost:

-   Un premier modèle simple est entraîné sur un jeu d'entraînement dans lequel toutes les observations ont le même poids.
-   A l'issue de cette première itération, les observations mal prédites reçoivent une pondération plus élevée que les observations bien prédites, et un deuxième modèle est entraîné sur ce jeu d'entraînement pondéré.
-   Ce deuxième modèle est ajouté au premier, puis on repondère à nouveau les observations en fonction de la qualité de prédiction de ce nouveau modèle.
-   Cette procédure est répétée en ajoutant de nouveaux modèles et en ajustant les pondérations.

L'algorithme Adaboost a été au coeur de la littérature sur le *boosting* à la fin des années 1990 et dans les années 2000, en raison de ses performances sur les problèmes de classification binaire. Il a toutefois été progressivement remplacé par les algorithmes de *gradient boosting* mis au point quelques années plus tard.

#### L'invention du *boosting boosting* : la *Gradient Boosting Machine*

La *Gradient Boosting Machine* (GBM) propose une approche assez différente: elle introduit le *gradient boosting* en reformulant le *boosting* sous la forme d'un problème de descente de gradient. Voici une esquisse du fonctionnement de la *Gradient Boosting Machine*:

-   Un premier modèle simple est entraîné sur les données d'entraînement, de façon à minimiser une fonction de perte qui mesure l'écart entre la variable à prédire et la prédiction du modèle.
-   A l'issue de cette première itération, on calcule la dérivée partielle (*gradient*) de la fonction de perte par rapport à la prédiction en chaque point de l'ensemble d'entraînement. Ce gradient indique à la fois dans quelle direction et dans quelle ampleur la prédiction devrait être modifiée afin de réduire la perte.
-   A la deuxième itération, on ajoute un deuxième modèle qui va tâcher d'améliorer le modèle complet en prédisant le mieux possible l'opposé de ce gradient.
-   Ce deuxième modèle est ajouté au premier, puis on recalcule la dérivée partielle de la fonction de perte par rapport à la prédiction de ce nouveau modèle.
-   Cette procédure est répétée en ajoutant de nouveaux modèles et en recalculant le gradient à chaque étape.
-   La qualité du modèle final est évaluée sur un ensemble de test.

La mécanique de la *Gradient Boosting Machine* est résumée de façon plus formelle dans l'algorithem REFERENCE.



Commentaire: On pourrait peut-être donner la version formelle de la GBM, mais c'est peut-être inutile. 
1.  Initialiser le modèle avec $f_0\left(\mathbf{x}\right) = y_0$.
2.  Pour $m = 1, \dots, M:$
    (a) Entraîner le $m$-ième modèle:
    $ \left(\hat{\beta}_m, \hat{\theta}_m\right) = \argmin_{\beta, \mathbf{\theta}} \sum_{i=1}^N L\left(y_i, f_{m-1}\left(\mathbf{x}_i\right) + \beta b\left(\mathbf{x}_i, \mathbf{\theta}\right)\right) $
    (b) Définir $f_m\left(\mathbf{x}\right) = f_{m-1}\left(\mathbf{x}\right) + \hat{\beta}_m b\left(\mathbf{x}_i, \mathbf{\hat{\theta}_m}\right)$




L'approche de *gradient boosting* proposée par @friedman2001greedy présente deux grands avantages. D'une part, elle peut être utilisée avec n'importe quelle fonction de perte différentiable, ce qui permet d'appliquer le gradient boosting à de multiples problèmes (régression, classification binaire ou multiclasse, *learning-to-rank*...). D'autre part, elle offre souvent des performances comparables ou supérieures aux autres approches de *boosting*. Le *gradient boosting* d'arbres de décision (*Gradient boosted Decision Trees* - GBDT) est donc devenue l'approche de référence en matière de *boosting*: toutes les implémentations modernes du *gradient boosting* comme `scikit-learn`, `XGBoost`, `LightGBM`, et `CatBoost` sont des extensions et améliorations de la *Gradient Boosting Machine*.

AJOUTER ICI LA GBM en pseudo-code


### La mécanique du *gradient boosting*

La méthode de *gradient boosting* proposée @friedman2001greedy a fait l'objet de multiples implémentations intégrant de nombreuses optimisations et raffinements, parmi lesquelles XGBoost (@chen2016xgboost), LightGBM (@ke2017lightgbm) et CatBoost (@prokhorenkova2018catboost). S'il existe quelques différences entre ces implémentations, elles partagent néanmoins la même mécanique d'ensemble, que la section qui suit va présenter en détail en s'appuyant sur l'implémentation proposée par XBGoost.[^1]

[^1]: Cette partie reprend la structure et les notations de la partie 2 de @chen2016xgboost.

#### Le modèle à entraîner

On veut entraîner un modèle comprenant $K$ arbres de régression ou de classification:

$$ \hat{y}_{i} =F\left(\mathbf{x}_i\right) = \sum_{k=1}^{K} f_k\left(\mathbf{x}_i\right) $$

Chaque arbre $f$ est défini par trois paramètres:

-   sa structure qui est une fonction $q: \mathbb{R}^m \rightarrow \{1, \dots, T\}$ qui à un vecteur $\mathbf{x}$ de dimension $m$ associe une feuille terminale de l'arbre;
-   son nombre de feuilles terminales $T$;
-   les valeurs figurant sur ses feuilles terminales $\mathbf{w}\in \mathbb{R}^T$ (appelées poids ou *weights*).

Le modèle est entraîné avec une **fonction-objectif** constituée d'une **fonction de perte** $l$ et d'une **fonction de régularisation** $\Omega$: 

- La fonction de perte mesure la distance entre la prédiction $\hat{y}$ et la vraie valeur $y$ et présente généralement les propriétés suivantes: elle est convexe et dérivable deux fois, et atteint son minimum lorsque $\hat{y} = y$. 
- La fonction de régularisation pénalise la complexité du modèle. Dans le cas présent, elle pénalise les arbres avec un grand nombre de feuilles ($T$ élevé) et les arbres avec des poids élevés ($w_j$ élevés en valeur absolue).

$$ \mathcal{L}(\phi) = \underbrace{\sum_i l(\hat{y}_{i}, y_{i})}_{\substack{\text{Perte sur les} \\ \text{observations}}} + \underbrace{\sum_k \Omega(f_{k})}_{\substack{\text{Fonction de} \\ \text{régularisation}}}\,\,\text{avec}\,\,\Omega(f) = \gamma T + \frac{1}{2} \lambda \sum_{k=1}^K \sum_{j=1}^{T_k} w_j^2 
$$ {#eq-fct-obj-initial}

Un point essentiel est que __toute la mécanique du _gradient boosting_ est indépendante de la fonction de perte choisie et de la nature du problème modélisé__. En particulier, toutes les formules restent inchangées, que l'on traite un un problème de classification, de régression ou de classement (_ranking_). C'est là l'une des grandes forces du _gradient boosting_: un seul cadre conceptuel permet de traiter de multiples situations.


#### Isoler le $t$-ième arbre

La fonction-objectif introduite précédemment est très complexe et ne peut être utilisée directement pour entraîner le modèle, car il faudrait entraîner tous les arbres en même temps. On reformule donc cette fonction objectif de façon à isoler le $t$-ième arbre, qui pourra ensuite être entraîné seul, une fois que les $t-1$ arbres précédents auront été entraînés. Pour cela, on note $F_t(x) = \hat{y}_i^{(t)}$ la prédiction à l'issue de l'étape $t$: $\hat{y}_i^{(t)} = \sum_{k=1}^t f_k(\mathbf{x}_i)$, et on note $\mathcal{L}^{(t)}$ la fonction-objectif au moment de l'entraînement du $t$-ième arbre:

$$
\begin{aligned}
\mathcal{L}^{(t)} 
&= \sum_{i=1}^{n} l(y_i, \hat{y}_{i}^{(t)}) + \sum_{k=1}^t\Omega(f_k) \\
&= \sum_{i=1}^{n} l\left(y_i, \hat{y}_{i}^{(t-1)} + f_{t}(\mathbf{x}_i)\right) + \Omega(f_t) + constant
\end{aligned}
$$


#### Faire apparaître le gradient de la fonction de perte

Une fois isolé le $t$-ième arbre, on fait un développement limité d'ordre 2 de $l(y_i, \hat{y}_{i}^{(t-1)} + f_{t}(\mathbf{x}_i))$ au voisinage de $\hat{y}_{i}^{(t-1)}$, en considérant que la prédiction du $t$-ième arbre $f_{t}(\mathbf{x}_i)$ est un incrément de petite taille:

$$ \mathcal{L}^{(t)} \approx \sum_{i=1}^{n} [\underbrace{l(y_i, \hat{y}_{i}^{(t-1)})}_{\text{(A)}} + g_i f_t(\mathbf{x}_i)+ \frac{1}{2} h_i f^2_t(\mathbf{x}_i)] + \underbrace{\sum_{j=1}^{t-1}\Omega(f_j)}_{\text{(B)}} + \Omega(f_t) $$

avec 

$$ g_i = \frac{\partial l(y_i, \hat{y}_i^{(t-1)})}{\partial\hat{y}_i^{(t-1)}} \text{et} h_i = \frac{\partial^2 l(y_i, \hat{y}_i^{(t-1)})}{{\partial \hat{y}_i^{(t-1)}}^2} $$


Les termes $g_i$ et $h_i$ désignent respectivement la dérivée première (le gradient) et la dérivée seconde (la hessienne) de la fonction de perte par rapport à la variable prédite. Dans cette équation, les termes (A) et (B) sont constants car les $t-1$ arbres précédents ont déjà été entraînés et ne sont pas modifiés par l'entraînement du $t$-ième arbre. <!-- Autrement dit, la seule façon d'améliorer le modèle sera de trouver un $t$-ième arbre $f_t$ qui minimise la fonction-objectif \mathcal{L}^{(t)} ainsi réécrite.  --> On peut donc retirer ces termes pour obtenir la fonction-objectif simplifiée $\tilde{L}^{(t)}$ qui sera utilisée pour l'entraînement du $t$-ième arbre.

$$ \mathcal{\tilde{L}}^{(t)} = \sum_{i=1}^{n} [g_i f_t(\mathbf{x}_i)+ \frac{1}{2} h_i f^2_t(\mathbf{x}_i)] + \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_i^2 
$$ {#eq-fct-obj-final}

<!-- Cette expression montre que le problème initial où il fallait entraîner un grand nombre d'arbres simultanément (équation @eq-fct-obj-initial) à un problème beaucoup plus simple dans lequel il n'y a plus qu'un seul arbre à entraîner (équation @eq-fct-obj-final). -->

#### Calculer les poids optimaux

A partir de l'équation @eq-fct-obj-final, il est possible de faire apparaître les poids $w_j$ du $t$-ième arbre. Pour un arbre donné comprenant $T$ feuilles ($q: \mathbb{R}^T \rightarrow \{1, \dots, T\}$), on définit $I_j = \{ i | q(\mathbf{x}_i) = j \}$ l'ensemble des observations situées sur la feuille terminale $j$, et $w_j$ la valeur prédite par l'arbre pour la feuille $j$. Avec cette notation, on réorganise $\mathcal{\tilde{L}}^{(t)}$ en regroupant les observations selon la feuille terminale à laquelle elles appartiennent:

$$
\begin{align*}
 \mathcal{\tilde{L}}^{(t)} =&   \sum_{j=1}^{T} \sum_{i\in I_{j}} \bigg[g_i f_t(\mathbf{x}_i)\phantom{\frac{1}{2}} &+ \frac{1}{2} h_i f^2_t(\mathbf{x}_i)\bigg]&+ \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_i^2 \\
     &= \sum_{j=1}^{T} \sum_{i\in I_{j}} \bigg[g_i w_j &+ \frac{1}{2} h_i w_j^2\bigg] &+ \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_i^2 \\
     &= \sum^T_{j=1} \bigg[w_j\sum_{i\in I_{j}} g_i &+ \frac{1}{2} w_j^2 \left( \sum_{i \in I_{j}} h_i + \lambda \right) \bigg] &+ \gamma T
\end{align*}
$$

Dans la dernière expression, la fonction de perte simplifiée se reformule comme une combinaison quadratique des poids $w_j$, dans laquelle les dérivées première et seconde de la fonction de perte interviennent sous forme de pondérations ($\sum_{i \in I_j} g_i$ et $\sum_{i \in I_j} h_i$). Pour un arbre donné, les poids optimaux $w_j$ sont ceux minimisent cette fonction de perte, compte tenu de ces pondérations. Il se trouve que le calcul de ces poids optimaux est très simple: le poids optimal $w_j^{\ast}$ de la feuille $j$ est donné par l'équation:

$$ w_j^{\ast} = -\frac{\sum_{i \in I_j} g_i}{\sum_{i \in I_j} h_i + \lambda} $$ {#eq-w-j-optimal}

#### Construire le $t$-ième arbre

En combinant les équations @eq-fct-obj-final et @eq-w-j-optimal, on déduit que la valeur optimale de la fonction objectif pour l'arbre $q$ est égale à 

$$ \mathcal{\tilde{L}}^{(t)}(q) = -\frac{1}{2} \sum_{j=1}^T \frac{\left(\sum_{i\in I_j} g_i\right)^2}{\sum_{i\in I_j} h_i+\lambda} + \gamma T $$ {#eq-fct-obj-optimal}

Cette équation est utile car elle permet de comparer la qualité de deux arbres, et de déterminer lequel est le meilleur. On pourrait penser que l'équation @eq-fct-obj-optimal est à elle seule suffisante pour choisir le $t$-ième arbre: il suffirait d'énumérer les arbres possibles, de calculer la qualité de chacun d'entre eux, et de retenir le meilleur. Bien que cette approche soit possible en théorie, elle est inemployable en pratique car le nombre d'arbres possibles est extrêmement élevé. Par conséquent, cette équation n'est pas utilisée telle quelle, mais sert à comparer les règles de décision possibles à chaque étape d'une optimisation gloutonne.

Imaginons qu'on envisage de décomposer la feuille $I$ en deux nouvelles feuilles $I_L$ et $I_R$ (avec $I = I_L \cup I_R$), selon une condition logique reposant sur une variable et une valeur de cette variable (exemple: $x_6 > 11$). Par application de l'équation @eq-fct-obj-optimal, le gain potentiel induit par cette règle de décision est égal à:

$$ Gain = \frac{1}{2} \left[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\right] - \gamma \,\,\text{avec}\,\, G = \sum_i g_i \,\,\text{et}\,\, H = \sum_i H_i$$ {#eq-fct-eval-split}


<!-- $\text{Gain}_{\text{split}} = \frac{1}{2} \left[\frac{\left(\sum_{i\in I_L} g_i\right)^2}{\sum_{i\in I_L} h_i+\lambda}+\frac{\left(\sum_{i\in I_R} g_i\right)^2}{\sum_{i\in I_R} h_i+\lambda}-\frac{\left(\sum_{i\in I} g_i\right)^2}{\sum_{i\in I} h_i+\lambda}\right] - \gamma$ -->


L'équation @eq-fct-eval-split est au coeur de la mécanique du *gradient boosting* car elle permet de comparer les règles de décision possibles. 

#### Chercher les _splits_ optimaux

La méthode de construction des arbres dans les algorithmes de _gradient boosting_ est identique à celle décrite dans la partie **REFERENCE A LA PARTIE CART/RF**,:le $t$-ième arbre n'est pas défini en une fois, mais construit de façon gloutonne (_greedy_). Il y a toutefois une différence importante: ces algorithmes utilisent l'équation @eq-fct-obj-optimal pour choisir la règle de décision (_split_) à chaque étape de la construction de l'arbre, et non une mesure d'hétérogénéité. Sur tous les autres points la construction de l'arbre est identique à celle des arbres CART:

- On commence par le noeud-racine comprenant l'ensemble des données d'entraînement, et on recherche la règle de décision des données qui maximise le gain mesuré par l'équation @eq-fct-obj-optimal. Puis on recherche la règle de décision optimale pour chacun des deux noeuds-enfants, et ainsi de suite jusqu'à ce qu'un critère d'arrêt soit atteint.
- Pour chaque noeud, l'algorithme de détermination des règles de décision (*split finding algorithm*) consiste en une double boucle sur les variables et les valeurs prises par ces variables, qui énumère un grand nombre de règles de décision possibles et mesure le gain associé à chacun d'entre elles avec l'équation @eq-fct-eval-split. La règle de décision retenue sera simplement celui dont le gain est le plus élevé.

L'algorithme de détermination des règles de décision est le composant le plus intense en calcul des algorithmes de _gradient boosting_. Les différentes implémentations du _gradient boosting_ proposent donc de multiples améliorations et optimisations visant à le rendre le plus efficace possible. Certaines de ces optimisations sont présentées dans la partie LIEN A LA PARTIE HISTOGRAMME/CATVAR.


#### Mettre à jour itérativement le modèle

Une fois qu'un arbre a été entraîné, on met à jour le modèle par la formule suivante: 

$$ F_{t}(x)=F_{t-1}(x)+ \eta f_{t}(x) $$ {#eq-update-model}

Il est alors possible de commencer l'entraînement de l'arbre suivant, selon la même logique que précédemment. La raison d'être du paramètre $\eta$ est expliquée dans le paragraphe suivant.

### Un exemple sur un cas simple: la régression avec une fonction de perte quadratique

EST-CE UTILE?

### Le grand ennemi du _gradient boosting_: le surajustement

Une différence majeure entre les forêts aléatoires et les algorithmes de _boosting_ est que ces derniers ne contiennent en eux-mêmes aucune limite au surajustement, bien au contraire: le _gradient boosting_ est un algorithme conçu pour approximer le plus précisément possible la relation entre $X$ et $y$ telle qu'elle apparaît dans les données d'entraînement, qu'il s'agisse d'un signal pertinent ou d'un bruit statistique. Par conséquent, __tous les algorithmes de _gradient boosting_ sont très vulnérables au surajustement__. Plus précisément, il y a deux raisons à cela. D'une part, lors de l'entraînement d'un modèle de _gradient boosting_, chaque nouvel arbre essaie de réduire l'erreur résiduelle en s'ajustant toujours plus finement aux données. Ainsi, au fur et mesure que le nombre d'arbres augmente, l'algorithme capture non seulement des relations pertinentes entre $X$ et $y$, mais aussi le bruit et les particularités aléatoires de l'échantillon d'entraînement. D'autre part, les arbres de décision utilisés sont très flexibles et conçus pour refléter les relations entre $X$ et $y$ présentes dans les données d'entraînement, y compris celles qui ne se généralisent pas bien aux nouvelles données. Par exemple, un arbre très profond peut correspondre finement aux données d'entraînement, mais risque de manquer de robustesse sur les données de test. 

__La lutte contre le surajustement est donc un enjeu majeur de l'entraînement des modèles de _gradient boosting_.__ De multiples méthodes ont été proposées pour lutter contre le surajustement:

- la __technique de réduction__ (*shrinkage technique*) consiste à réduire l'influence de chaque arbre sur le modèle global en multipliant la prédiction de cet arbre par un facteur d'échelle compris entre 0 et 1 au moment de mettre à jour le modèle par l'équation @eq-update-model. Ce facteur d'échelle est appelé __taux d'apprentissage__ (_learning rate_); il s'agit du paramètre $\eta$ dans l'équation @eq-update-model. L'avantage principal de cette technique est que le modèle s'ajuste progressivement aux données, et est moins altéré par les erreurs dues à des variations aléatoires ou au bruit dans les données. Un taux d'apprentissage bas et un nombre d'itérations suffisant permettent souvent d'obtenir un modèle final plus performant sur des données de test. L'inconvénient est que réduire le taux d'apprentissage nécessite d'augmenter le nombre d'itérations pour obtenir des performances comparables, ce qui peut rallonger le temps d'entraînement.

- l'__hyperparamètre de régularisation $\lambda$__ intervient dans l'équation @eq-w-j-optimal et réduit la valeur absolue des poids des feuilles terminales. Cet hyperparamètre contribue à ce que chaque arbre prédise des valeurs peu élevées. L'intuition est la suivante: lorsqu'une feuille terminale contient un poids $w_i$ élevé en valeur absolue, ce poids est probablement dû au moins en partie à des observations inhabituelles ou aberrantes (et dont le gradient $g_i$ prend une valeur extrême); il est donc préférable de réduire légèrement ce poids pour ne pas donner trop d'importance à ces points aberrants.

- l'__hyperparamètre de régularisation $\gamma$__ intervient dans l'équation @eq-fct-eval-split. Ce paramètre mesure la réduction minimale de la perte requise pour qu'un nœud soit divisé; une valeur plus élevée aboutit à des arbres moins profonds et contribue à limiter le surajustement en empêchant l'algorithme de créer des _splits_ dont l'apport est très faible et potentiellement dû à des variations non significatives des données d'entraînement.

- la dernière approche consiste à __entraîner les arbres sur un échantillon d'observations et/ou de variables__. L'échantillonnage des observations permet de réduire l'influence des éventuels points extrêmes contenus dans les données (car ils n'apparaissent pas dans les données d'entraînement de certains arbres); l'échantillonnage des variables permet de varier les variables utilisées dans les _splits_.

<!-- #### La préparation des données

-   les variables catégorielles:
    -   ordonnées: passer en integer;
    -   non-ordonnées: OHE ou approche de Fisher.
-   les variables continues:
    -   inutile de faire des transformations monotones.
    -   Utile d'ajouter des transformations non monotones.

#### Les fonctions de perte -->

