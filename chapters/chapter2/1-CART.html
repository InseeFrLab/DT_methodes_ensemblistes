<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>cart – Introduction aux méthodes ensemblistes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/chapter2/2-bagging.html" rel="next">
<link href="../../chapters/chapter1/1-survol.html" rel="prev">
<link href="../../images/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ea167b1c95185128cea848f008f0cb8f.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../custom.css">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/chapter2/1-CART.html">Présentation formelle des algorithmes</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Introduction aux méthodes ensemblistes</a> 
        <div class="sidebar-tools-main">
    <a href="../.././pdf/dt_methodes_ensemblistes.pdf" title="NMFS Open Science" class="quarto-navigation-tool px-1" aria-label="NMFS Open Science"><i class="bi bi-file-pdf-fill"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction aux méthodes ensemblistes</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter1/1-survol.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Survol des méthodes ensemblistes</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/chapter2/1-CART.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Présentation formelle des algorithmes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter2/1-CART.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Les arbres de décision</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter2/2-bagging.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Le <em>bagging</em></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter2/3-random_forest.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">La forêt aléatoire</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter2/4-boosting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Le <em>gradient boosting</em></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter2/5-Sujets-avances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sujets avancés: traitement des données pendant l’entraînement</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../chapters/chapter3/1-preparation_donnees.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Comment bien utiliser les algorithmes?</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter3/1-preparation_donnees.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Préparation des données</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter3/2-guide_usage_RF.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Guide d’usage des forêts aléatoires</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter3/3-guide_usage_GB.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Guide d’usage du <em>gradient boosting</em></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sec-CART" id="toc-sec-CART" class="nav-link active" data-scroll-target="#sec-CART"><span class="header-section-number">1</span> Les arbres de décision</a>
  <ul class="collapse">
  <li><a href="#sec-partitionner" id="toc-sec-partitionner" class="nav-link" data-scroll-target="#sec-partitionner"><span class="header-section-number">1.1</span> Le principe fondamental: partitionner pour prédire</a>
  <ul class="collapse">
  <li><a href="#les-défis-du-partitionnement-optimal" id="toc-les-défis-du-partitionnement-optimal" class="nav-link" data-scroll-target="#les-défis-du-partitionnement-optimal"><span class="header-section-number">1.1.1</span> Les défis du partitionnement optimal</a></li>
  <li><a href="#les-solutions-apportées-par-les-arbres-de-décision" id="toc-les-solutions-apportées-par-les-arbres-de-décision" class="nav-link" data-scroll-target="#les-solutions-apportées-par-les-arbres-de-décision"><span class="header-section-number">1.1.2</span> Les solutions apportées par les arbres de décision</a></li>
  <li><a href="#terminologie-et-structure-dun-arbre-de-décision" id="toc-terminologie-et-structure-dun-arbre-de-décision" class="nav-link" data-scroll-target="#terminologie-et-structure-dun-arbre-de-décision"><span class="header-section-number">1.1.3</span> Terminologie et structure d’un arbre de décision</a></li>
  <li><a href="#propriétés-des-arbres-de-décision" id="toc-propriétés-des-arbres-de-décision" class="nav-link" data-scroll-target="#propriétés-des-arbres-de-décision"><span class="header-section-number">1.1.4</span> Propriétés des arbres de décision</a></li>
  </ul></li>
  <li><a href="#sec-construire" id="toc-sec-construire" class="nav-link" data-scroll-target="#sec-construire"><span class="header-section-number">1.2</span> La construction d’un arbre de décision par l’algorithme CART</a>
  <ul class="collapse">
  <li><a href="#définir-une-mesure-dimpureté-adaptée-au-problème" id="toc-définir-une-mesure-dimpureté-adaptée-au-problème" class="nav-link" data-scroll-target="#définir-une-mesure-dimpureté-adaptée-au-problème"><span class="header-section-number">1.2.1</span> Définir une mesure d’impureté adaptée au problème</a></li>
  <li><a href="#construire-larbre-de-décision-par-un-partitionnement-séquentiel" id="toc-construire-larbre-de-décision-par-un-partitionnement-séquentiel" class="nav-link" data-scroll-target="#construire-larbre-de-décision-par-un-partitionnement-séquentiel"><span class="header-section-number">1.2.2</span> Construire l’arbre de décision par un partitionnement séquentiel</a></li>
  <li><a href="#contrôler-la-complexité-de-larbre" id="toc-contrôler-la-complexité-de-larbre" class="nav-link" data-scroll-target="#contrôler-la-complexité-de-larbre"><span class="header-section-number">1.2.3</span> Contrôler la complexité de l’arbre</a></li>
  <li><a href="#utiliser-larbre-pour-prédire" id="toc-utiliser-larbre-pour-prédire" class="nav-link" data-scroll-target="#utiliser-larbre-pour-prédire"><span class="header-section-number">1.2.4</span> Utiliser l’arbre pour prédire</a></li>
  </ul></li>
  <li><a href="#avantages-et-limites-des-arbres-de-décision" id="toc-avantages-et-limites-des-arbres-de-décision" class="nav-link" data-scroll-target="#avantages-et-limites-des-arbres-de-décision"><span class="header-section-number">1.3</span> Avantages et limites des arbres de décision</a>
  <ul class="collapse">
  <li><a href="#avantages" id="toc-avantages" class="nav-link" data-scroll-target="#avantages"><span class="header-section-number">1.3.1</span> Avantages</a></li>
  <li><a href="#limites" id="toc-limites" class="nav-link" data-scroll-target="#limites"><span class="header-section-number">1.3.2</span> Limites</a></li>
  </ul></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/inseefrlab/DT_methodes_ensemblistes/edit/main/chapters/chapter2/1-CART.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/inseefrlab/DT_methodes_ensemblistes/blob/main/chapters/chapter2/1-CART.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li><li><a href="https://github.com/inseefrlab/DT_methodes_ensemblistes/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="sec-CART" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Les arbres de décision</h1>
<p>Les arbres de décision désignent un éventail d’algorithmes de <em>machine learning</em>, utilisés notamment pour des tâches de classification et de régression. Ces algorithmes constituent la brique élémentaire des méthodes ensemblistes à base d’arbres (forêt aléatoire et <em>gradient boosting</em>). Cette section a pour objectif de présenter ce qu’est un arbre de décision, sa structure et la terminologie associée (<a href="#sec-partitionner" class="quarto-xref">Section&nbsp;1.1</a>), puis de détailler la méthode de construction des arbres par l’algorithme CART (<a href="#sec-construire" class="quarto-xref">Section&nbsp;1.2</a>).</p>
<section id="sec-partitionner" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="sec-partitionner"><span class="header-section-number">1.1</span> Le principe fondamental: partitionner pour prédire</h2>
<p>Le principe des arbres de décision consiste à <strong>diviser l’espace des caractéristiques en sous-régions homogènes à l’aide de règles simples</strong>, puis de former pour chaque sous-région une prédiction à partir des observations présentes dans cette sous-région. Imaginons par exemple que l’on souhaite prédire le prix d’une maison en fonction de sa superficie et de son nombre de pièces, à partir d’un ensemble de transactions pour lesquelles le prix est connu. L’espace des caractéristiques (superficie et nombre de pièces) est vaste, et les prix des maisons (la <em>réponse</em> à prédire) sont très variables. L’idée centrale des arbres de décision est de diviser cet espace en zones plus petites, au sein desquelles les maisons ayant des surfaces et un nombre de pièces similaire ont des prix proches, et d’attribuer une prédiction identique à toutes les maisons situées dans la même zone. Malgré cette apparente simplicité, les arbres de décision sont puissants et capables de modéliser des interactions complexes et non linéaires entre les variables d’un jeu de données.</p>
<section id="les-défis-du-partitionnement-optimal" class="level3" data-number="1.1.1">
<h3 data-number="1.1.1" class="anchored" data-anchor-id="les-défis-du-partitionnement-optimal"><span class="header-section-number">1.1.1</span> Les défis du partitionnement optimal</h3>
<p>L’objectif principal est de trouver la partition de l’espace des caractéristiques qui offre les meilleures prédictions possibles. Cependant, cet objectif se heurte à plusieurs difficultés, et la complexité du problème augmente rapidement avec le nombre de caractéristiques et la taille de l’échantillon:</p>
<ul>
<li><p><strong>Infinité des découpages possibles</strong>: Il existe une infinité de façons de diviser l’espace des caractéristiques;</p></li>
<li><p><strong>Complexité de la paramétrisation</strong>: Il est difficile de représenter tous ces découpages avec un nombre limité de paramètres;</p></li>
<li><p><strong>Optimisation complexe</strong>: Même avec une paramétrisation, trouver le meilleur découpage nécessite une optimisation complexe, souvent irréaliste en pratique.</p></li>
</ul>
</section>
<section id="les-solutions-apportées-par-les-arbres-de-décision" class="level3" data-number="1.1.2">
<h3 data-number="1.1.2" class="anchored" data-anchor-id="les-solutions-apportées-par-les-arbres-de-décision"><span class="header-section-number">1.1.2</span> Les solutions apportées par les arbres de décision</h3>
<p>Pour surmonter ces difficultés, les algorithmes d’arbres de décision, et notamment le plus célèbre, l’algorithme CART (Classification And Regression Tree, <span class="citation" data-cites="breiman1984cart">Breiman et al. (<a href="#ref-breiman1984cart" role="doc-biblioref">1984</a>)</span>), procèdent à trois simplifications cruciales:</p>
<ol type="1">
<li><p><strong>Optimisation gloutonne (<strong>greedy optimization</strong>)</strong>: plutôt que de rechercher d’emblée un partitionnement optimal, les arbres de décision partitionnent l’espace selon une approche séquentielle. A chaque étape, l’arbre choisit la meilleure division possible d’une région en deux sous-régions, <em>indépendamment des étapes précédentes ou suivantes</em>. Ce processus est répété pour chaque sous-région, ce qui permet d’affiner progressivement le partitionnement de l’espace, jusqu’à ce qu’un critère d’arrêt soit atteint. Cette méthode dite “gloutonne” (<em>greedy</em>) s’avère très efficace, car elle décompose un problème d’optimisation complexe en une succession de problèmes plus simples et plus rapides à résoudre. Le résultat obtenu n’est pas nécessairement un optimum global, mais il s’en approche raisonnablement et surtout rapidement.</p></li>
<li><p><strong>Simplification des règles de partitionnement</strong>: au lieu d’explorer tous les règles de décision possibles, les arbres de décision se restreignent à des règles de décision très simples, appelés <strong>découpages binaires</strong> (<em>binary splits</em>): à chaque étape, l’algorithme divise chaque région de l’espace en deux sous-régions à l’aide d’une règle de décision (<em>decision rule</em>) qui ne fait appel qu’à <strong>une seule caractéristique</strong> (ou <em>variable</em>) et à <strong>un seul seuil</strong> (ou <em>critère</em>) pour cette segmentation. Cela revient à poser une question simple telle que: “La valeur de la caractéristique <span class="math inline">\(X\)</span> dépasse-t-elle le seuil <span class="math inline">\(x\)</span> ?” Par exemple: “La superficie de la maison est-elle supérieure à 100 m² ?”. Les deux réponses possibles (“Oui” ou “Non”) définissent deux nouvelles sous-régions distinctes de l’espace, chacune correspondant à un sous-ensemble de données plus homogènes.</p></li>
<li><p><strong>Simplicité des prédictions locales</strong>: une fois le partitionnement réalisé, une prédiction est calculée pour chaque région à partir des observations des données d’entraînement présentes dans cette région. Il s’agit souvent de la moyenne des valeurs cibles dans cette région (régression) ou de la classe majoritaire (classification). Un point essentiel est que la prédiction est constante au sein de chaque région.</p></li>
</ol>
<!-- En raison de leur nature **non-continue** et **non-différentiable**, il est impossible d'utiliser des méthodes d'optimisation classiques reposant sur le calcul de gradients. -->
</section>
<section id="terminologie-et-structure-dun-arbre-de-décision" class="level3" data-number="1.1.3">
<h3 data-number="1.1.3" class="anchored" data-anchor-id="terminologie-et-structure-dun-arbre-de-décision"><span class="header-section-number">1.1.3</span> Terminologie et structure d’un arbre de décision</h3>
<p>Cet algorithme est appelé <strong>arbre de décision</strong> (<em>decision tree</em>) en raison provient de la structure arborescente en forme d’arbre inversé qui apparaît lorsqu’on en fait une représentation graphique (voir figure <a href="#fig-decision-tree-description" class="quarto-xref">1</a>). Plus généralement, les principaux éléments qui composent les arbres de décision sont désignés par des termes issus du champ lexical des arbres:</p>
<ul>
<li><p><strong>Nœud Racine (<em>Root Node</em>)</strong>: Le nœud-racine est le point de départ de l’arbre de décision, il est situé au sommet de l’arbre. Il contient l’ensemble des données d’entraînement avant tout partitionnement. À ce niveau, l’algorithme cherche la caractéristique la plus discriminante, c’est-à-dire celle qui permet de diviser les données en deux régions de manière à minimiser un certain critère d’hétérogénéité (comme l’indice de Gini pour la classification ou la variance pour la régression).</p></li>
<li><p><strong>Nœuds Internes (<em>Internal Nodes</em>)</strong>: Les nœuds internes sont les points intermédiaires où l’algorithme CART applique des règles de décision pour diviser les données en sous-régions plus petites. Chaque nœud interne se définit par une <strong>règle de décision</strong> basée sur une variable et un seuil, qui sert à opérer un partitionnement des données. Le plus souvent, la règle de décision est exprimée sous la forme d’une inégalité telle que <code>x_3 &lt;= 7</code>. Chaque nœud interne a la fois un <strong>nœud-parent</strong> (<em>parent node</em>) dont il constitue une sous-région et deux <strong>nœuds-enfants</strong> (<em>child nodes</em>) qui le partitionnent. Le noeud de gauche regroupe les observations pour lesquelles la règle de décision est vérifiée (valeurs basses); le noeud de droite regroupe les observations pour lesquelles la règle de décision n’est pas vérifiée (valeurs élevées).</p></li>
<li><p><strong>Branches (<em>Branches</em>)</strong>: Les branches sont les connexions entre les nœuds et représentent le chemin suivies par les données. Chaque branche correspond à une décision binaire, “Oui” ou “Non”, qui oriente les observations vers une nouvelle subdivision de l’espace des caractéristiques.</p></li>
<li><p><strong>Nœuds Terminaux ou Feuilles (<em>Leaf Nodes</em>, <em>Terminal Nodes</em> ou <em>Leaves</em>)</strong>: Les nœuds terminaux, situés à l’extrémité des branches, sont les points où le processus de division s’arrête. Ils fournissent la prédiction finale. Dans un problème de classification, la prédiction d’une feuille est soit la classe majoritaire parmi les observations de la feuille (par exemple, “Oui” ou “Non”), soit une probabilité d’appartenir à chaque classe. Dans un problème de régression, la prédiction d’une feuille est une valeur numérique, souvent la moyenne des observations de la feuille.</p></li>
<li><p><strong>Profondeur</strong> (<em>Depth</em>): La profondeur d’un arbre de décision correspond à la longueur du chemin le plus long entre le nœud-racine et une feuille. Le nœud-racine est situé par définition à la profondeur 0, et chaque niveau supplémentaire de l’arbre ajoute une unité à la profondeur. La profondeur totale de l’arbre est donc le nombre maximal de décisions (ou de nœuds internes) à traverser pour passer du nœud-racine à une feuille.</p></li>
</ul>
<div id="fig-decision-tree-description" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-decision-tree-description-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../figures/decision_tree_description.svg" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-decision-tree-description-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Structure d’un arbre de décision
</figcaption>
</figure>
</div>
</section>
<section id="propriétés-des-arbres-de-décision" class="level3" data-number="1.1.4">
<h3 data-number="1.1.4" class="anchored" data-anchor-id="propriétés-des-arbres-de-décision"><span class="header-section-number">1.1.4</span> Propriétés des arbres de décision</h3>
<p>Les arbres de décision ont plusieurs propriétés qui contribuent à leur puissance prédictive et facilitent leur usage en pratique:</p>
<ul>
<li><p><strong>Les arbres de décision ne font aucune hypothèse <em>a priori</em> sur la relation entre les variables explicatives et la variable-cible</strong>. C’est une différence majeure avec les modèles économétriques standards, tels que la régression linéaire qui suppose une relation linéaire de la forme <span class="math inline">\(E(y) = \mathbf{X \beta}\)</span>.</p></li>
<li><p><strong>Un arbre de décision est une fonction constante par morceaux</strong>: la prédiction est <strong>identique</strong> pour toutes les observations situées dans la même région, et ne peut varier qu’entre régions<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. Une conséquence de cette propriété est qu’<strong>un arbre de décision peut capter sans difficultés les non-linéarités</strong> dans la relation entre la variable-cible et les variables numériques (voir la figure <a href="#fig-nonlinearite" class="quarto-xref">2</a>). Il est donc inutile d’inclure des variables supplémentaires telles que le carré ou le cube des variables continues.</p>
<div id="fig-nonlinearite" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nonlinearite-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../figures/non_linearite.png" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nonlinearite-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Arbre de décision et non-linéarité
</figcaption>
</figure>
</div></li>
<li><p><strong>Les arbres de décision sont par construction capables de capter des interactions entre variables explicatives sans qu’il soit nécessaire de les spécifier explicitement</strong>. En effet, la prédiction pour une observation dépend de la combinaison des différentes variables intervenant dans les règles de décision qui mènent à la feuille terminale, ce qui traduit une interaction entre les variables. La figure <a href="#fig-decision-tree-interaction" class="quarto-xref">3</a> illustre ces interactions avec un arbre de décision qui prédit le salaire en fonction de l’âge, du niveau d’étude, et de l’expérience. On voit d’une part que les feuilles terminales sont définies par la conjonction de règles de décision qui font intervenir ces trois variables, avec des seuils différents selon les branches de l’arbre, et d’autre part que l’effet d’une caractéristique sur le salaire dépend des autres caractéristiques de l’individu. Par exemple, une augmentation de l’expérience de 7 à 8 années se traduira une augmentation de salaire de 100 € si l’individu a moins de 35 ans et un niveau d’études inférieur au bac, par une augmentation de 200 € s’il a plus de 35 ans et un niveau d’étude supérieur à Bac+ 3, et sera sans effet sur le salaire dans les autres cas.</p>
<div id="fig-decision-tree-interaction" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-decision-tree-interaction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../figures/decision_tree_interaction.svg" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-decision-tree-interaction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Interactions dans un arbre de décision
</figcaption>
</figure>
</div></li>
<li><p><strong>Dans un arbre de décision, les valeurs prises par les variables numériques (par exemple l’âge) n’ont pas d’importance par elles-mêmes, c’est l’<em>ordre</em> de ces valeurs qui est essentiel</strong>. Ainsi, dans la règle de décision “L’âge est-il inférieur à 30 ans?”, ce n’est pas la valeur “30” qui importe par elle-même, c’est le fait qu’elle <strong>sépare les observations</strong> en deux groupes, selon que l’âge est inférieur ou supérieur à 30 ans. Cette propriété a pour conséquence que les arbres de décision sont insensibles aux modifications strictement monotones des variables continues. Par exemple, remplacer l’âge par l’âge au carré ne changera rien à l’arbre de décision, car les règles de décision “L’âge est-il inférieur à 30 ans?” et “L’âge au carré est-il inférieur à 900?” sont strictement équivalentes (car elles définissent les mêmes groupes).</p></li>
</ul>
<!-- ### Illustration

Supposons que nous souhaitions prédire le prix d'une maison en fonction de sa superficie et de son nombre de pièces. Un arbre de décision pourrait procéder ainsi:

1. **Première division**: "La superficie de la maison est-elle inférieure à 100 m² ?"
   - Oui: Aller à la branche de gauche.
   - Non: Aller à la branche de droite.
2. **Deuxième division (branche de gauche)**: "Le nombre de pièces est-il inférieur à 4 ?"
   - Oui: Prix élevé (par exemple, plus de 300 000 €).
   - Non: Prix moyen (par exemple, entre 200 000 € et 300 000 €).
3. **Deuxième division (branche de droite)**: "Le nombre de pièces est-il inférieur à 2 ?"
   - Oui: Prix moyen (par exemple, entre 150 000 € et 200 000 €).
   - Non: Prix bas (par exemple, moins de 150 000 €).

Cet arbre utilise des règles simples pour diviser l'espace des caractéristiques (superficie et nombre de pièces) en sous-groupes homogènes et fournir une prédiction (estimer le prix d'une maison). -->
</section>
</section>
<section id="sec-construire" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="sec-construire"><span class="header-section-number">1.2</span> La construction d’un arbre de décision par l’algorithme CART</h2>
<p>Depuis les années 1980, de multiples algorithmes ont été proposés pour construire des arbres de décision, notamment CART (<span class="citation" data-cites="breiman1984cart">Breiman et al. (<a href="#ref-breiman1984cart" role="doc-biblioref">1984</a>)</span>), C4.5 (<span class="citation" data-cites="quinlan2014c4">Quinlan (<a href="#ref-quinlan2014c4" role="doc-biblioref">2014</a>)</span>) et MARS (<span class="citation" data-cites="friedman1991multivariate">Friedman (<a href="#ref-friedman1991multivariate" role="doc-biblioref">1991</a>)</span>). La présente section présente la méthode de construction et l’utilisation d’un arbre de décision par l’algorithme CART. Cette méthode comprend quatre étapes:</p>
<ul>
<li><p>Choisir une mesure d’impureté adaptée au problème;</p></li>
<li><p>Construire l’arbre de décision par un partitionnement séquentiel;</p></li>
<li><p>Élaguer l’arbre de décision;</p></li>
<li><p>Utiliser l’arbre pour prédire.</p></li>
</ul>
<section id="définir-une-mesure-dimpureté-adaptée-au-problème" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="définir-une-mesure-dimpureté-adaptée-au-problème"><span class="header-section-number">1.2.1</span> Définir une mesure d’impureté adaptée au problème</h3>
<p>La <strong>mesure d’impureté</strong> quantifie l’hétérogénéité des observations au sein d’un nœud par rapport à la variable cible (classe pour la classification, ou valeur continue pour la régression). Plus précisément, une mesure d’impureté est conçue pour croître avec la dispersion dans un nœud: plus un nœud est homogène, plus son impureté est faible. Un nœud est dit <strong>pur</strong> lorsque toutes les observations qu’il contient appartiennent à la même classe (classification) ou présentent des valeurs similaires voire identiques (régression). Le choix de la mesure d’impureté dépend du type de problème (voir ci-dessous).</p>
<!-- - **Classification**: L'**indice de Gini** ou l'**entropie** sont très souvent utilisées pour évaluer la dispersion des classes dans chaque nœud. 

- **Régression**: La **somme des erreurs quadratiques** (SSE) est souvent utilisée pour mesurer la variance des valeurs cibles dans chaque nœud.  -->
<p><strong>La mesure d’impureté est un élément essentiel de la construction des arbres de décision.</strong> En effet, c’est elle qui est utilisée pour comparer entre elles les règles de décision possibles. À chaque étape de la croissance de l’arbre (<em>tree growing</em>), l’algorithme sélectionne la règle de décision qui réduit le plus l’impureté, afin de définir des nœuds les plus homogènes possibles. L’arbre final dépend donc de la mesure d’impureté utilisée: si pour un problème donné on construit un second arbre avec une autre mesure d’impureté, on obtient généralement un arbre différent du premier (car les règles de décision retenues à chaque nœud ne sont plus les mêmes).</p>
<section id="mesures-dimpureté-pour-les-problèmes-de-classification" class="level4" data-number="1.2.1.1">
<h4 data-number="1.2.1.1" class="anchored" data-anchor-id="mesures-dimpureté-pour-les-problèmes-de-classification"><span class="header-section-number">1.2.1.1</span> Mesures d’impureté pour les problèmes de classification</h4>
<p>Dans un problème de classification où l’on souhaite classifier des observations parmi <span class="math inline">\(K\)</span> classes, une <strong>mesure d’impureté</strong> <span class="math inline">\(I(t)\)</span> est une fonction qui quantifie l’hétérogénéité des classes dans un nœud donnée. Les mesures d’impureté usuelles détaillées ci-dessous partagent les deux propriétés suivantes:</p>
<ul>
<li><p><strong>Pureté maximale</strong>: lorsque toutes les observations du nœud appartiennent à une seule classe, c’est-à-dire que la proportion <span class="math inline">\(p_k = 1\)</span> pour une classe <span class="math inline">\(k\)</span> et <span class="math inline">\(p_j = 0\)</span> pour toutes les autres classes <span class="math inline">\(j \neq k\)</span>, l’impureté est minimale et <span class="math inline">\(I(t) = 0\)</span>. Cela indique que le nœud est <strong>entièrement pur</strong>, ou homogène.</p></li>
<li><p><strong>Impureté maximale</strong>: lorsque les observations sont réparties de manière uniforme entre toutes les classes, c’est-à-dire que la proportion <span class="math inline">\(p_k = \frac{1}{K}\)</span> pour chaque classe <span class="math inline">\(k\)</span>, l’impureté atteint son maximum. Cette situation reflète une <strong>impureté élevée</strong>, car le nœud est très hétérogène et contient une forte incertitude sur la classe des observations.</p></li>
</ul>
<p>Il existe trois mesures d’impureté couramment utilisées en classification:</p>
<p><strong>1. L’indice de Gini</strong></p>
<p>L’<strong>indice de Gini</strong> mesure la probabilité qu’un individu sélectionné au hasard dans un nœud soit mal classé si on lui attribue une classe au hasard, en fonction de la distribution des classes dans ce nœud. Pour un nœud <span class="math inline">\(t\)</span> contenant <span class="math inline">\(K\)</span> classes, l’indice de Gini <span class="math inline">\(G(t)\)</span> est donné par</p>
<p><span class="math display">\[ G(t) = 1 - \sum_{k=1}^{K} p_k^2 \]</span></p>
<p>où <span class="math inline">\(p_k\)</span> est la proportion d’observations appartenant à la classe <span class="math inline">\(k\)</span> dans le nœud <span class="math inline">\(t\)</span>.</p>
<p><strong>Critère de choix</strong>: L’indice de Gini est très souvent utilisé parce qu’il est simple à calculer et capture bien l’homogénéité des classes au sein d’un nœud. Il privilégie les partitions où une classe domine fortement dans chaque sous-région.</p>
<p><strong>2. L’entropie (ou entropie de Shannon)</strong></p>
<p>L’<strong>entropie</strong> est une autre mesure de l’impureté utilisée dans les arbres de décision. Elle mesure la quantité d’incertitude ou de désordre dans un nœud, en s’appuyant sur la théorie de l’information. Pour un nœud <span class="math inline">\(t\)</span> contenant <span class="math inline">\(K\)</span> classes, l’entropie <span class="math inline">\(E(t)\)</span> est définie par:</p>
<p><span class="math display">\[
E(t) = - \sum_{k=1}^{K} p_k \log(p_k)
\]</span></p>
<p>où <span class="math inline">\(p_k\)</span> est la proportion d’observations de la classe <span class="math inline">\(k\)</span> dans le nœud <span class="math inline">\(t\)</span>.</p>
<!-- **Propriété**:

- Comme pour l'indice de Gini, si toutes les observations d'un nœud appartiennent à la même classe, l'entropie est nulle ($E(t) = 0$), indiquant un nœud pur.

- L'entropie atteint son maximum lorsque les observations sont uniformément réparties entre les classes, reflétant une grande incertitude dans la classification.-->
<p><strong>Critère de choix</strong>: L’entropie a tendance à être plus sensible aux changements dans les distributions des classes que l’indice de Gini, car elle attribue un poids plus élevé aux événements rares (valeurs de <span class="math inline">\(p_k\)</span> très faibles). Elle est souvent utilisée lorsque l’erreur de classification des classes minoritaires est particulièrement importante.</p>
<p><strong>3. Taux d’erreur</strong></p>
<p>Le <strong>taux d’erreur</strong> est une autre mesure de l’impureté parfois utilisée dans les arbres de décision. Il représente la proportion d’observations mal classées dans un nœud. Pour un nœud <span class="math inline">\(t\)</span>, le taux d’erreur <span class="math inline">\(\text{TE}(t)\)</span> est donné par:</p>
<p><span class="math display">\[
\text{TE}(t) = 1 - \max(p_k)
\]</span></p>
<p>où <span class="math inline">\(\max(p_k)\)</span> est la proportion d’observations appartenant à la classe majoritaire dans le nœud.</p>
<!-- **Propriété**:

- Si toutes les observations d'un nœud appartiennent à la même classe, le taux d'erreur est nul ($\text{TE}(t) = 0$), indiquant un nœud pur.

- Le taux d'erreur atteint son maximum lorsque les observations sont uniformément réparties entre les classes, reflétant une grande incertitude dans la classification.-->
<p><strong>Critère de choix</strong>: Bien que le taux d’erreur soit simple à comprendre, il est moins souvent utilisé dans la construction des arbres de décision parce qu’il est moins sensible que l’indice de Gini ou l’entropie aux petits changements dans la distribution des classes.</p>
</section>
<section id="mesures-dimpureté-pour-les-problèmes-de-régression" class="level4" data-number="1.2.1.2">
<h4 data-number="1.2.1.2" class="anchored" data-anchor-id="mesures-dimpureté-pour-les-problèmes-de-régression"><span class="header-section-number">1.2.1.2</span> Mesures d’impureté pour les problèmes de régression</h4>
<!-- Dans un problème de classification où l'on souhaite classifier des observations parmi $K$ classes, une **mesure d'impureté** $I(t)$ est une fonction qui quantifie l'hétérogénéité des classes dans un nœud donnée. Les mesures d'impureté usuelles détaillées ci-dessous partagent les deux propriétés suivantes: -->
<p>Dans les problèmes de régression, l’objectif est de partitionner les données de manière à réduire au maximum la variabilité des valeurs au sein de chaque sous-région. Pour mesurer cette variabilité, la mesure d’impureté la plus couramment employée est la somme des erreurs quadratiques (SSE). Elle évalue l’impureté d’une région en quantifiant à quel point les valeurs de cette région s’écartent de la moyenne locale. Pour un nœud <span class="math inline">\(t\)</span>, contenant <span class="math inline">\(N\)</span> observations avec des valeurs <span class="math inline">\(y_i\)</span>, la SSE est donnée par:</p>
<p><span class="math display">\[
\text{SSE}(t) = \sum_{i=1}^{N} (y_i - \hat{y})^2
\]</span></p>
<p>où <span class="math inline">\(\hat{y}\)</span> est la moyenne des valeurs <span class="math inline">\(y_i\)</span> dans le nœud.</p>
<p>Cette mesure d’impureté a des propriétés similaires à celles présentées pour la classification: si toutes les valeurs de <span class="math inline">\(y_i\)</span> dans un nœud sont proches de la moyenne <span class="math inline">\(\hat{y}\)</span>, la SSE sera faible, indiquant une homogénéité élevée dans le nœud. Inversement, une SSE élevée indique une grande variabilité dans les valeurs, donc un nœud impur. Une limite de cette mesure d’impureté est qu’elle est particulièrement sensible aux écarts élevés entre les valeurs observées et la moyenne prédite, et donc aux valeurs extrêmes.</p>
</section>
</section>
<section id="construire-larbre-de-décision-par-un-partitionnement-séquentiel" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="construire-larbre-de-décision-par-un-partitionnement-séquentiel"><span class="header-section-number">1.2.2</span> Construire l’arbre de décision par un partitionnement séquentiel</h3>
<p>Une fois la mesure d’impureté définie, l’algorithme CART construit séquentiellement le partitionnement de l’espace des caractéristiques en comparant les règles de décision possibles (voir figure <a href="#fig-decision-tree-construction" class="quarto-xref">4</a>). La première étape de partitionnement part du nœud-racine, qui comprend l’ensemble des données d’entraînement. L’algorithme construit toutes les règles de décision candidates en parcourant toutes les valeurs de toutes les caractéristiques, les évalue en calculant la réduction de l’impureté induite par chacune d’entre elles et sélectionne la règle de décision (caractéristique et seuil) qui entraîne la réduction d’impureté maximale. Par exemple, l’algorithme évalue la règle candidate “Superficie &gt; 100 m²” en calculant la somme des impuretés au sein des deux sous-régions générées par cette règle (“Oui” et “Non”), puis calcule la différence entre cette somme et l’impureté du noeud-racine. L’algorithme évalue ensuite la règle candidate “Superficie &gt; 101 m²” de la même façon, et ainsi de suite pour toutes les valeurs de superficie, puis évalue les règles candidates construites avec le nombre de pièces, et enfin sélectionne la meilleure règle. La deuxième étape du partitionnement reproduit le même processus, cette fois au niveau d’un des deux nœuds-enfants, et ainsi de suite.</p>
<div id="fig-decision-tree-construction" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-decision-tree-construction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../figures/decision_tree_construction.svg" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-decision-tree-construction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Construction d’un arbre de décision
</figcaption>
</figure>
</div>
<p>L’algorithme CART poursuit ce partitionnement récursif jusqu’à ce qu’un <strong>critère d’arrêt</strong> prédéfini soit atteint. Dans la plupart des implémentations de CART, les valeurs par défaut de ces critères d’arrêt sont telles que que l’algorithme construit un arbre maximal: le plus profond possible, avec une observation par feuille terminale.</p>
</section>
<section id="contrôler-la-complexité-de-larbre" class="level3" data-number="1.2.3">
<h3 data-number="1.2.3" class="anchored" data-anchor-id="contrôler-la-complexité-de-larbre"><span class="header-section-number">1.2.3</span> Contrôler la complexité de l’arbre</h3>
<p>Il est généralement préférable d’éviter les arbres trop complexes car ils sont souvent affectés par un problème de surajustement. Deux approches permettent de contrôler la complexité d’un arbre de décision:</p>
<ul>
<li><p>Approche <em>a priori</em>: la complexité de l’arbre peut être plafonnée pendant sa construction à l’aide d’hyperparamètres telles que la profondeur maximale de l’arbre, le nombre minimal d’observations par feuille ou la réduction minimale de l’impureté nécessaire à chaque étape pour ajouter un noeud interne. Cette approche est simple à mettre en oeuvre, mais peut aboutir à des arbres trop simples et peu prédictifs si les hyperparamètres sont mal choisis.</p></li>
<li><p>Approche <em>a posteriori</em>: l’autre approche consiste à construire un arbre maximal puis à procéder à un <strong>élagage</strong> (<em>tree pruning</em>) qui vise à simplifier l’arbre et à augmenter sa capacité à généraliser sur de nouvelles données en supprimant progressivement les branches les moins utiles. Il existe différents critères d’élagage, parmi lesquels le chemin de coût-complexité <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p></li>
</ul>
</section>
<section id="utiliser-larbre-pour-prédire" class="level3" data-number="1.2.4">
<h3 data-number="1.2.4" class="anchored" data-anchor-id="utiliser-larbre-pour-prédire"><span class="header-section-number">1.2.4</span> Utiliser l’arbre pour prédire</h3>
<p>Une fois l’arbre construit, la prédiction pour une nouvelle observation s’effectue en suivant les branches de l’arbre depuis le nœud racine jusqu’à un nœud terminal (ou feuille), comme l’illustre la figure @#fig-decision-tree-prediction. À chaque nœud interne, une décision est prise en fonction des valeurs des caractéristiques de l’observation, ce qui détermine la direction à suivre vers l’une des sous-régions. Ce cheminement se poursuit jusqu’à ce que l’observation atteigne une feuille, où la prédiction finale sera simplement la valeur associée à cette feuille.</p>
<div id="fig-decision-tree-prediction" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-decision-tree-prediction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../figures/decision_tree_prediction.svg" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-decision-tree-prediction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Prédire avec un arbre de décision
</figcaption>
</figure>
</div>
</section>
</section>
<section id="avantages-et-limites-des-arbres-de-décision" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="avantages-et-limites-des-arbres-de-décision"><span class="header-section-number">1.3</span> Avantages et limites des arbres de décision</h2>
<section id="avantages" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="avantages"><span class="header-section-number">1.3.1</span> Avantages</h3>
<ul>
<li><p><strong>Simplicité et interprétabilité</strong>: Les arbres de décision sont faciles à comprendre et à visualiser (à condition qu’ils ne soient pas trop profonds).</p></li>
<li><p><strong>Facilité d’usage</strong>: les arbres de décision ne demandent pas de transformations complexes des données.</p></li>
<li><p><strong>Flexibilité</strong>: Ils peuvent gérer des caractéristiques numériques et catégorielles, ainsi que les valeurs manquantes.</p></li>
<li><p><strong>Gestion des interactions</strong>: Les arbres sont des modèles non paramétriques et ne font aucune hypothèse sur la distribution des variables. Ils capturent aisément les relations non-linéaires et les interactions entre les caractéristiques.</p></li>
</ul>
</section>
<section id="limites" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="limites"><span class="header-section-number">1.3.2</span> Limites</h3>
<ul>
<li><p><strong>Surapprentissage</strong>: Les arbres de décision peuvent facilement devenir trop complexes et être surajustés d’entraînement, ce qui nuit à leur capacité prédictive sur de nouvelles données.</p></li>
<li><p><strong>Biais envers les classes majoritaires</strong>: En présence de données déséquilibrées, les arbres de décision peuvent privilégier la classe majoritaire, ce qui dégrade la performance sur les classes minoritaires.</p></li>
<li><p><strong>Optimisation locale</strong>: L’approche gloutonne peut conduire à des solutions globalement sous-optimales (optimum local).</p></li>
<li><p><strong>Instabilité</strong>: De petits changements dans les données peuvent entraîner des changements significatifs dans la structure de l’arbre (manque de robustesse).</p></li>
</ul>



</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-breiman1984cart" class="csl-entry" role="listitem">
Breiman, Leo, Jerome Friedman, Richard Olshen, and Charles Stone. 1984. <span>“Cart.”</span> <em>Classification and Regression Trees</em>.
</div>
<div id="ref-friedman1991multivariate" class="csl-entry" role="listitem">
Friedman, Jerome H. 1991. <span>“Multivariate Adaptive Regression Splines.”</span> <em>The Annals of Statistics</em> 19 (1): 1–67.
</div>
<div id="ref-quinlan2014c4" class="csl-entry" role="listitem">
Quinlan, J Ross. 2014. <em>C4. 5: Programs for Machine Learning</em>. Elsevier.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Il existe néanmoins des variantes d’arbres de décision où la prédiction n’est pas constante au sein de chaque feuille, mais elles sont peu courantes en pratique et ne sont pas couvertes par le présent document<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Les approches d’élagage sont détaillées notamment dans ce <a href="https://eric.univ-lyon2.fr/ricco/cours/slides/cart_post_elagage_arbres_decision.pdf">cours</a> et dans la <a href="https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html">documentation de <code>scikit-learn</code></a>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/github\.com\/inseefrlab\/DT_methodes_ensemblistes");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../chapters/chapter1/1-survol.html" class="pagination-link" aria-label="Survol des méthodes ensemblistes">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Survol des méthodes ensemblistes</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/chapter2/2-bagging.html" class="pagination-link" aria-label="Le _bagging_">
        <span class="nav-page-text">Le <em>bagging</em></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© CC-1.0</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/inseefrlab/DT_methodes_ensemblistes/edit/main/chapters/chapter2/1-CART.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/inseefrlab/DT_methodes_ensemblistes/blob/main/chapters/chapter2/1-CART.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li><li><a href="https://github.com/inseefrlab/DT_methodes_ensemblistes/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This page is built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>