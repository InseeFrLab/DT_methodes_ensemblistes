---
title: "Hello Typst!"
author: |
  [Olivier Meslin](https://github.com/oliviermeslin)
  [Mélina Hillion](https://github.com/melinahillion)
format:
  typst:
    toc: true
    section-numbering: 1.1.1
---

Restriction du champ: méthodes ensemblistes à base d'arbres.

Lecture de base: chapitres 9-12: https://bradleyboehmke.github.io/HOML/

# Survol des méthodes ensemblistes

__Principe__: 

## Principe des méthodes ensemblistes

Plutôt que de chercher à construire d'emblée un unique modèle très complexe, les approches ensemblistes vise à obtenir un modèle très performant en combinant un grand nombre de modèles simples.

Il existe trois grandes approches ensemblistes: 

- le _bagging_;
- le _stacking_;
- le _boosting_.

Le présent document se concentre sur deux approches: le _bagging_ et le _boosting_.

## Pourquoi utiliser des méthodes ensemblistes?

Avantages:

Inconvénients:

## Comment fonctionnent les méthodes ensemblistes?

Présentation intuitive et peu matheuse


# Présentation des algorithmes

Quatre temps:

- arbres de décision et de régression (CART);
- bagging;
- forêts aléatoires;
- boosting.

## Le point de départ: les arbres de décision et de régression

Présenter _decision tree_ et _regression tree_. Reprendre des éléments du chapitre 9 de https://bradleyboehmke.github.io/HOML/

Principes d'un arbre: 

- fonction constante par morceaux;
- partition de l'espace;



- interactions entre variables.

Illustration, et représentation graphique.

## Le _bagging_

Reprendre des éléments du chapitre 10 de https://bradleyboehmke.github.io/HOML/

Mettre une description de l'algorithme en pseudo-code?

## Les _random forests_

Reprendre des éléments du chapitre 11 de https://bradleyboehmke.github.io/HOML/

Mettre une description de l'algorithme en pseudo-code?

<!-- https://neptune.ai/blog/ensemble-learning-guide -->
<!-- https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/ -->

- Présentation;
- Bien insister sur les avantages des RF: 1/ faible nombre d'hyperparamètres; 2/ faible sensibilité aux hyperparamètres; 3/ limite intrinsèque à l'overfitting.

## Le _boosting_

Reprendre des éléments du chapitre 12 de https://bradleyboehmke.github.io/HOML/ et des éléments de la formation boosting.

Le *boosting* combine l'[**approche ensembliste**]{.orange} avec une [**modélisation additive par étapes**]{.orange} (*forward stagewise additive modeling*).

Mettre une description de l'algorithme en pseudo-code?

- Présentation;
- Avantage du boosting: performances particulièrement élevées.
- Inconvénients: 1/ nombre élevé d'hyperparamètres; 2/ sensibilité des performances aux hyperparamètres; 3/ risque élevé d'overfitting.

- Préciser qu'il est possible d'utiliser du subsampling par lignes et colonnes pour un algoithme de boosting. Ce point est abordé plus en détail dans la partie sur les hyperparamètres.

# Présentation détaillée du _gradient boosting_

Attention présentation détaillée et matheuse. Cette partie porte précisément sur l'optimisation par le gradient.

Bien préciser:

- il existe des implémentations du _boosting_ qui ne sont pas du _gradient boosting_ (exemple: l'_adaptative boosting_ de l'algorithme AdaBoost).
- Il existe de multiples implémentations du _gradient boosting_ (GBM, lightGBM, XGBoost, Catboost...), globalement similaires mais qui diffèrent sur des points de détail. La présentation qui suit doit donc être complétée par la lecture de la documentation des différents algorithmes.  
- cette approche permet de construire des forêts aléatoires et des modèles de _boosting_. 

L'exposé qui suit reprend les notations de l'article qui a introduit XGBoost (2016). Il est important de numéroter les équations pour faire le lien entre la partie qui liste les hyperparamètres et les équations dans lesquels ils interviennent.

## Rappels sur l'apprentissage supervisé

Reprendre les éléments figurant dans la formation boosting.

## La mécanique du boosting

Reprendre les éléments figurant dans la formation boosting.

## La construction d'un arbre par la descente de gradient

Donner les formules d'XGBoost. Donner un algorithme en pseudo code décrivant la façon dont XGBoost énumère les splits possibles.

## Deux points importants

- La gestion des variables continues (quantization par histogramme);
- La gestion des variables catégorielles.



# Comment utiliser ces algorithmes

## Différence entre RF et _boosting_

Comment choisir entre forêt aléatoire et boosting

## Rôle et interprétation des principaux hyperparamètres

gamma, beta, alpha, lambda, eta, M, T, nb de quantiles;
method = "hist"

## Diagnostics post-entraînement

Mesure d'importance: intérêt et limites.

# Cas d'usage

- Données (pouvant être rendues) publiques
- Notebooks déployables sur le datalab
- Code en Python

## Régression

### Cas général

### Régression en présence d'outliers

=> Changement de fonction de perte

## Classification

### Cas général

### Classification déséquilibrée

=> Pondération de la classe minoritaire

<!-- IMPORTANT -->
<!-- Formations sur le ML -->
<!-- https://github.com/davidrpugh/machine-learning-for-tabular-data -->
<!-- IMPORTANT -->

<!-- Petites questions: -->
<!-- - Quelle implémentation des RF veut-on présenter? Je suis favorable à avoir un seul framework RF/Boosting, mais c'est peut-être pas standard. Quelques références: -->

<!--     - https://konfuzio.com/en/random-forest/ -->
<!--     - https://xgboost.readthedocs.io/en/stable/tutorials/rf.html -->



<!-- Petites notes complémentaires -->
<!-- - Interprétabilité: https://selfexplainml.github.io/PiML-Toolbox/_build/html/index.html -->
<!-- - Comparaison arbres et autres (Papier R Avouac): https://proceedings.neurips.cc/paper_files/paper/2022/file/0378c7692da36807bdec87ab043cdadc-Paper-Datasets_and_Benchmarks.pdf -->


