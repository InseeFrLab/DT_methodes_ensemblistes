[
  {
    "objectID": "chapters/chapter2/boosting.html",
    "href": "chapters/chapter2/boosting.html",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "",
    "text": "Le fondement théorique du boosting est un article de de 1990 (Shapire (1990)) qui a démontré théoriquement que, sous certaines conditions, il est possible de transformer un modèle prédictif peu performant en un modèle prédictif très performant. Plus précisément, un modèle ayant un pouvoir prédictif arbitrairement élevé (appelé strong learner) peut être construit en combinant des modèles simples dont les prédictions ne sont que légèrement meilleures que le hasard (appelé weak learners). Le boosting est donc une méthode qui combine une approche ensembliste reposant sur un grand nombre de modèles simples avec un entraînement séquentiel: chaque modèle simple (souvent des arbres de décision peu profonds) tâche d’améliorer la prédiction globale en corrigeant les erreurs des prédictions précédentes à chaque étape. Bien qu’une approche de boosting puisse en théorie mobiliser différentes classes de weak learners, en pratique les weak learners utilisés par les algorithmes de boosting sont presque toujours des arbres de décision.\nS’il existe plusieurs variantes, les algorithmes de boosting suivent la même logique :\n\nUn premier modèle simple et peu performant est entraîné sur les données.\nUn deuxième modèle est entraîné de façon à corriger les erreurs du premier modèle (par exemple en pondérant davantage les observations mal prédites);\nCe processus est répété en ajoutant des modèles simples, chaque modèle corrigeant les erreurs commises par l’ensemble des modèles précédents;\nTous ces modèles sont finalement combinés (souvent par une somme pondérée) pour obtenir un modèle complexe et performant.\n\nEn termes plus techniques, les algorithmes de boosting partagent trois caractéristiques communes:\n\nIls visent à trouver une approximation \\(\\hat{F}\\)“)`{=typst} d’une fonction inconnue \\(F*: \\mathbf{x} \\mapsto y\\) à partir d’un ensemble d’entraînement \\((y_i, \\mathbf{x_i})_{i= 1,\\dots,n}\\);\nIls supposent que la fonction \\(F*\\) peut être approchée par une somme pondérée de modèles simples \\(f\\) de paramètres \\(\\theta\\):\n\n\\[ F\\left(\\mathbf{x}\\right) = \\sum_{m=1}^M \\beta_m f\\left(\\mathbf{x}, \\mathbf{\\theta}_m\\right) \\]\n\nils reposent sur une modélisation additive par étapes, qui décompose l’entraînement de ce modèle complexe en une séquence d’entraînements de petits modèles. Chaque étape de l’entraînement cherche le modèle simple \\(f\\) qui améliore la puissance prédictive du modèle complet, sans modifier les modèles précédents, puis l’ajoute de façon incrémentale à ces derniers:\n\n\\[ F_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\hat{\\beta}_m f(\\mathbf{x}_i, \\mathbf{\\hat{\\theta}_m}) \\]\n\nMETTRE ICI UNE FIGURE EN UNE DIMENSION, avec des points et des modèles en escalier qui s’affinent.\n\n\n\n\n\nDans les années 1990, de nombreux travaux ont tâché de proposer des mise en application du boosting (Breiman (1998), Grove and Schuurmans (1998)) et ont comparé les mérites des différentes approches. Deux approches ressortent particulièrement de cette littérature: Adaboost (Adaptive Boosting, Freund and Schapire (1997)) et la Gradient Boosting Machine (Friedman (2001)). Ces deux approches reposent sur des principes très différents.\nLe principe d’Adaboost consiste à pondérer les erreurs commises à chaque itération en donnant plus d’importance aux observations mal prédites, de façon à obliger les modèles simples à se concentrer sur les observations les plus difficiles à prédire. Voici une esquisse du fonctionnement d’AdaBoost:\n\nUn premier modèle simple est entraîné sur un jeu d’entraînement dans lequel toutes les observations ont le même poids.\nA l’issue de cette première itération, les observations mal prédites reçoivent une pondération plus élevé que les observations bien prédites, et un deuxième modèle est entraîné sur ce jeu d’entraînement pondéré.\nCe deuxième modèle est ajouté au premier, puis on repondère à nouveau les observations en fonction de la qualité de prédiction de ce nouveau modèle.\nCette procédure est répétée en ajoutant de nouveaux modèles et en ajustant les pondérations.\n\nL’algorithme Adaboost a été au coeur de la littérature sur le boosting à la fin des années 1990 et dans les années 2000, en raison de ses performances sur les problèmes de classification binaire. Il a toutefois été progressivement remplacé par les algorithmes de gradient boosting inventé quelques années plus tard.\n\n\n\nLa Gradient Boosting Machine (GBM) propose une approche assez différente: elle introduit le gradient boosting en reformulant le boosting sous la forme d’un problème de descente de gradient. Voici une esquisse du fonctionnement de la Gradient Boosting Machine:\n\nUn premier modèle simple est entraîné sur un jeu d’entraînement, de façon à minimiser une fonction de perte qui mesure l’écart entre la variable à prédire et la prédiction du modèle.\nA l’issue de cette première itération, on calcule la dérivée partielle (gradient) de la fonction de perte par rapport à la prédiction en chaque point de l’ensemble d’entraînement. Ce gradient indique dans quelle direction et dans quelle ampleur la prédiction devrait être modifiée afin de réduire la perte.\nA la deuxième itération, on ajoute un deuxième modèle qui va tâcher d’améliorer le modèle complet en prédisant le mieux possible l’opposé de ce gradient.\nCe deuxième modèle est ajouté au premier, puis on recalcule la dérivée partielle de la fonction de perte par rapport à la prédiction de ce nouveau modèle.\nCette procédure est répétée en ajoutant de nouveaux modèles et en recalculant le gradient à chaque étape.\n\n\nL’approche de gradient boosting proposée par Friedman (2001) présente deux grands avantages. D’une part, elle peut être utilisée avec n’importe quelle fonction de perte différentiable, ce qui permet d’appliquer le gradient boosting à de multiples problèmes (régression, classification binaire ou multiclasse, learning-to-rank…). D’autre part, elle offre souvent des performances comparables ou supérieures aux autres approches de boosting. Le gradient boosting d’arbres de décision (Gradient boosted Decision Trees - GBDT) est donc devenue l’approche de référence en matière de boosting. En particulier, les implémentations modernes du gradient boosting comme XGBoost, LightGBM, et CatBoost sont des extensions et améliorations de la Gradient Boosting Machine.",
    "crumbs": [
      "Détail",
      "Le *boosting*"
    ]
  },
  {
    "objectID": "chapters/chapter2/boosting.html#le-boosting",
    "href": "chapters/chapter2/boosting.html#le-boosting",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "",
    "text": "Le fondement théorique du boosting est un article de de 1990 (Shapire (1990)) qui a démontré théoriquement que, sous certaines conditions, il est possible de transformer un modèle prédictif peu performant en un modèle prédictif très performant. Plus précisément, un modèle ayant un pouvoir prédictif arbitrairement élevé (appelé strong learner) peut être construit en combinant des modèles simples dont les prédictions ne sont que légèrement meilleures que le hasard (appelé weak learners). Le boosting est donc une méthode qui combine une approche ensembliste reposant sur un grand nombre de modèles simples avec un entraînement séquentiel: chaque modèle simple (souvent des arbres de décision peu profonds) tâche d’améliorer la prédiction globale en corrigeant les erreurs des prédictions précédentes à chaque étape. Bien qu’une approche de boosting puisse en théorie mobiliser différentes classes de weak learners, en pratique les weak learners utilisés par les algorithmes de boosting sont presque toujours des arbres de décision.\nS’il existe plusieurs variantes, les algorithmes de boosting suivent la même logique :\n\nUn premier modèle simple et peu performant est entraîné sur les données.\nUn deuxième modèle est entraîné de façon à corriger les erreurs du premier modèle (par exemple en pondérant davantage les observations mal prédites);\nCe processus est répété en ajoutant des modèles simples, chaque modèle corrigeant les erreurs commises par l’ensemble des modèles précédents;\nTous ces modèles sont finalement combinés (souvent par une somme pondérée) pour obtenir un modèle complexe et performant.\n\nEn termes plus techniques, les algorithmes de boosting partagent trois caractéristiques communes:\n\nIls visent à trouver une approximation \\(\\hat{F}\\)“)`{=typst} d’une fonction inconnue \\(F*: \\mathbf{x} \\mapsto y\\) à partir d’un ensemble d’entraînement \\((y_i, \\mathbf{x_i})_{i= 1,\\dots,n}\\);\nIls supposent que la fonction \\(F*\\) peut être approchée par une somme pondérée de modèles simples \\(f\\) de paramètres \\(\\theta\\):\n\n\\[ F\\left(\\mathbf{x}\\right) = \\sum_{m=1}^M \\beta_m f\\left(\\mathbf{x}, \\mathbf{\\theta}_m\\right) \\]\n\nils reposent sur une modélisation additive par étapes, qui décompose l’entraînement de ce modèle complexe en une séquence d’entraînements de petits modèles. Chaque étape de l’entraînement cherche le modèle simple \\(f\\) qui améliore la puissance prédictive du modèle complet, sans modifier les modèles précédents, puis l’ajoute de façon incrémentale à ces derniers:\n\n\\[ F_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\hat{\\beta}_m f(\\mathbf{x}_i, \\mathbf{\\hat{\\theta}_m}) \\]\n\nMETTRE ICI UNE FIGURE EN UNE DIMENSION, avec des points et des modèles en escalier qui s’affinent.\n\n\n\n\n\nDans les années 1990, de nombreux travaux ont tâché de proposer des mise en application du boosting (Breiman (1998), Grove and Schuurmans (1998)) et ont comparé les mérites des différentes approches. Deux approches ressortent particulièrement de cette littérature: Adaboost (Adaptive Boosting, Freund and Schapire (1997)) et la Gradient Boosting Machine (Friedman (2001)). Ces deux approches reposent sur des principes très différents.\nLe principe d’Adaboost consiste à pondérer les erreurs commises à chaque itération en donnant plus d’importance aux observations mal prédites, de façon à obliger les modèles simples à se concentrer sur les observations les plus difficiles à prédire. Voici une esquisse du fonctionnement d’AdaBoost:\n\nUn premier modèle simple est entraîné sur un jeu d’entraînement dans lequel toutes les observations ont le même poids.\nA l’issue de cette première itération, les observations mal prédites reçoivent une pondération plus élevé que les observations bien prédites, et un deuxième modèle est entraîné sur ce jeu d’entraînement pondéré.\nCe deuxième modèle est ajouté au premier, puis on repondère à nouveau les observations en fonction de la qualité de prédiction de ce nouveau modèle.\nCette procédure est répétée en ajoutant de nouveaux modèles et en ajustant les pondérations.\n\nL’algorithme Adaboost a été au coeur de la littérature sur le boosting à la fin des années 1990 et dans les années 2000, en raison de ses performances sur les problèmes de classification binaire. Il a toutefois été progressivement remplacé par les algorithmes de gradient boosting inventé quelques années plus tard.\n\n\n\nLa Gradient Boosting Machine (GBM) propose une approche assez différente: elle introduit le gradient boosting en reformulant le boosting sous la forme d’un problème de descente de gradient. Voici une esquisse du fonctionnement de la Gradient Boosting Machine:\n\nUn premier modèle simple est entraîné sur un jeu d’entraînement, de façon à minimiser une fonction de perte qui mesure l’écart entre la variable à prédire et la prédiction du modèle.\nA l’issue de cette première itération, on calcule la dérivée partielle (gradient) de la fonction de perte par rapport à la prédiction en chaque point de l’ensemble d’entraînement. Ce gradient indique dans quelle direction et dans quelle ampleur la prédiction devrait être modifiée afin de réduire la perte.\nA la deuxième itération, on ajoute un deuxième modèle qui va tâcher d’améliorer le modèle complet en prédisant le mieux possible l’opposé de ce gradient.\nCe deuxième modèle est ajouté au premier, puis on recalcule la dérivée partielle de la fonction de perte par rapport à la prédiction de ce nouveau modèle.\nCette procédure est répétée en ajoutant de nouveaux modèles et en recalculant le gradient à chaque étape.\n\n\nL’approche de gradient boosting proposée par Friedman (2001) présente deux grands avantages. D’une part, elle peut être utilisée avec n’importe quelle fonction de perte différentiable, ce qui permet d’appliquer le gradient boosting à de multiples problèmes (régression, classification binaire ou multiclasse, learning-to-rank…). D’autre part, elle offre souvent des performances comparables ou supérieures aux autres approches de boosting. Le gradient boosting d’arbres de décision (Gradient boosted Decision Trees - GBDT) est donc devenue l’approche de référence en matière de boosting. En particulier, les implémentations modernes du gradient boosting comme XGBoost, LightGBM, et CatBoost sont des extensions et améliorations de la Gradient Boosting Machine.",
    "crumbs": [
      "Détail",
      "Le *boosting*"
    ]
  },
  {
    "objectID": "chapters/chapter2/boosting.html#la-mécanique-du-gradient-boosting",
    "href": "chapters/chapter2/boosting.html#la-mécanique-du-gradient-boosting",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "La mécanique du gradient boosting",
    "text": "La mécanique du gradient boosting\nDepuis la publication de Friedman (2001), la méthode de gradient boosting a connu de multiples développements et raffinements, parmi lesquels XGBoost (Chen and Guestrin (2016)), LightGBM (Ke et al. (2017)) et CatBoost (Prokhorenkova et al. (2018)). S’il existe quelques différences entre ces implémentations, elles partagent néanmoins la même mécanique d’ensemble, que la section qui suit va présenter en détail en s’appuyant sur l’implémentation proposée par XBGoost.(^Cette partie reprend la structure et les notations de la partie 2 de Chen and Guestrin 2016.)\nChoses importantes à mettre en avant:\n\nLe boosting est fondamentalement différent des forêts aléatoires. See ESL, chapitre 10.\nToute la mécanique est indépendante de la fonction de perte choisie. En particulier, elle est applicable indifféremment à des problèmes de classification et de régression.\nLes poids sont calculés par une formule explicite, ce qui rend les calculs extrêmement rapides.\nComment on interprète le gradient et la hessienne: cas avec une fonction de perte quadratique.\nLe boosting est fait pour overfitter; contrairement aux RF, il n’y a pas de limite à l’overfitting. Donc lutter contre le surapprentissage est un élément particulièrement important de l’usage des algorithmes de boosting.\n\n\nLe modèle à entraîner\nOn veut entraîner un modèle comprenant \\(K\\) arbres de régression ou de classification:\n\\[\\hat{y}_{i} = \\phi\\left(\\mathbf{x}_i\\right) = \\sum_{k=1}^{K} f_k\\left(\\mathbf{x}_i\\right) \\]\nChaque arbre \\(f\\) est défini par trois paramètres:\n\nsa structure qui est une fonction \\(q: \\mathbb{R}^m \\rightarrow \\{1, \\dots, T\\}\\) qui à un vecteur d’inputs \\(\\mathbf{x}\\) de dimension \\(m\\) associe une feuille terminale de l’arbre);\nson nombre de feuilles terminales \\(T\\);\nles valeurs figurant sur ses feuilles terminales \\(\\mathbf{w}\\in \\mathbb{R}^T\\) (appelées poids ou weights).\n\nLe modèle est entraîné avec une fonction-objectif constituée d’une fonction de perte \\(l\\) et d’une fonction de régularisation \\(\\Omega\\). La fonction de perte mesure la distance entre la prédiction \\(hat(y)\\) et la vraie valeur \\(y\\) et présente généralement les propriétés suivantes: elle est convexe et dérivable deux fois, et atteint son minimum lorsque \\(hat(y) = y\\). La fonction de régularisation pénalise la complexité du modèle. Dans le cas présent, elle pénalise les arbres avec un grand nombre de feuilles (\\(T\\) élevé) et les arbres avec des poids élevés (\\(w_t\\) élevés en valeur absolue).\n\\[ \\mathcal{L}(\\phi) = \\underbrace{\\sum_i l(\\hat{y}_{i}, y_{i})}_{\\substack{\\text{Perte sur les} \\\\ \\text{observations}}} + \\underbrace{\\sum_k \\Omega(f_{k})}_{\\substack{\\text{Fonction de} \\\\ \\text{régularisation}}}\\text{avec}\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{t=1}^T w_j^2  \\tag{1}\\]\n\n\nIsoler le \\(k\\)-ième arbre\nLa fonction-objectif introduite précédemment est très complexe et ne peut être utilisée directement pour entraîner le modèle, car il faudrait entraîner tous les arbres en même temps. On va donc reformuler donc cette fonction objectif de façon à isoler le \\(k\\)-ième arbre, qui pourra ensuite être entraîné seul, une fois que les \\(k-1\\) arbres précédents auront été entraînés. Pour cela, on note \\(\\hat{y}_i^{(k)}\\) la prédiction à l’issue de l’étape \\(k\\): \\(\\hat{y}_i^{(k)} = \\sum_{j=1}^t f_j(\\mathbf{x}_i)\\), et on $1 la fonction-objectif \\(\\mathcal{L}^{(k)}\\) au moment de l’entraînement du \\(k\\)-ième arbre:\n\\[\n\\begin{aligned}\n\\mathcal{L}^{(t)}\n&= \\sum_{i=1}^{n} l(y_i, \\hat{y}_{i}^{(t)}) + \\sum_{k=1}^t\\Omega(f_k) \\\\\n&= \\sum_{i=1}^{n} l\\left(y_i, \\hat{y}_{i}^{(t-1)} + f_{t}(\\mathbf{x}_i)\\right) + \\Omega(f_t) + constant\n\\end{aligned}\n\\]\n\n\nFaire apparaître le gradient\nUne fois isolé le \\(k\\)-ième arbre, on fait un développement limité d’ordre 2 de \\(l(y_i, \\hat{y}_{i}^{(k-1)} + f_{k}(\\mathbf{x}_i))\\) au voisinage de \\(\\hat{y}_{i}^{(k-1)}\\), en considérant que la prédiction du \\(k\\)-ième arbre \\(f_{k}(\\mathbf{x}_i)\\) est\n\\[ \\mathcal{L}^{(t)} \\approx \\sum_{i=1}^{n} [l(y_i, \\hat{y}_{i}^{(t-1)}) + g_i f_t(\\mathbf{x}_i)+ \\frac{1}{2} h_i f^2_t(\\mathbf{x}_i)] + \\Omega(f_t) \\]\navec\n\\[ g_i = \\frac{\\partial l(y_i, \\hat{y}_i^{(t-1)})}{\\partial\\hat{y}_i^{(t-1)}} \\text{et} h_i = \\frac{\\partial^2 l(y_i, \\hat{y}_i^{(t-1)})}{{\\partial \\hat{y}_i^{(t-1)}}^2} \\]\nLes termes \\(g_i\\) et \\(h_i\\) désignent respectivement la dérivée première (le gradient) et la dérivée seconde (la hessienne) de la fonction de perte par rapport à la variable prédite. Il est important de noter que les termes (A) et (B) sont constants car les \\(k-1\\) arbres précédents ont déjà été entraînés et ne sont pas modifiés par l’entraînement du \\(k\\)-ième arbre.  On peut donc retirer ces termes pour obtenir la fonction-objectif simplifiée qui sera utilisée pour l’entraînement du \\(k\\)-ième arbre.\n\\[ \\mathcal{\\tilde{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i)+ \\frac{1}{2} h_i f^2_t(\\mathbf{x}_i)] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_i^2 \\]\nCette expression est importante car elle montre qu’on est passé d’un problème complexe où il fallait entraîner un grand nombre d’arbres simultanément (équation Equation 1) à un problème beaucoup plus simple dans lequel il n’y a qu’un seul arbre à entraîner.\n\n\nCalculer les poids optimaux\nA partir de l’expression précédente, il est possible de faire apparaître les poids \\(w_j\\) du \\(k\\)-ième arbre. Pour une structure d’arbre donnée (\\(q: \\mathbb{R}^m \\rightarrow \\{1, \\dots, T\\}\\)), on définit \\(I_j = \\{ i | q(\\mathbf{x}_i) = j \\}\\) l’ensemble des observations situées sur la feuille \\(j\\) puis on réorganise \\(\\mathcal{\\tilde{L}}^{(k)}\\):\n\\[\n\\begin{align*}\n\\mathcal{\\tilde{L}}^{(t)} =&   \\sum_{j=1}^{T} \\sum_{i\\in I_{j}} \\bigg[g_i f_t(\\mathbf{x}_i)\\phantom{\\frac{1}{2}} &+ \\frac{1}{2} h_i f^2_t(\\mathbf{x}_i)\\bigg]&+ \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_i^2 \\\\\n     &= \\sum_{j=1}^{T} \\sum_{i\\in I_{j}} \\bigg[g_i w_j &+ \\frac{1}{2} h_i w_j^2\\bigg] &+ \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_i^2 \\\\\n     &= \\sum^T_{j=1} \\bigg[w_j\\sum_{i\\in I_{j}} g_i &+ \\frac{1}{2} w_j^2 \\sum_{i \\in I_{j}} h_i + \\lambda \\bigg] &+ \\gamma T\n\\end{align*}\n\\]\nDans la dernière expression, on voit que la fonction de perte simplifiée se reformule comme une combinaison quadratique des poids \\(w_j\\), dans laquelle les dérivées première et seconde de la fonction de perte interviennent sous forme de pondérations. Tout l’enjeu de l’entraînement devient donc de trouver les poids optimaux \\(w_j\\) qui minimiseront cette fonction de perte, compte tenu de ces opérations.\nIl se trouve que le calculs de ces poids optimaux est très simple: pour une structure d’arbre donnée (\\(q: \\mathbb{R}^m \\rightarrow \\{1, \\dots, T\\}\\)), le poids optimal w_j^{} de la feuille \\(j\\) est donné par l’équation:\nPar conséquent, la valeur optimale de la fonction objectif pour l’arbre \\(q\\) est égale à\n\\[\\mathcal{\\tilde{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^T \\frac{\\left(\\sum_{i\\in I_j} g_i\\right)^2}{\\sum_{i\\in I_j} h_i+\\lambda} + \\gamma T \\tag{2}\\]\nCette équation est utile car elle permet de comparer simplement la qualité de deux arbres, et de déterminer lequel est le meilleur.\n\n\nConstruire le \\(k\\)-ième arbre\nDans la mesure où elle permet de comparer des arbres, on pourrait penser que l’équation Equation 2 est suffisante pour choisir directement le \\(k\\)-ième arbre: il suffirait d’énumérer les arbres possibles, de calculer la qualité de chacun d’entre eux, et de retenir le meilleur. Bien que cette approche soit possible théoriquement, elle est inemployable en pratique car le nombre d’arbres possibles est extrêmement élevé. Par conséquent, le \\(k\\)-ième arbre n’est pas défini en une fois, mais construit de façon gloutonne:\nREFERENCE A LA PARTIE CART/RF?\n\non commence par le noeud racine et on cherche le split qui réduit au maximum la perte en séparant les données d’entraînement entre les deux noeuds-enfants.\npour chaque noeud enfant, on cherche le split qui réduit au maximum la perte en séparant en deux la population de chacun de ces noeuds.\nCette procédure recommence jusqu’à que l’arbre ait atteint sa taille maximale (définie par une combinaison d’hyperparamètres d$1s dans la partie référence à ajouter).\n\n\n\nChoisir les splits\nTraduire split par critère de partition?\nReste à comprendre comment le critère de partition optimal est choisi à chaque étape de la construction de l’arbre. Imaginons qu’on envisage de décomposer la feuille \\(I\\) en deux nouvelles feuilles \\(I_L\\) et \\(I_R\\) (avec \\(I = I_L \\cup I_R\\)), selon une condition logique reposant sur une variable et une valeur de cette variable (exemple: \\(x_6 &gt; 11\\)). Par application de l’équation Equation 2, le gain potentiel induit par ce critère de partition est égal à:\n\\[ Gain = \\frac{1}{2} \\left[\\frac{G_L^2}{H_L+\\lambda}+\\frac{G_R^2}{H_R+\\lambda}-\\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda}\\right] - \\gamma  \\tag{3}\\]\n\nCette dernière équation est au coeur de la mécanique du gradient boosting car elle permet de comparer les critères de partition possibles. Plus précisément, l’algorithme de détermination des critère de partition (split finding algorithm) consiste en une double boucle sur les variables et les valeurs prises par ces variables, qui énumère un grand nombre de critères de partition et mesure le gain associé à chacun d’entre eux avec l’équation Equation 3. Le critère de partition retenu est simplement celui dont le gain est le plus élevé.\nL’algorithme qui détermine les critère de partition est un enjeu de performance essentiel dans le gradient boosting. En effet, utiliser l’algorithme le plus simple (énumérer tous les critères de partition possibles, en balayant toutes les valeurs de toutes les variables) s’avère très coûteux dès lors que les données contiennent soit un grand nombre de variables, soit des variables continues prenant un grand nombre de valeurs. C’est pourquoi les algorithmes de détermination des critère de partition ont fait l’objet de multiples améliorations et optimisations visant à réduire leur coût computationnel sans dégrader la qualité des critères de partition.\n\n\nLa suite\n\nLes moyens de lutter contre l’overfitting:\n\nle shrinkage;\nle subsampling des lignes et des colonnes;\nles différentes pénalisations.\n\n\n\nLes hyperparamètres\n\nLes principaux hyperparamètres d’XGBoost\n\n\n\n\n\n\n\nHyperparamètre\nDescription\nValeur par défaut\n\n\n\n\nbooster\nLe type de weak learner utilisé\n'gbtree'\n\n\nlearning_rate\nLe taux d’apprentissage\n0.3\n\n\nmax_depth\nLa profondeur maximale des arbres\n6\n\n\nmax_leaves\nLe nombre maximal de feuilles des arbres\n0\n\n\nmin_child_weight\nLe poids minimal qu’une feuille doit contenir\n1\n\n\nn_estimators\nLe nombre d’arbres\n100\n\n\nlambda ou reg_lambda\nLa pénalisation L2\n1\n\n\nalpha ou reg_alpha\nLa pénalisation L1\n0\n\n\ngamma\nLe gain minimal nécessaire pour ajouter un noeud supplémentaire\n0\n\n\ntree_method\nLa méthode utilisée pour rechercher les splits\n'hist'\n\n\nmax_bin\nLe nombre utilisés pour discrétiser les variables continues\n0\n\n\nsubsample\nLe taux d’échantillonnage des données d’entraîenment\n1\n\n\nsampling_method\nLa méthode utilisée pour échantillonner les données d’entraînement\n'uniform'\n\n\ncolsample_bytree  colsample_bylevel  colsample_bynode\nTaux d’échantillonnage des colonnes par arbre, par niveau et par noeud\n1, 1 et 1\n\n\nscale_pos_weight\nLe poids des observations de la classe positive (classification uniquement)\n1\n\n\nsample_weight\nLa pondération des données d’entraînement\n1\n\n\nenable_categorical\nActiver le support des variables catégorielles\nFalse\n\n\nmax_cat_to_onehot\nNombre de modalités en-deça duquel XGBoost utilise le one-hot-encoding\nA COMPLETER\n\n\nmax_cat_threshold\nNombre maximal de catégories considérées dans le partitionnement optimal des variables catégorielles\nA COMPLETER\n\n\n\n\n\nLa préparation des données\n\nles variables catégorielles:\n\nordonnées: passer en integer;\nnon-ordonnées: OHE ou approche de Fisher.\n\nles variables continues:\n\ninutile de faire des transformations monotones.\nUtile d’ajouter des transformations non monotones.\n\n\n\n\nLes fonctions de perte\n\n\n\nListe des hyperparamètres d’une RF\nSource: Probst, Wright, and Boulesteix (2019)\n\nstructure of each individual tree:\n\ndudu\ndudu\ndudu\n\nstructure and size of the forest:\nThe level of randomness (je dirais plutôt : )",
    "crumbs": [
      "Détail",
      "Le *boosting*"
    ]
  },
  {
    "objectID": "chapters/chapter2/random_forest.html",
    "href": "chapters/chapter2/random_forest.html",
    "title": "La forêt aléatoire",
    "section": "",
    "text": "La forêt aléatoire (random forests) est une méthode ensembliste puissante, largement utilisée pour les tâches de classification et de régression. Elle combine la simplicité des arbres de décision avec la puissance de l’agrégation pour améliorer les performances prédictives et réduire le risque de surapprentissage (overfitting).\nLa méthode s’appuie sur la technique du bagging, qui consiste à entraîner chaque arbre sur un échantillon (bootstrap) tiré au hasard à partir du jeu de données initial. Elle introduit un degré supplémentaire de randomisation au moment de la construction d’un arbre, puisqu’à chaque nouvelle division (noeud), un sous-ensemble de variables sur lequel sera fondé le critère de séparation est sélectionné aléatoirement. Cette randomisation supplémentaire réduit la corrélation entre les arbres, ce qui permet de diminuer la variance des prédiction du modèle agrégé.\nObjectifs:\n\ncomprendre les propriétés fondamentales des forêts aléatoires afin de comprendre comment elles améliorent les performances des modèles\ncomprendre les étapes permettant de construire une forêt aléatoire : échantillonnage bootstrap, sélection de variables, partitions, prédiction, évaluation, interprétation\npermettre l’optimisation des performances du modèle: fournir une présentation formelle pour guider le choix des hyperparamètres, la préparation des données, etc.\n\n\n\nLes forêts aléatoires reposent sur plusieurs éléments essentiels :\n\nLes arbres CART: Les modèles élémentaires sont des arbres CART non élagués, c’est-à-dire autorisés à pousser jusqu’à l’atteinte d’un critère d’arrêt défini en amont.\nL’échantillonnage bootstrap: Chaque arbre est construit à partir d’un échantillon aléatoire tiré avec remise du jeu de données d’entraînement.\nLa sélection aléatoire de caractéristiques (variables) : Lors de la construction d’un arbre, à chaque nœud de celui-ci, un sous-ensemble aléatoire de variables est sélectionné. La meilleure division est ensuite choisie parmi ces caractéristiques aléatoires.\nL’agrégation des prédictions : Comme pour le bagging, les prédictions de tous les arbres sont combinées On procède généralement à la moyenne (ou à la médiane) des prédictions dans le cas de la régression, et au vote majoritaire (ou à la moyenne des probabilités prédites pour chaque classe) dans le cas de la classification.\n\n\n\n\nLes propriétés théoriques des forêts aléatoires permettent de comprendre pourquoi (et dans quelles situations) elles sont particulièrement robustes et performantes.\n\n\nL’agrégation de plusieurs arbres dans une forêt aléatoire permet de réduire la variance globale du modèle, ce qui améliore la stabilité et la précision des prédictions lorsque les biais initiaux sont “faibles”. La démonstration est présentée dans la section bagging.\n\n\n\nLes forêts aléatoires sont très performantes en pratique, mais la question de leur convergence vers une solution optimale lorsque la taille de l’échantillon tend vers l’infini reste ouverte (Loupes 2014). Plusieurs travaux théoriques ont toutefois fourni des preuves de consistance pour des versions simplifiées de l’algorithme (par exemple, Biau 2012).\nMême si la convergence vers le modèle optimal n’est pas encore formellement établie, il est possible de montrer, grâce à la Loi des Grands Nombres, que l’erreur de généralisation de l’ensemble diminue lorsque le nombre d’arbres augmente. Cela implique que la forêt aléatoire ne souffre pas d’un surapprentissage (également appelé overfitting) croissant avec le nombre d’arbres inclus dans le modèle, ce qui la rend particulièrement robuste.\n\n\n\nL’erreur de généralisation des forêts aléatoires est influencée par deux facteurs principaux :\n\nLa force (puissance prédictrice) des arbres individuels : La force d’un arbre dépend de sa capacité à faire des prédictions correctes (sans biais). Pour que l’ensemble soit performant, chaque arbre doit être suffisamment prédictif.\nLa corrélation entre les arbres : Une corrélation faible entre les arbres améliore les performances globales. En effet, des arbres fortement corrélés auront tendance à faire des erreurs similaires. La randomisation des caractéristiques à chaque nœud contribue à réduire cette corrélation, ce qui améliore la précision globale.\n\nDans le cas de la régression, où l’objectif est de minimiser l’erreur quadratique moyenne, la décomposition de la variance de l’ensemble (la forêt aléatoire) permet de faire apparaître le rôle de la corrélation entre les arbres:\n\\[\n\\text{Var}(\\hat{f}(x)) = \\rho(x) \\sigma(x)^2 + \\frac{1 - \\rho(x)}{M} \\sigma(x)^2\n\\]\nOù \\(\\rho(x)\\) est le coefficient de corrélation entre les arbres individuels, \\(\\sigma(x)^2\\) est la variance d’un arbre individuel, \\(M\\) est le nombre d’arbres dans la forêt.\nConséquences:\n\nSi \\(\\rho(x)\\) est faible : La variance est significativement réduite avec l’augmentation du nombre d’arbres \\(M\\).\nSi \\(\\rho(x)\\) est élevée : La réduction de variance est moindre, car les arbres sont plus corrélés entre eux.\n\nL’objectif des forêts aléatoires est donc de minimiser la corrélation entre les arbres tout en maximisant leur capacité à prédire correctement, ce qui permet de réduire la variance globale sans augmenter excessivement le biais.\n\n\n\n\n\nNombre d’arbres : plus le nombre d’arbes est élevé, plus la variance est réduite, jusqu’à un certain point de saturation. Souvent, quelques centaines d’arbres suffisent à stabiliser les performances des modèles.\nNombre de variables à sélectionner à chaque noeud: cet hyperparamètre permet de contrôler l’interdépendance entre les arbres. Avec \\(K\\) le nombre total de variables, une règle empirique usuelle est de considérer \\(\\sqrt{K}\\) pour une classification et \\(K/3\\) pour une régression.\nProfondeur des arbres : laisser les arbres se développer pleinement (sans élagage) pour profiter de la réduction de variance par agrégation.\n\n\n\n\n\n\nL’estimation Out-of-Bag (OOB) est une méthode particulièrement efficace pour évaluer les performances des forêts aléatoires sans nécessité une validation croisée ou de réserver une partie des données pour l’étape du test. Cette technique repose sur le fait que chaque arbre dans une forêt aléatoire est construit à partir d’un échantillon bootstrap du jeu de données d’origine, c’est-à-dire un échantillon tiré avec remise. Or, en moyenne, environ 36 % des observations ne sont pas inclus dans chaque échantillon bootstrap, ce qui signifie qu’elles ne sont pas utilisées pour entraîner l’arbre correspondant. Ces observations laissées de côté forment un échantillon out-of-bag. Chaque arbre peut donc être évalué sur son échantillon out-of-bag plutôt que sur un échantillon test.\nProcédure d’Estimation OOB:\n\nConstruction des arbres : Chaque arbre de la forêt est construit à partir d’un échantillon bootstrap tiré avec remise à partir du jeu de données d’origine. Cela signifie que certaines observations seront sélectionnées plusieurs fois, tandis que d’autres ne seront pas sélectionnées du tout.\nPrédiction OOB : Pour chaque observation \\((x_i, y_i)\\) qui n’a pas été inclus dans l’échantillon bootstrap qui a permi de construire un arbre donné, l’arbre est utilisé pour prédire la valeur de \\(y_i\\). Ainsi, chaque observation est prédite par tous les arbres pour lesquels elle fait partie de l’échantillon out-of-bag.\nAgrégation des prédictions : La prédiction finale pour chaque échantillon out-of-bag est obtenue en moyennant les prédictions de tous les arbres pour lesquels cet échantillon était OOB (pour la régression) ou par un vote majoritaire (pour la classification).\nCalcul de l’erreur OOB : L’erreur OOB est ensuite calculée en comparant les prédictions agrégées avec les valeurs réelles des observations \\(y_i\\). Cette erreur est une bonne approximation de l’erreur de généralisation du modèle.\n\nAvantages de l’Estimation OOB:\n\nPas besoin de jeu de validation séparé : L’un des principaux avantages de l’estimation OOB est qu’elle ne nécessite pas de réserver une partie des données pour la validation. Cela est particulièrement utile lorsque la taille du jeu de données est limitée, car toutes les données peuvent être utilisées pour l’entraînement tout en ayant une estimation fiable de la performance.\nEstimation directe et éfficace : Contrairement à la validation croisée qui peut être coûteuse en temps de calcul, l’estimation OOB est disponible “gratuitement” pendant la construction des arbres. Cela permet d’évaluer la performance du modèle sans avoir besoin de réentraîner plusieurs fois le modèle.\nApproximation de l’erreur de généralisation : L’erreur OOB est considérée comme une bonne approximation de l’erreur de généralisation, comparable à celle obtenue par une validation croisée 10-fold.\n\n\n\n\nLa validation croisée est une technique d’évaluation couramment utilisée en apprentissage automatique pour estimer la capacité d’un modèle à généraliser à de nouvelles données. Bien que l’estimation Out-of-Bag (OOB) soit généralement suffisante pour les forêts aléatoires, la validation croisée permet d’obtenir une évaluation plus robuste, notamment sur des jeux de données de petite taille.\nL’idée derrière la validation croisée est de maximiser l’utilisation des données disponibles en réutilisant chaque observation à la fois pour l’entraînement et pour le test. Cela permet d’obtenir une estimation de la performance du modèle qui est moins sensible aux fluctuations dues à la division des données en ensembles d’entraînement et de test. La validation croisée répète cette division plusieurs fois, puis moyenne les résultats pour obtenir une estimation plus fiable.\nProcédure de validation croisée:\nLa validation croisée la plus courante est la validation croisée en k sous-échantillons (k-fold cross-validation):\n\nDivision des données : Le jeu de données est divisé en k sous-échantillons égaux, appelés folds. Typiquement, k est choisi entre 5 et 10, mais il peut être ajusté en fonction de la taille des données.\nEntraînement et test : Le modèle est entraîné sur k - 1 sous-échantillons et testé sur le sous-échantillon restant. Cette opération est répétée k fois, chaque sous-échantillon jouant à tour de rôle le rôle de jeu de test.\nCalcul de la performance : Les k performances obtenues (par exemple, l’erreur quadratique moyenne pour une régression, ou l’accuracy (exactitude) pour une classification) sont moyennées pour obtenir une estimation finale de la performance du modèle.\n\nAvantages de la validation croisée:\n\nUtilisation optimale des données : En particulier lorsque les données sont limitées, la validation croisée maximise l’utilisation de l’ensemble des données en permettant à chaque échantillon de contribuer à la fois à l’entraînement et au test.\nRéduction de la variance : En utilisant plusieurs divisions des données, on obtient une estimation de la performance moins sensible aux particularités d’une seule division.\n\nBien que plus coûteuse en termes de calcul, la validation croisée est souvent préférée lorsque les données sont limitées ou lorsque l’on souhaite évaluer différents modèles ou hyperparamètres avec précision.\nLeave-One-Out Cross-Validation (LOOCV) : Il s’agit d’un cas particulier où le nombre de sous-échantillons est égal à la taille du jeu de données. En d’autres termes, chaque échantillon est utilisé une fois comme jeu de test, et tous les autres échantillons pour l’entraînement. LOOCV fournit une estimation très précise de la performance, mais est très coûteuse en temps de calcul, surtout pour de grands jeux de données.\n\n\n\nL’estimation Out-of-Bag (OOB) et la validation croisée sont deux méthodes clés pour optimiser les hyper-paramètres d’une forêt aléatoire. Les deux approches permettent de comparer différentes combinaisons d’hyper-paramètres et de sélectionner celles qui maximisent les performances prédictives, l’OOB étant souvent plus rapide et moins coûteuse, tandis que la validation croisée est plus fiable dans des situations où le surapprentissage est un risque important.\n\n\n\n\nObjectif: Identifier les variables qui contribuent le plus à la prédiction/influencent le plus la variable cible, extraire des caractéristiques pertinentes pour comprendre les mécanismes de prédiction sous-jacent, établir des règles de décision simplifiées etc.\nMéthodes ususelles (mais biaisées):\n\nRéduction moyenne de l’impureté (Mean Decrease in Impurity) : pour chaque variable, on calcule la moyenne des réductions d’impureté qu’elle a engendrées dans tous les nœuds de tous les arbres où elle est impliquée. Les variables présentant la réduction moyenne d’impureté la plus élevée sont considérées comme les prédicteurs les plus importants.\nPermutation importance (Mean Decrease Accuracy) : Pour chaque variable, les performances du modèle sont comparées avant et après la permutation de ses valeurs. La différence moyenne de performance correspond à la MDA. L’idée est que si l’on permute aléatoirement les valeurs d’une variable (cassant ainsi sa relation avec la cible), une variable importante entraînera une hausse significative de l’erreur de généralisation.\n\nIl est essentiel de noter que ces deux mesures peuvent présenter des biais importants. Elles sont notamment sensibles aux variables catégorielles avec de nombreuses modalités, qui peuvent apparaître artificiellement importantes. Elles sont également fortement biaisée en présence de variables explicatives corrélées ou d’interactions complexes avec la cible.\n\n\n\n\nVariables catégorielles : Encoder correctement (one-hot encoding, ordinal encoding).\nValeurs manquantes : Les forêts aléatoires peuvent gérer les données manquantes, mais une imputation préalable peut améliorer les performances.\nÉchelle des Variables : Pas nécessaire de normaliser, les arbres sont invariants aux transformations monotones.",
    "crumbs": [
      "Détail",
      "La forêt aléatoire"
    ]
  },
  {
    "objectID": "chapters/chapter2/random_forest.html#principe-de-la-forêt-aléatoire",
    "href": "chapters/chapter2/random_forest.html#principe-de-la-forêt-aléatoire",
    "title": "La forêt aléatoire",
    "section": "",
    "text": "Les forêts aléatoires reposent sur plusieurs éléments essentiels :\n\nLes arbres CART: Les modèles élémentaires sont des arbres CART non élagués, c’est-à-dire autorisés à pousser jusqu’à l’atteinte d’un critère d’arrêt défini en amont.\nL’échantillonnage bootstrap: Chaque arbre est construit à partir d’un échantillon aléatoire tiré avec remise du jeu de données d’entraînement.\nLa sélection aléatoire de caractéristiques (variables) : Lors de la construction d’un arbre, à chaque nœud de celui-ci, un sous-ensemble aléatoire de variables est sélectionné. La meilleure division est ensuite choisie parmi ces caractéristiques aléatoires.\nL’agrégation des prédictions : Comme pour le bagging, les prédictions de tous les arbres sont combinées On procède généralement à la moyenne (ou à la médiane) des prédictions dans le cas de la régression, et au vote majoritaire (ou à la moyenne des probabilités prédites pour chaque classe) dans le cas de la classification.",
    "crumbs": [
      "Détail",
      "La forêt aléatoire"
    ]
  },
  {
    "objectID": "chapters/chapter2/random_forest.html#pourquoi-et-dans-quelles-situations-la-random-forest-fonctionne-fondements-théoriques-des-forêts-aléatoires",
    "href": "chapters/chapter2/random_forest.html#pourquoi-et-dans-quelles-situations-la-random-forest-fonctionne-fondements-théoriques-des-forêts-aléatoires",
    "title": "La forêt aléatoire",
    "section": "",
    "text": "Les propriétés théoriques des forêts aléatoires permettent de comprendre pourquoi (et dans quelles situations) elles sont particulièrement robustes et performantes.\n\n\nL’agrégation de plusieurs arbres dans une forêt aléatoire permet de réduire la variance globale du modèle, ce qui améliore la stabilité et la précision des prédictions lorsque les biais initiaux sont “faibles”. La démonstration est présentée dans la section bagging.\n\n\n\nLes forêts aléatoires sont très performantes en pratique, mais la question de leur convergence vers une solution optimale lorsque la taille de l’échantillon tend vers l’infini reste ouverte (Loupes 2014). Plusieurs travaux théoriques ont toutefois fourni des preuves de consistance pour des versions simplifiées de l’algorithme (par exemple, Biau 2012).\nMême si la convergence vers le modèle optimal n’est pas encore formellement établie, il est possible de montrer, grâce à la Loi des Grands Nombres, que l’erreur de généralisation de l’ensemble diminue lorsque le nombre d’arbres augmente. Cela implique que la forêt aléatoire ne souffre pas d’un surapprentissage (également appelé overfitting) croissant avec le nombre d’arbres inclus dans le modèle, ce qui la rend particulièrement robuste.\n\n\n\nL’erreur de généralisation des forêts aléatoires est influencée par deux facteurs principaux :\n\nLa force (puissance prédictrice) des arbres individuels : La force d’un arbre dépend de sa capacité à faire des prédictions correctes (sans biais). Pour que l’ensemble soit performant, chaque arbre doit être suffisamment prédictif.\nLa corrélation entre les arbres : Une corrélation faible entre les arbres améliore les performances globales. En effet, des arbres fortement corrélés auront tendance à faire des erreurs similaires. La randomisation des caractéristiques à chaque nœud contribue à réduire cette corrélation, ce qui améliore la précision globale.\n\nDans le cas de la régression, où l’objectif est de minimiser l’erreur quadratique moyenne, la décomposition de la variance de l’ensemble (la forêt aléatoire) permet de faire apparaître le rôle de la corrélation entre les arbres:\n\\[\n\\text{Var}(\\hat{f}(x)) = \\rho(x) \\sigma(x)^2 + \\frac{1 - \\rho(x)}{M} \\sigma(x)^2\n\\]\nOù \\(\\rho(x)\\) est le coefficient de corrélation entre les arbres individuels, \\(\\sigma(x)^2\\) est la variance d’un arbre individuel, \\(M\\) est le nombre d’arbres dans la forêt.\nConséquences:\n\nSi \\(\\rho(x)\\) est faible : La variance est significativement réduite avec l’augmentation du nombre d’arbres \\(M\\).\nSi \\(\\rho(x)\\) est élevée : La réduction de variance est moindre, car les arbres sont plus corrélés entre eux.\n\nL’objectif des forêts aléatoires est donc de minimiser la corrélation entre les arbres tout en maximisant leur capacité à prédire correctement, ce qui permet de réduire la variance globale sans augmenter excessivement le biais.",
    "crumbs": [
      "Détail",
      "La forêt aléatoire"
    ]
  },
  {
    "objectID": "chapters/chapter2/random_forest.html#les-hyper-paramètres-clés",
    "href": "chapters/chapter2/random_forest.html#les-hyper-paramètres-clés",
    "title": "La forêt aléatoire",
    "section": "",
    "text": "Nombre d’arbres : plus le nombre d’arbes est élevé, plus la variance est réduite, jusqu’à un certain point de saturation. Souvent, quelques centaines d’arbres suffisent à stabiliser les performances des modèles.\nNombre de variables à sélectionner à chaque noeud: cet hyperparamètre permet de contrôler l’interdépendance entre les arbres. Avec \\(K\\) le nombre total de variables, une règle empirique usuelle est de considérer \\(\\sqrt{K}\\) pour une classification et \\(K/3\\) pour une régression.\nProfondeur des arbres : laisser les arbres se développer pleinement (sans élagage) pour profiter de la réduction de variance par agrégation.",
    "crumbs": [
      "Détail",
      "La forêt aléatoire"
    ]
  },
  {
    "objectID": "chapters/chapter2/random_forest.html#evaluation-des-performances-du-modèle-et-choix-des-hyper-paramètres",
    "href": "chapters/chapter2/random_forest.html#evaluation-des-performances-du-modèle-et-choix-des-hyper-paramètres",
    "title": "La forêt aléatoire",
    "section": "",
    "text": "L’estimation Out-of-Bag (OOB) est une méthode particulièrement efficace pour évaluer les performances des forêts aléatoires sans nécessité une validation croisée ou de réserver une partie des données pour l’étape du test. Cette technique repose sur le fait que chaque arbre dans une forêt aléatoire est construit à partir d’un échantillon bootstrap du jeu de données d’origine, c’est-à-dire un échantillon tiré avec remise. Or, en moyenne, environ 36 % des observations ne sont pas inclus dans chaque échantillon bootstrap, ce qui signifie qu’elles ne sont pas utilisées pour entraîner l’arbre correspondant. Ces observations laissées de côté forment un échantillon out-of-bag. Chaque arbre peut donc être évalué sur son échantillon out-of-bag plutôt que sur un échantillon test.\nProcédure d’Estimation OOB:\n\nConstruction des arbres : Chaque arbre de la forêt est construit à partir d’un échantillon bootstrap tiré avec remise à partir du jeu de données d’origine. Cela signifie que certaines observations seront sélectionnées plusieurs fois, tandis que d’autres ne seront pas sélectionnées du tout.\nPrédiction OOB : Pour chaque observation \\((x_i, y_i)\\) qui n’a pas été inclus dans l’échantillon bootstrap qui a permi de construire un arbre donné, l’arbre est utilisé pour prédire la valeur de \\(y_i\\). Ainsi, chaque observation est prédite par tous les arbres pour lesquels elle fait partie de l’échantillon out-of-bag.\nAgrégation des prédictions : La prédiction finale pour chaque échantillon out-of-bag est obtenue en moyennant les prédictions de tous les arbres pour lesquels cet échantillon était OOB (pour la régression) ou par un vote majoritaire (pour la classification).\nCalcul de l’erreur OOB : L’erreur OOB est ensuite calculée en comparant les prédictions agrégées avec les valeurs réelles des observations \\(y_i\\). Cette erreur est une bonne approximation de l’erreur de généralisation du modèle.\n\nAvantages de l’Estimation OOB:\n\nPas besoin de jeu de validation séparé : L’un des principaux avantages de l’estimation OOB est qu’elle ne nécessite pas de réserver une partie des données pour la validation. Cela est particulièrement utile lorsque la taille du jeu de données est limitée, car toutes les données peuvent être utilisées pour l’entraînement tout en ayant une estimation fiable de la performance.\nEstimation directe et éfficace : Contrairement à la validation croisée qui peut être coûteuse en temps de calcul, l’estimation OOB est disponible “gratuitement” pendant la construction des arbres. Cela permet d’évaluer la performance du modèle sans avoir besoin de réentraîner plusieurs fois le modèle.\nApproximation de l’erreur de généralisation : L’erreur OOB est considérée comme une bonne approximation de l’erreur de généralisation, comparable à celle obtenue par une validation croisée 10-fold.\n\n\n\n\nLa validation croisée est une technique d’évaluation couramment utilisée en apprentissage automatique pour estimer la capacité d’un modèle à généraliser à de nouvelles données. Bien que l’estimation Out-of-Bag (OOB) soit généralement suffisante pour les forêts aléatoires, la validation croisée permet d’obtenir une évaluation plus robuste, notamment sur des jeux de données de petite taille.\nL’idée derrière la validation croisée est de maximiser l’utilisation des données disponibles en réutilisant chaque observation à la fois pour l’entraînement et pour le test. Cela permet d’obtenir une estimation de la performance du modèle qui est moins sensible aux fluctuations dues à la division des données en ensembles d’entraînement et de test. La validation croisée répète cette division plusieurs fois, puis moyenne les résultats pour obtenir une estimation plus fiable.\nProcédure de validation croisée:\nLa validation croisée la plus courante est la validation croisée en k sous-échantillons (k-fold cross-validation):\n\nDivision des données : Le jeu de données est divisé en k sous-échantillons égaux, appelés folds. Typiquement, k est choisi entre 5 et 10, mais il peut être ajusté en fonction de la taille des données.\nEntraînement et test : Le modèle est entraîné sur k - 1 sous-échantillons et testé sur le sous-échantillon restant. Cette opération est répétée k fois, chaque sous-échantillon jouant à tour de rôle le rôle de jeu de test.\nCalcul de la performance : Les k performances obtenues (par exemple, l’erreur quadratique moyenne pour une régression, ou l’accuracy (exactitude) pour une classification) sont moyennées pour obtenir une estimation finale de la performance du modèle.\n\nAvantages de la validation croisée:\n\nUtilisation optimale des données : En particulier lorsque les données sont limitées, la validation croisée maximise l’utilisation de l’ensemble des données en permettant à chaque échantillon de contribuer à la fois à l’entraînement et au test.\nRéduction de la variance : En utilisant plusieurs divisions des données, on obtient une estimation de la performance moins sensible aux particularités d’une seule division.\n\nBien que plus coûteuse en termes de calcul, la validation croisée est souvent préférée lorsque les données sont limitées ou lorsque l’on souhaite évaluer différents modèles ou hyperparamètres avec précision.\nLeave-One-Out Cross-Validation (LOOCV) : Il s’agit d’un cas particulier où le nombre de sous-échantillons est égal à la taille du jeu de données. En d’autres termes, chaque échantillon est utilisé une fois comme jeu de test, et tous les autres échantillons pour l’entraînement. LOOCV fournit une estimation très précise de la performance, mais est très coûteuse en temps de calcul, surtout pour de grands jeux de données.\n\n\n\nL’estimation Out-of-Bag (OOB) et la validation croisée sont deux méthodes clés pour optimiser les hyper-paramètres d’une forêt aléatoire. Les deux approches permettent de comparer différentes combinaisons d’hyper-paramètres et de sélectionner celles qui maximisent les performances prédictives, l’OOB étant souvent plus rapide et moins coûteuse, tandis que la validation croisée est plus fiable dans des situations où le surapprentissage est un risque important.",
    "crumbs": [
      "Détail",
      "La forêt aléatoire"
    ]
  },
  {
    "objectID": "chapters/chapter2/random_forest.html#interprétation-et-importance-des-variables",
    "href": "chapters/chapter2/random_forest.html#interprétation-et-importance-des-variables",
    "title": "La forêt aléatoire",
    "section": "",
    "text": "Objectif: Identifier les variables qui contribuent le plus à la prédiction/influencent le plus la variable cible, extraire des caractéristiques pertinentes pour comprendre les mécanismes de prédiction sous-jacent, établir des règles de décision simplifiées etc.\nMéthodes ususelles (mais biaisées):\n\nRéduction moyenne de l’impureté (Mean Decrease in Impurity) : pour chaque variable, on calcule la moyenne des réductions d’impureté qu’elle a engendrées dans tous les nœuds de tous les arbres où elle est impliquée. Les variables présentant la réduction moyenne d’impureté la plus élevée sont considérées comme les prédicteurs les plus importants.\nPermutation importance (Mean Decrease Accuracy) : Pour chaque variable, les performances du modèle sont comparées avant et après la permutation de ses valeurs. La différence moyenne de performance correspond à la MDA. L’idée est que si l’on permute aléatoirement les valeurs d’une variable (cassant ainsi sa relation avec la cible), une variable importante entraînera une hausse significative de l’erreur de généralisation.\n\nIl est essentiel de noter que ces deux mesures peuvent présenter des biais importants. Elles sont notamment sensibles aux variables catégorielles avec de nombreuses modalités, qui peuvent apparaître artificiellement importantes. Elles sont également fortement biaisée en présence de variables explicatives corrélées ou d’interactions complexes avec la cible.",
    "crumbs": [
      "Détail",
      "La forêt aléatoire"
    ]
  },
  {
    "objectID": "chapters/chapter2/random_forest.html#préparation-des-données-feature-engineering",
    "href": "chapters/chapter2/random_forest.html#préparation-des-données-feature-engineering",
    "title": "La forêt aléatoire",
    "section": "",
    "text": "Variables catégorielles : Encoder correctement (one-hot encoding, ordinal encoding).\nValeurs manquantes : Les forêts aléatoires peuvent gérer les données manquantes, mais une imputation préalable peut améliorer les performances.\nÉchelle des Variables : Pas nécessaire de normaliser, les arbres sont invariants aux transformations monotones.",
    "crumbs": [
      "Détail",
      "La forêt aléatoire"
    ]
  },
  {
    "objectID": "chapters/chapter1/comparaison_GB_RF.html",
    "href": "chapters/chapter1/comparaison_GB_RF.html",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "",
    "text": "Comparaison RF-GBDT\nLes forêts aléatoires et le gradient boosting paraissent très similaires au premier abord: il s’agit de deux approches ensemblistes, qui construisent des modèles très prédictifs performants en combinant un grand nombre d’arbres de décision. Mais en réalité, ces deux approches présentent plusieurs différences fondamentales:\n\nLes deux approches reposent sur des fondements théoriques différents: la loi des grands nombres pour les forêts aléatoires, la théorie de l’apprentissage statistique pour le boosting.\nLes arbres n’ont pas le même statut dans les deux approches. Dans une forêt aléatoire, les arbres sont entraînés indépendamment les uns des autres et constituent chacun un modèle à part entière, qui peut être utilisé, représenté et interprété isolément. Dans un modèle de boosting, les arbres sont entraînés séquentiellement, ce qui implique que chaque arbre n’a pas de sens indépendamment de l’ensemble des arbres qui l’ont précédé dans l’entraînement.\nLes points d’attention dans l’entraînement ne sont pas les mêmes: arbitrage puissance-corrélation dans la RF, arbitrage puissance-overfitting dans le boosting.\n\n\n\noverfitting: borne théorique à l’overfitting dans les RF, contre pas de borne dans le boosting. Deux conséquences: 1/ lutter contre l’overfitting est essentiel dans l’usage du boosting; 2/ le boosting est plus sensible au bruit et aux erreurs sur \\(y\\) que la RF.\nConditions d’utilisation: la RF peut être utilisée en OOB, pas le boosting.\nComplexité d’usage: peu d’hyperparamètres dans les RF, contre un grand nombre dans le boosting.",
    "crumbs": [
      "Survol",
      "Comparaison RF-GBDT"
    ]
  },
  {
    "objectID": "chapters/chapter1/survol.html",
    "href": "chapters/chapter1/survol.html",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "",
    "text": "Principe: cette partie propose une présentation intuitive des méthodes ensemblistes, à destination notamment des managers sans bagage en machine learning. Elle ne contient aucune formalisation mathématique.",
    "crumbs": [
      "Survol"
    ]
  },
  {
    "objectID": "chapters/chapter1/survol.html#que-sont-les-méthodes-ensemblistes",
    "href": "chapters/chapter1/survol.html#que-sont-les-méthodes-ensemblistes",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "Que sont les méthodes ensemblistes?",
    "text": "Que sont les méthodes ensemblistes?\n\nL’union fait la force\nPlutôt que de chercher à construire d’emblée un unique modèle très complexe, les approches ensemblistes vise à obtenir un modèle très performant en combinant un grand nombre de modèles simples.\nIl existe quatre grandes approches ensemblistes:\n\nle bagging;\nla random forest;\nle stacking;\nle boosting.\n\nLe présent document se concentre sur deux approches: la random forest et le boosting.\nLes méthodes ensemblistes désignent un ensemble d’algorithmes d’apprentissage supervisé (notamment les forêts aléatoires et le boosting) développés depuis le début des années 2000. Ces méthodes consistent à entraîner plusieurs modèles de base, puis à combiner les résultats obtenus afin de produire une prédiction consolidée. Les modèles de base, dits “apprenants faibles” (“weak learners”), sont généralement peu complexes. Le choix de ces modèles et la manière dont leurs prédictions sont combinées sont des facteurs clés pour la performance de ces approches.\nLes méthodes ensemblistes peuvent être divisées en deux grandes familles selon qu’elles s’appuient sur des modèles entrainés en parallèle ou de manière imbriquée ou séquentielle. Lorsque les modèles sont entrainés en parallèle, chaque modèle de base est entraîné en utilisant soit un échantillon aléatoire des données d’entraînement, soit un sous-ensemble des variables disponibles, et le plus souvent une combinaison des deux, auquel cas on parle de forêt aléatoire. Les implémentations les plus courantes des forêts aléatoires sont les packages ranger en R et scikit-learn en Python. Lorsque les modèles de base sont entrainés de manière séquentielle, chaque modèle de base vise à minimiser l’erreur de prédiction de l’ensemble des modèles de base précédents. Les implémentations les plus courantes du boosting sont actuellement XGBoost, CatBoost et LightGBM.",
    "crumbs": [
      "Survol"
    ]
  },
  {
    "objectID": "chapters/chapter1/survol.html#pourquoi-utiliser-des-méthodes-ensemblistes",
    "href": "chapters/chapter1/survol.html#pourquoi-utiliser-des-méthodes-ensemblistes",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "Pourquoi utiliser des méthodes ensemblistes?",
    "text": "Pourquoi utiliser des méthodes ensemblistes?\nLes méthodes ensemblistes sont particulièrement bien adaptées à de nombreux cas d’usage de la statistique publique, pour deux raisons. D’une part, elles sont conçues pour s’appliquer à des données tabulaires (enregistrements en lignes, variables en colonnes), structure de données omniprésente dans la statistique publique. D’autre part, elles peuvent être mobilisées dans toutes les situations où on utilise une régression linéaire ou une régression logistisque (imputation, repondération…).\nLes méthodes ensemblistes présentent trois avantages par rapport aux méthodes économétriques traditionnelles (régression linéaire et régression logistique):\n\nElles ont une puissance prédictive supérieure: alors que les méthodes traditionnelles supposent fréquemment l’existence d’une relation linéaire ou log-linéaire entre \\(y\\) et \\(\\mathbf{X}\\), les méthodes ensemblistes ne font quasiment aucune hypothèse sur la relation entre \\(y\\) et \\(\\mathbf{X}\\), et se contentent d’approximer le mieux possible cette relation à partir des données disponibles. En particulier, les modèles ensemblistes peuvent facilement modéliser des non-linéarités de la relation entre \\(y\\) et \\(\\mathbf{X}\\) et des interactions entre variables explicatives sans avoir à les spécifier explicitement au préalable, alors que les méthodes traditionnelles supposent fréquemment l’existence d’une relation linéaire ou log-linéaire entre \\(y\\) et \\(\\mathbf{X}\\).\nElles nécessitent moins de préparation des données: elles ne requièrent pas de normalisation des variables explicatives et peuvent s’accommoder des valeurs manquantes (selon des techniques variables selon les algorithmes).\nElles sont généralement moins sensibles aux valeurs extrêmes et à l’hétéroscédasticité des variables explicatives que les approches traditionnelles.\n\nElles présentent par ailleurs deux inconvénients rapport aux méthodes économétriques traditionnelles. Premièrement, bien qu’il existe désormais de multiples approches permettent d’interpétrer partiellement les modèles ensemblistes, leur interprétabilité reste globalement moindre que celle d’une régression linéaire ou logistique. Deuxièmement, les modèles ensemblistes sont plus complexes que les approches traditionnelles, et leurs hyperparamètres doivent faire l’objet d’une optimisation, par exemple au travers d’une validation croisée. Ce processus d’optimisation est généralement plus complexe et plus long que l’estimation d’une régression linéaire ou logistique. En revanche, utiliser des méthodes ensemblistes ne requiert pas de connaissances avancées en informatique ou de puissance de calcul importante.\n\n\n\n\n\n\nEt par rapport au deep learning?\n\n\n\nSi les approches de deep learning sont sans conteste très performantes pour le traitement du langage naturel et le traitement d’image, leur supériorité n’est pas établie pour les applications reposant sur des données tabulaires. Les comparaisons disponibles dans la littérature concluent en effet que les méthodes ensemblistes à base d’arbres sont soit plus performantes que les approches de deep learning (Grinsztajn, Oyallon, and Varoquaux (2022), Shwartz-Ziv and Armon (2022)), soit font jeu égal avec elles (McElfresh et al. (2024)). Ces études ont identifié trois avantages des méthodes ensemblistes: elles sont peu sensibles aux variables explicatives non pertinentes, robustes aux valeurs extrêmes des variables explicatives, et capables d’approximer des fonctions très irrégulières. De plus, dans la pratique les méthodes ensemblistes sont souvent plus rapides à entraîner et moins gourmandes en ressources informatiques, et l’optimisation des hyperparamètres s’avère souvent moins complexe (Shwartz-Ziv and Armon (2022)).",
    "crumbs": [
      "Survol"
    ]
  },
  {
    "objectID": "chapters/chapter1/survol.html#comment-fonctionnent-les-méthodes-ensemblistes",
    "href": "chapters/chapter1/survol.html#comment-fonctionnent-les-méthodes-ensemblistes",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "Comment fonctionnent les méthodes ensemblistes?",
    "text": "Comment fonctionnent les méthodes ensemblistes?\nQuatre temps:\n\nles arbres de décision et de régression (CART);\nles forêts aléatoires;\nle boosting.\n\n\nLe point de départ: les arbres de décision et de régression\nPrésenter decision tree et regression tree. Reprendre des éléments du chapitre 9 de https://bradleyboehmke.github.io/HOML/\nPrincipes d’un arbre:\n\nfonction constante par morceaux;\npartition de l’espace;\ninteractions entre variables.\n\nIllustration, et représentation graphique (sous forme d’arbre et de graphique).\n\n\nCritères de performance et sélection d’un modèle\nLa performance d’un modèle augmente généralement avec sa complexité, jusqu’à atteindre un maximum, puis diminue. L’objectif est d’obtenir un modèle qui minimise à la fois le sous-apprentissage (biais) et le sur-apprentissage (variance). C’est ce qu’on appelle le compromis biais/variance. Cette section présente très brièvement les critères utilisés pour évaluer et comparer les performances des modèles.\n\n\n\n\n\n\nFigure 1: Représentation schématique d’un algorithme de forêt aléatoire\n\n\n\n\n\n\n\n\n\nFigure 2: Représentation schématique d’un algorithme de boosting",
    "crumbs": [
      "Survol"
    ]
  },
  {
    "objectID": "chapters/chapter1/survol.html#le-bagging-les-random-forests-et-le-boosting",
    "href": "chapters/chapter1/survol.html#le-bagging-les-random-forests-et-le-boosting",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "Le bagging, les random forests et le boosting",
    "text": "Le bagging, les random forests et le boosting\nIl existe plusieurs types de méthodes ensemblistes, toutes ayant en commun la combinaison de modèles élémentaires. Le présent document présente les 3 principales méthodes : le Bagging, la Random Forests et le Boosting.\n\nLe bagging (Bootstrap Aggregating)\nPrésenter le bagging en reprenant des éléments du chapitre 10 de https://bradleyboehmke.github.io/HOML. Mettre une description de l’algorithme en pseudo-code?\n\nPrésentation avec la figure en SVG;\nIllustration avec un cas d’usage de classification en deux dimensions.\n\nLe bagging, ou Bootstrap Aggregating, est une méthode ensembliste qui comporte trois étapes principales :\nCréation de sous-échantillons : À partir du jeu de données initial, plusieurs sous-échantillons sont générés par échantillonnage aléatoire avec remise (bootstrapping). Chaque sous-échantillon a la même taille que le jeu de données original, mais peut contenir des observations répétées, tandis que d'autres peuvent être omises. Cette technique permet de diversifier les données d'entraînement en créant des échantillons variés, ce qui aide à réduire la variance et à améliorer la robustesse du modèle.\n\nEntraînement parallèle : Un modèle distinct est entraîné sur chaque sous-échantillon de manière indépendante. Cette technique permet un gain d'efficacité et un meilleur contrôle du surapprentissage (overfitting).\n\nAgrégation des prédictions : Les prédictions des modèles sont combinées pour produire le résultat final. En classification, la prédiction finale est souvent déterminée par un vote majoritaire, tandis qu'en régression, elle correspond généralement à la moyenne des prédictions. En combinant les prédictions de plusieurs modèles, le bagging renforce la stabilité et la performance globale de l'algorithme, notamment en réduisant la variance des prédictions.\nLe bagging appliqué aux arbres de décision est la forme la plus courante de cette technique.\nLe bagging est particulièrement efficace pour réduire la variance des modèles, ce qui les rend moins vulnérables au surapprentissage. Cette caractéristique est particulièrement utile dans les situations où la robustesse et la capacité de généralisation des modèles sont cruciales. De plus, comme le bagging repose sur des processus indépendants, l’exécution est plus plus rapide dans des environnements distribués.\nCependant, bien que chaque modèle de base soit construit indépendamment sur des sous-échantillons distincts, les variables utilisées pour générer ces modèles ne sont pas forcément indépendantes d’un modèle à l’autre. Dans le cas du bagging appliqué aux arbres de décision, cela conduit souvent à des arbres ayant une structure similaire.\nLes forêts aléatoires apportent une amélioration à cette approche en réduisant cette corrélation entre les arbres, ce qui permet d’augmenter la précision de l’ensemble du modèle.\n\n\nLes random forests\nExpliquer que les random forests sont une amélioration du bagging, en reprenant des éléments du chapitre 11 de https://bradleyboehmke.github.io/HOML/\n\n\n\nPrésentation avec la figure en SVG;\nDifficile d’illustrer avec un exemple (car on ne peut pas vraiment représenter le feature sampling);\nBien insister sur les avantages des RF: 1/ faible nombre d’hyperparamètres; 2/ faible sensibilité aux hyperparamètres; 3/ limite intrinsèque à l’overfitting.\n\n\n\nLe boosting\nReprendre des éléments du chapitre 12 de https://bradleyboehmke.github.io/HOML/ et des éléments de la formation boosting.\nLe boosting combine l’approche ensembliste avec une modélisation additive par étapes (forward stagewise additive modeling).\n\nPrésentation;\nAvantage du boosting: performances particulièrement élevées.\nInconvénients: 1/ nombre élevé d’hyperparamètres; 2/ sensibilité des performances aux hyperparamètres; 3/ risque élevé d’overfitting.\nPréciser qu’il est possible d’utiliser du subsampling par lignes et colonnes pour un algoithme de boosting. Ce point est abordé plus en détail dans la partie sur les hyperparamètres.",
    "crumbs": [
      "Survol"
    ]
  },
  {
    "objectID": "chapters/chapter2/bagging.html",
    "href": "chapters/chapter2/bagging.html",
    "title": "Le bagging",
    "section": "",
    "text": "Le bagging, ou “bootstrap aggregating”, est une méthode ensembliste qui vise à améliorer la stabilité et la précision des algorithmes d’apprentissage automatique en agrégeant plusieurs modèles (Breiman (1996)). Chaque modèle est entraîné sur un échantillon distinct généré par une technique de rééchantillonnage (bootstrap). Ces modèles sont ensuite combinés pour produire une prédiction agrégée, souvent plus robuste et généralisable que celle obtenue par un modèle unique.\n\n\n\nLe bagging comporte trois étapes principales:\n\nL’échantillonnage bootstrap : L’échantillonnage bootstrap consiste à créer des échantillons distincts en tirant aléatoirement avec remise des observations du jeu de données initial. Chaque échantillon bootstrap contient le même nombre d’observations que le jeu de données initial, mais certaines observations sont répétées (car sélectionnées plusieurs fois), tandis que d’autres sont omises.\nL’entraînement de plusieurs modèles : Un modèle (aussi appelé apprenant de base ou weak learner) est entraîné sur chaque échantillon bootstrap. Les modèles peuvent être des arbres de décision, des régressions ou tout autre algorithme d’apprentissage. Le bagging est particulièrement efficace avec des modèles instables, tels que les arbres de décision non élagués.\nL’agrégation des prédictions : Les prédictions de tous les modèles sont ensuite agrégées, en procédant généralement à la moyenne (ou à la médiane) des prédictions dans le cas de la régression, et au vote majoritaire (ou à la moyenne des probabilités prédites pour chaque classe) dans le cas de la classification, afin d’obtenir des prédictions plus précises et généralisables.\n\n\n\n\nCertains modèles sont très sensibles aux données d’entraînement, et leurs prédictions sont très instables d’un échantillon à l’autre. L’objectif du bagging est de construire un prédicteur plus précis en agrégeant les prédictions de plusieurs modèles entraînés sur des échantillons (légèrement) différents les uns des autres.\nBreiman (1996) montre que cette méthode est particulièrement efficace lorsqu’elle est appliquée à des modèles très instables, dont les performances sont particulièrement sensibles aux variations du jeu de données d’entraînement, et peu biaisés.\nCette section vise à mieux comprendre comment (et sous quelles conditions) l’agrégation par bagging permet de construire un prédicteur plus performant.\nDans la suite, nous notons \\(φ(x, L)\\) un prédicteur (d’une valeur numérique dans le cas de la régression ou d’un label dans le cas de la classification), entraîné sur un ensemble d’apprentissage \\(L\\), et prenant en entrée un vecteur de caractéristiques \\(x\\).\n\n\nDans le contexte de la régression, l’objectif est de prédire une valeur numérique \\(Y\\) à partir d’un vecteur de caractéristiques \\(x\\). Un modèle de régression \\(\\phi(x, L)\\) est construit à partir d’un ensemble d’apprentissage \\(L\\), et produit une estimation de \\(Y\\) pour chaque observation \\(x\\).\n\n\nDans le cas de la régression, le prédicteur agrégé est défini comme suit :\n$ _A(x) = E_L[(x, L)] $\noù \\(\\phi_A(x)\\) représente la prédiction agrégée, \\(E_L[.]\\) correspond à l’espérance prise sur tous les échantillons d’apprentissage possibles \\(L\\), chacun étant tiré selon la même distribution que le jeu de données initial, et \\(\\phi(x, L)\\) correspond à la prédiction du modèle construit sur l’échantillon d’apprentissage \\(L\\).\n\n\n\nPour mieux comprendre comment l’agrégation améliore la performance globale d’un modèle individuel \\(\\phi(x, L)\\), revenons à la décomposition biais-variance de l’erreur quadratique moyenne (il s’agit de la mesure de performance classiquement considérée dans un problème de régression):\n\\[E_L[\\left(Y - \\phi(x, L)\\right)^2] = \\underbrace{\\left(E_L\\left[\\phi(x, L) - Y\\right]\\right)^2}_{\\text{Biais}^2} + \\underbrace{E_L[\\left(\\phi(x, L) - E_L[\\phi(x, L)]\\right)^2]}_{\\text{Variance}} \\tag{1}\\]\n\nLe biais est la différence entre la valeur observée \\(Y\\) que l’on souhaite prédire et la prédiction moyenne \\(E_L[\\phi(x, L)]\\). Si le modèle est sous-ajusté, le biais sera élevé.\nLa variance est la variabilité des prédictions (\\(\\phi(x, L)\\)) autour de leur moyenne (\\(E_L[\\phi(x, L)]\\)). Un modèle avec une variance élevée est très sensible aux fluctuations au sein des données d’entraînement: ses prédictions varient beaucoup lorsque les données d’entraînement se modifient.\n\nL’équation Equation 1 illustre l’arbitrage biais-variance qui est omniprésent en machine learning: plus la complexité d’un modèle s’accroît (exemple: la profondeur d’un arbre), plus son biais sera plus faible (car ses prédictions seront de plus en plus proches des données d’entraînement), et plus sa variance sera élevée (car ses prédictions, étant très proches des données d’entraînement, auront tendance à varier fortement d’un jeu d’entraînement à l’autre).\n\n\n\nBreiman (1996) compare l’erreur quadratique moyenne d’un modèle individuel avec celle du modèle agrégé et démontre l’inégalité suivante :\n\n\\[ (Y - \\phi_A(x))^2 \\leq E_L[(Y - \\phi(x, L))^2] \\tag{2}\\]\n\nLe terme \\((Y - \\phi_A(x))^2\\) représente l’erreur quadratique du prédicteur agrégé \\(\\phi_A(x)\\);\nLe terme \\(E_L[(Y - \\phi(x, L))^2]\\) est l’erreur quadratique moyenne d’un prédicteur individuel \\(\\phi(x, L)\\) entraîné sur un échantillon aléatoire \\(L\\). Cette erreur varie en fonction des données d’entraînement.\n\nCette inégalité montre que l’erreur quadratique moyenne du prédicteur agrégé est toujours inférieure ou égale à la moyenne des erreurs des prédicteurs individuels. Puisque le biais du prédicteur agrégé est identique au biais du prédicteur individuel, alors l’inégalité précédente implique que la variance du modèle agrégé \\(\\phi_A(x)\\) est toujours inférieure ou égale à la variance moyenne d’un modèle individuel :\n\\(\\text{Var}(\\phi_A(x)) = \\text{Var}(E_L[\\phi(x, L)]) \\leq E_L[\\text{Var}(\\phi(x, L))]\\)\nAutrement dit, le processus d’agrégation réduit l’erreur de prédiction globale en réduisant la variance des prédictions, tout en conservant un biais constant.\nCe résultat ouvre la voie à des considérations pratiques immédiates. Lorsque le modèle individuel est instable et présente une variance élevée, l’inégalité \\(Var(\\phi_A(x)) \\leq E_L[Var(\\phi(x,L))]\\) est forte, ce qui signifie que l’agrégation peut améliorer significativement la performance globale du modèle. En revanche, si \\(ϕ(x,L)\\) varie peu d’un ensemble d’entraînement à un autre (modèle stable avec variance faible), alors \\(Var(\\phi_A(x))\\) est proche de \\(E_L[Var(\\phi(x,L))]\\), et la réduction de variance apportée par l’agrégation est faible. Ainsi, le bagging est particulièrement efficace pour les modèles instables, tels que les arbres de décision, mais moins efficace pour les modèles stables tels que les méthodes des k plus proches voisins.\n\n\n\n\nDans le cas de la classification, le mécanisme de réduction de la variance par le bagging permet, sous une certaine condition, d’atteindre un classificateur presque optimal (nearly optimal classifier). Ce concept a été introduit par Breiman (1996) pour décrire un modèle qui tend à classer une observation dans la classe la plus probable, avec une performance approchant celle du classificateur Bayésien optimal (la meilleure performance théorique qu’un modèle de classification puisse atteindre).\nPour comprendre ce résutlat, introduisons \\(Q(j|x) = E_L(1_{φ(x, L) = j}) = P(φ(x, L) = j)\\), la probabilité qu’un modèle \\(φ(x, L)\\) prédise la classe \\(j\\) pour l’observation \\(x\\), et \\(P(j|x)\\), la probabilité réelle (conditionnelle) que \\(x\\) appartienne à la classe \\(j\\).\n\n\nUn classificateur \\(φ(x, L)\\) est dit order-correct pour une observation \\(x\\) si, en espérance, il identifie correctement la classe la plus probable, même s’il ne prédit pas toujours avec exactitude les probabilités associées à chaque classe \\(Q(j∣x)\\).\nCela signifie que si l’on considérait tous les ensemble de données possibles, et que l’on évaluait les prédictions du modèle en \\(x\\), la majorité des prédictions correspondraient à la classe à laquelle il a la plus grande probabilité vraie d’appartenir \\(P(j∣x)\\).\nFormellement, un prédicteur est dit “order-correct” pour une entrée \\(x\\) si :\n\\(argmax_j Q(j|x) = argmax_j P(j|x)\\)\noù \\(P(j|x)\\) est la vraie probabilité que l’observation \\(x\\) appartienne à la classe \\(j\\), et \\(Q(j|x)\\) est la probabilité que \\(x\\) appartienne à la classe \\(j\\) prédite par le modèle \\(φ(x, L)\\).\nUn classificateur est order-correct si, pour chaque observation \\(x\\), la classe qu’il prédit correspond à celle qui a la probabilité maximale \\(P(j|x)\\) dans la distribution vraie.\n\n\n\nDans le cas de la classification, le prédicteur agrégé est défini par le vote majoritaire. Cela signifie que si \\(K\\) classificateurs sont entraînés sur \\(K\\) échantillons distincts, la classe prédite pour \\(x\\) est celle qui reçoit le plus de votes de la part des modèles individuels.\nFormellement, le classificateur agrégé \\(φA(x)\\) est défini par :\n\\(φA(x) =  \\text{argmax}_j \\sum_{L} I(\\phi(x, L) = j) = argmax_j Q(j|x)\\)\n\n\n\nBreiman (1996) montre que si chaque prédicteur individuel \\(φ(x, L)\\) est order-correct pour une observation \\(x\\), alors le prédicteur agrégé \\(φA(x)\\), obtenu par vote majoritaire, atteint la performance optimale pour cette observation, c’est-à-dire qu’il converge vers la classe ayant la probabilité maximale \\(P(j∣x)\\) pour l’observation \\(x\\) lorsque le nombre de prédicteurs individuels augmente. Le vote majoritaire permet ainsi de réduire les erreurs aléatoires des classificateurs individuels.\nLe classificateur agrégé \\(ϕA\\) est optimal s’il prédit systématiquement la classe la plus probable pour l’observation \\(x\\) dans toutes les régions de l’espace.\nCependant, dans les régions de l’espace où les classificateurs individuels ne sont pas order-corrects (c’est-à-dire qu’ils se trompent majoritairement sur la classe d’appartenance), l’agrégation par vote majoritaire n’améliore pas les performances. Elles peuvent même se détériorer par rapport aux modèles individuels si l’agrégation conduit à amplifier des erreurs systématiques (biais).\n\n\n\n\n\nEn pratique, au lieu d’utiliser tous les ensembles d’entraînement possibles \\(L\\), le bagging repose sur un nombre limité d’échantillons bootstrap tirés avec remise à partir d’un même jeu de données initial, ce qui peut introduire des biais par rapport au prédicteur agrégé théorique.\nLes échantillons bootstrap présentent les limites suivantes :\n\nUne taille effective réduite par rapport au jeu de données initial: Bien que chaque échantillon bootstrap présente le même nombre d’observations que le jeu de données initial, environ 1/3 des observations (uniques) du jeu initial sont absentes de chaque échantillon bootstrap (du fait du tirage avec remise). Cela peut limiter la capacité des modèles à capturer des relations complexes au sein des données (et aboutir à des modèles individuels sous-ajustés par rapport à ce qui serait attendu théoriquement), en particulier lorsque l’échantillon initial est de taille modeste.\nUne dépendance entre échantillons : Les échantillons bootstrap sont tirés dans le même jeu de données, ce qui génère une dépendance entre eux, qui réduit la diversité des modèles. Cela peut limiter l’efficacité de la réduction de variance dans le cas de la régression, voire acroître le biais dans le cas de la classification.\nUne couverture incomplète de l’ensemble des échantillons possibles: Les échantillons bootstrap ne couvrent pas l’ensemble des échantillons d’entraînement possibles, ce qui peut introduire un biais supplémentaire par rapport au prédicteur agrégé théorique.\n\n\n\n\n\n\nLe bagging est particulièrement utile lorsque les modèles individuels présentent une variance élevée et sont instables. Dans de tels cas, l’agrégation des prédictions peut réduire significativement la variance globale, améliorant ainsi la performance du modèle agrégé. Les situations où le bagging est recommandé incluent typiquement:\n\nLes modèles instables : Les modèles tels que les arbres de décision non élagués, qui sont sensibles aux variations des données d’entraînement, bénéficient grandement du bagging. L’agrégation atténue les fluctuations des prédictions dues aux différents échantillons.\nLes modèles avec biais faibles: En classification, si les modèles individuels sont order-corrects pour la majorité des observations, le bagging peut améliorer la précision en renforçant les prédictions correctes et en réduisant les erreurs aléatoires.\n\nInversement, le bagging peut être moins efficace ou même néfaste dans certaines situations :\n\nLes modèles stables avec variance faible : Si les modèles individuels sont déjà stables et présentent une faible variance (par exemple, la régression linéaire), le bagging n’apporte que peu d’amélioration, car la réduction de variance supplémentaire est minimale.\nLa présence de biais élevée : Si les modèles individuels sont biaisés, entraînant des erreurs systématiques, le bagging peut amplifier ces erreurs plutôt que de les corriger. Dans de tels cas, il est préférable de s’attaquer d’abord au biais des modèles avant de considérer l’agrégation.\nLes échantillons de petite taille : Avec des ensembles de données limités, les échantillons bootstrap peuvent ne pas être suffisamment diversifiés ou représentatifs, ce qui réduit l’efficacité du bagging et peut augmenter le biais des modèles.\n\nCe qui qu’il faut retenir: le bagging peut améliorer substantiellement la performance des modèles d’apprentissage automatique lorsqu’il est appliqué dans des conditions appropriées. Il est essentiel d’évaluer la variance et le biais des modèles individuels, ainsi que la taille et la représentativité du jeu de données, pour déterminer si le bagging est une stratégie adaptée. Lorsqu’il est utilisé judicieusement, le bagging peut conduire à des modèles plus robustes et précis, exploitant efficacement la puissance de l’agrégation pour améliorer la performance des modèles individuels.\n\n\n\n\n\n“Optimal performance is often found by bagging 50–500 trees. Data sets that have a few strong predictors typically require less trees; whereas data sets with lots of noise or multiple strong predictors may need more. Using too many trees will not lead to overfitting. However, it’s important to realize that since multiple models are being run, the more iterations you perform the more computational and time requirements you will have. As these demands increase, performing k-fold CV can become computationally burdensome.”\n\n\n\n“A benefit to creating ensembles via bagging, which is based on resampling with replacement, is that it can provide its own internal estimate of predictive performance with the out-of-bag (OOB) sample (see Section 2.4.2). The OOB sample can be used to test predictive performance and the results usually compare well compared to k-fold CV assuming your data set is sufficiently large (say n≥1,000). Consequently, as your data sets become larger and your bagging iterations increase, it is common to use the OOB error estimate as a proxy for predictive performance.”\n\n\n\n\n\nOu bien ne commencer les mises en pratique qu’avec les random forest ?",
    "crumbs": [
      "Détail",
      "Le bagging"
    ]
  },
  {
    "objectID": "chapters/chapter2/bagging.html#principe-du-bagging",
    "href": "chapters/chapter2/bagging.html#principe-du-bagging",
    "title": "Le bagging",
    "section": "",
    "text": "Le bagging comporte trois étapes principales:\n\nL’échantillonnage bootstrap : L’échantillonnage bootstrap consiste à créer des échantillons distincts en tirant aléatoirement avec remise des observations du jeu de données initial. Chaque échantillon bootstrap contient le même nombre d’observations que le jeu de données initial, mais certaines observations sont répétées (car sélectionnées plusieurs fois), tandis que d’autres sont omises.\nL’entraînement de plusieurs modèles : Un modèle (aussi appelé apprenant de base ou weak learner) est entraîné sur chaque échantillon bootstrap. Les modèles peuvent être des arbres de décision, des régressions ou tout autre algorithme d’apprentissage. Le bagging est particulièrement efficace avec des modèles instables, tels que les arbres de décision non élagués.\nL’agrégation des prédictions : Les prédictions de tous les modèles sont ensuite agrégées, en procédant généralement à la moyenne (ou à la médiane) des prédictions dans le cas de la régression, et au vote majoritaire (ou à la moyenne des probabilités prédites pour chaque classe) dans le cas de la classification, afin d’obtenir des prédictions plus précises et généralisables.",
    "crumbs": [
      "Détail",
      "Le bagging"
    ]
  },
  {
    "objectID": "chapters/chapter2/bagging.html#pourquoi-et-dans-quelles-situations-le-bagging-fonctionne",
    "href": "chapters/chapter2/bagging.html#pourquoi-et-dans-quelles-situations-le-bagging-fonctionne",
    "title": "Le bagging",
    "section": "",
    "text": "Certains modèles sont très sensibles aux données d’entraînement, et leurs prédictions sont très instables d’un échantillon à l’autre. L’objectif du bagging est de construire un prédicteur plus précis en agrégeant les prédictions de plusieurs modèles entraînés sur des échantillons (légèrement) différents les uns des autres.\nBreiman (1996) montre que cette méthode est particulièrement efficace lorsqu’elle est appliquée à des modèles très instables, dont les performances sont particulièrement sensibles aux variations du jeu de données d’entraînement, et peu biaisés.\nCette section vise à mieux comprendre comment (et sous quelles conditions) l’agrégation par bagging permet de construire un prédicteur plus performant.\nDans la suite, nous notons \\(φ(x, L)\\) un prédicteur (d’une valeur numérique dans le cas de la régression ou d’un label dans le cas de la classification), entraîné sur un ensemble d’apprentissage \\(L\\), et prenant en entrée un vecteur de caractéristiques \\(x\\).\n\n\nDans le contexte de la régression, l’objectif est de prédire une valeur numérique \\(Y\\) à partir d’un vecteur de caractéristiques \\(x\\). Un modèle de régression \\(\\phi(x, L)\\) est construit à partir d’un ensemble d’apprentissage \\(L\\), et produit une estimation de \\(Y\\) pour chaque observation \\(x\\).\n\n\nDans le cas de la régression, le prédicteur agrégé est défini comme suit :\n$ _A(x) = E_L[(x, L)] $\noù \\(\\phi_A(x)\\) représente la prédiction agrégée, \\(E_L[.]\\) correspond à l’espérance prise sur tous les échantillons d’apprentissage possibles \\(L\\), chacun étant tiré selon la même distribution que le jeu de données initial, et \\(\\phi(x, L)\\) correspond à la prédiction du modèle construit sur l’échantillon d’apprentissage \\(L\\).\n\n\n\nPour mieux comprendre comment l’agrégation améliore la performance globale d’un modèle individuel \\(\\phi(x, L)\\), revenons à la décomposition biais-variance de l’erreur quadratique moyenne (il s’agit de la mesure de performance classiquement considérée dans un problème de régression):\n\\[E_L[\\left(Y - \\phi(x, L)\\right)^2] = \\underbrace{\\left(E_L\\left[\\phi(x, L) - Y\\right]\\right)^2}_{\\text{Biais}^2} + \\underbrace{E_L[\\left(\\phi(x, L) - E_L[\\phi(x, L)]\\right)^2]}_{\\text{Variance}} \\tag{1}\\]\n\nLe biais est la différence entre la valeur observée \\(Y\\) que l’on souhaite prédire et la prédiction moyenne \\(E_L[\\phi(x, L)]\\). Si le modèle est sous-ajusté, le biais sera élevé.\nLa variance est la variabilité des prédictions (\\(\\phi(x, L)\\)) autour de leur moyenne (\\(E_L[\\phi(x, L)]\\)). Un modèle avec une variance élevée est très sensible aux fluctuations au sein des données d’entraînement: ses prédictions varient beaucoup lorsque les données d’entraînement se modifient.\n\nL’équation Equation 1 illustre l’arbitrage biais-variance qui est omniprésent en machine learning: plus la complexité d’un modèle s’accroît (exemple: la profondeur d’un arbre), plus son biais sera plus faible (car ses prédictions seront de plus en plus proches des données d’entraînement), et plus sa variance sera élevée (car ses prédictions, étant très proches des données d’entraînement, auront tendance à varier fortement d’un jeu d’entraînement à l’autre).\n\n\n\nBreiman (1996) compare l’erreur quadratique moyenne d’un modèle individuel avec celle du modèle agrégé et démontre l’inégalité suivante :\n\n\\[ (Y - \\phi_A(x))^2 \\leq E_L[(Y - \\phi(x, L))^2] \\tag{2}\\]\n\nLe terme \\((Y - \\phi_A(x))^2\\) représente l’erreur quadratique du prédicteur agrégé \\(\\phi_A(x)\\);\nLe terme \\(E_L[(Y - \\phi(x, L))^2]\\) est l’erreur quadratique moyenne d’un prédicteur individuel \\(\\phi(x, L)\\) entraîné sur un échantillon aléatoire \\(L\\). Cette erreur varie en fonction des données d’entraînement.\n\nCette inégalité montre que l’erreur quadratique moyenne du prédicteur agrégé est toujours inférieure ou égale à la moyenne des erreurs des prédicteurs individuels. Puisque le biais du prédicteur agrégé est identique au biais du prédicteur individuel, alors l’inégalité précédente implique que la variance du modèle agrégé \\(\\phi_A(x)\\) est toujours inférieure ou égale à la variance moyenne d’un modèle individuel :\n\\(\\text{Var}(\\phi_A(x)) = \\text{Var}(E_L[\\phi(x, L)]) \\leq E_L[\\text{Var}(\\phi(x, L))]\\)\nAutrement dit, le processus d’agrégation réduit l’erreur de prédiction globale en réduisant la variance des prédictions, tout en conservant un biais constant.\nCe résultat ouvre la voie à des considérations pratiques immédiates. Lorsque le modèle individuel est instable et présente une variance élevée, l’inégalité \\(Var(\\phi_A(x)) \\leq E_L[Var(\\phi(x,L))]\\) est forte, ce qui signifie que l’agrégation peut améliorer significativement la performance globale du modèle. En revanche, si \\(ϕ(x,L)\\) varie peu d’un ensemble d’entraînement à un autre (modèle stable avec variance faible), alors \\(Var(\\phi_A(x))\\) est proche de \\(E_L[Var(\\phi(x,L))]\\), et la réduction de variance apportée par l’agrégation est faible. Ainsi, le bagging est particulièrement efficace pour les modèles instables, tels que les arbres de décision, mais moins efficace pour les modèles stables tels que les méthodes des k plus proches voisins.\n\n\n\n\nDans le cas de la classification, le mécanisme de réduction de la variance par le bagging permet, sous une certaine condition, d’atteindre un classificateur presque optimal (nearly optimal classifier). Ce concept a été introduit par Breiman (1996) pour décrire un modèle qui tend à classer une observation dans la classe la plus probable, avec une performance approchant celle du classificateur Bayésien optimal (la meilleure performance théorique qu’un modèle de classification puisse atteindre).\nPour comprendre ce résutlat, introduisons \\(Q(j|x) = E_L(1_{φ(x, L) = j}) = P(φ(x, L) = j)\\), la probabilité qu’un modèle \\(φ(x, L)\\) prédise la classe \\(j\\) pour l’observation \\(x\\), et \\(P(j|x)\\), la probabilité réelle (conditionnelle) que \\(x\\) appartienne à la classe \\(j\\).\n\n\nUn classificateur \\(φ(x, L)\\) est dit order-correct pour une observation \\(x\\) si, en espérance, il identifie correctement la classe la plus probable, même s’il ne prédit pas toujours avec exactitude les probabilités associées à chaque classe \\(Q(j∣x)\\).\nCela signifie que si l’on considérait tous les ensemble de données possibles, et que l’on évaluait les prédictions du modèle en \\(x\\), la majorité des prédictions correspondraient à la classe à laquelle il a la plus grande probabilité vraie d’appartenir \\(P(j∣x)\\).\nFormellement, un prédicteur est dit “order-correct” pour une entrée \\(x\\) si :\n\\(argmax_j Q(j|x) = argmax_j P(j|x)\\)\noù \\(P(j|x)\\) est la vraie probabilité que l’observation \\(x\\) appartienne à la classe \\(j\\), et \\(Q(j|x)\\) est la probabilité que \\(x\\) appartienne à la classe \\(j\\) prédite par le modèle \\(φ(x, L)\\).\nUn classificateur est order-correct si, pour chaque observation \\(x\\), la classe qu’il prédit correspond à celle qui a la probabilité maximale \\(P(j|x)\\) dans la distribution vraie.\n\n\n\nDans le cas de la classification, le prédicteur agrégé est défini par le vote majoritaire. Cela signifie que si \\(K\\) classificateurs sont entraînés sur \\(K\\) échantillons distincts, la classe prédite pour \\(x\\) est celle qui reçoit le plus de votes de la part des modèles individuels.\nFormellement, le classificateur agrégé \\(φA(x)\\) est défini par :\n\\(φA(x) =  \\text{argmax}_j \\sum_{L} I(\\phi(x, L) = j) = argmax_j Q(j|x)\\)\n\n\n\nBreiman (1996) montre que si chaque prédicteur individuel \\(φ(x, L)\\) est order-correct pour une observation \\(x\\), alors le prédicteur agrégé \\(φA(x)\\), obtenu par vote majoritaire, atteint la performance optimale pour cette observation, c’est-à-dire qu’il converge vers la classe ayant la probabilité maximale \\(P(j∣x)\\) pour l’observation \\(x\\) lorsque le nombre de prédicteurs individuels augmente. Le vote majoritaire permet ainsi de réduire les erreurs aléatoires des classificateurs individuels.\nLe classificateur agrégé \\(ϕA\\) est optimal s’il prédit systématiquement la classe la plus probable pour l’observation \\(x\\) dans toutes les régions de l’espace.\nCependant, dans les régions de l’espace où les classificateurs individuels ne sont pas order-corrects (c’est-à-dire qu’ils se trompent majoritairement sur la classe d’appartenance), l’agrégation par vote majoritaire n’améliore pas les performances. Elles peuvent même se détériorer par rapport aux modèles individuels si l’agrégation conduit à amplifier des erreurs systématiques (biais).",
    "crumbs": [
      "Détail",
      "Le bagging"
    ]
  },
  {
    "objectID": "chapters/chapter2/bagging.html#léchantillage-par-bootstrap-peut-détériorer-les-performances-théoriques-du-modèle-agrégé",
    "href": "chapters/chapter2/bagging.html#léchantillage-par-bootstrap-peut-détériorer-les-performances-théoriques-du-modèle-agrégé",
    "title": "Le bagging",
    "section": "",
    "text": "En pratique, au lieu d’utiliser tous les ensembles d’entraînement possibles \\(L\\), le bagging repose sur un nombre limité d’échantillons bootstrap tirés avec remise à partir d’un même jeu de données initial, ce qui peut introduire des biais par rapport au prédicteur agrégé théorique.\nLes échantillons bootstrap présentent les limites suivantes :\n\nUne taille effective réduite par rapport au jeu de données initial: Bien que chaque échantillon bootstrap présente le même nombre d’observations que le jeu de données initial, environ 1/3 des observations (uniques) du jeu initial sont absentes de chaque échantillon bootstrap (du fait du tirage avec remise). Cela peut limiter la capacité des modèles à capturer des relations complexes au sein des données (et aboutir à des modèles individuels sous-ajustés par rapport à ce qui serait attendu théoriquement), en particulier lorsque l’échantillon initial est de taille modeste.\nUne dépendance entre échantillons : Les échantillons bootstrap sont tirés dans le même jeu de données, ce qui génère une dépendance entre eux, qui réduit la diversité des modèles. Cela peut limiter l’efficacité de la réduction de variance dans le cas de la régression, voire acroître le biais dans le cas de la classification.\nUne couverture incomplète de l’ensemble des échantillons possibles: Les échantillons bootstrap ne couvrent pas l’ensemble des échantillons d’entraînement possibles, ce qui peut introduire un biais supplémentaire par rapport au prédicteur agrégé théorique.",
    "crumbs": [
      "Détail",
      "Le bagging"
    ]
  },
  {
    "objectID": "chapters/chapter2/bagging.html#le-bagging-en-pratique",
    "href": "chapters/chapter2/bagging.html#le-bagging-en-pratique",
    "title": "Le bagging",
    "section": "",
    "text": "Le bagging est particulièrement utile lorsque les modèles individuels présentent une variance élevée et sont instables. Dans de tels cas, l’agrégation des prédictions peut réduire significativement la variance globale, améliorant ainsi la performance du modèle agrégé. Les situations où le bagging est recommandé incluent typiquement:\n\nLes modèles instables : Les modèles tels que les arbres de décision non élagués, qui sont sensibles aux variations des données d’entraînement, bénéficient grandement du bagging. L’agrégation atténue les fluctuations des prédictions dues aux différents échantillons.\nLes modèles avec biais faibles: En classification, si les modèles individuels sont order-corrects pour la majorité des observations, le bagging peut améliorer la précision en renforçant les prédictions correctes et en réduisant les erreurs aléatoires.\n\nInversement, le bagging peut être moins efficace ou même néfaste dans certaines situations :\n\nLes modèles stables avec variance faible : Si les modèles individuels sont déjà stables et présentent une faible variance (par exemple, la régression linéaire), le bagging n’apporte que peu d’amélioration, car la réduction de variance supplémentaire est minimale.\nLa présence de biais élevée : Si les modèles individuels sont biaisés, entraînant des erreurs systématiques, le bagging peut amplifier ces erreurs plutôt que de les corriger. Dans de tels cas, il est préférable de s’attaquer d’abord au biais des modèles avant de considérer l’agrégation.\nLes échantillons de petite taille : Avec des ensembles de données limités, les échantillons bootstrap peuvent ne pas être suffisamment diversifiés ou représentatifs, ce qui réduit l’efficacité du bagging et peut augmenter le biais des modèles.\n\nCe qui qu’il faut retenir: le bagging peut améliorer substantiellement la performance des modèles d’apprentissage automatique lorsqu’il est appliqué dans des conditions appropriées. Il est essentiel d’évaluer la variance et le biais des modèles individuels, ainsi que la taille et la représentativité du jeu de données, pour déterminer si le bagging est une stratégie adaptée. Lorsqu’il est utilisé judicieusement, le bagging peut conduire à des modèles plus robustes et précis, exploitant efficacement la puissance de l’agrégation pour améliorer la performance des modèles individuels.\n\n\n\n\n\n“Optimal performance is often found by bagging 50–500 trees. Data sets that have a few strong predictors typically require less trees; whereas data sets with lots of noise or multiple strong predictors may need more. Using too many trees will not lead to overfitting. However, it’s important to realize that since multiple models are being run, the more iterations you perform the more computational and time requirements you will have. As these demands increase, performing k-fold CV can become computationally burdensome.”\n\n\n\n“A benefit to creating ensembles via bagging, which is based on resampling with replacement, is that it can provide its own internal estimate of predictive performance with the out-of-bag (OOB) sample (see Section 2.4.2). The OOB sample can be used to test predictive performance and the results usually compare well compared to k-fold CV assuming your data set is sufficiently large (say n≥1,000). Consequently, as your data sets become larger and your bagging iterations increase, it is common to use the OOB error estimate as a proxy for predictive performance.”",
    "crumbs": [
      "Détail",
      "Le bagging"
    ]
  },
  {
    "objectID": "chapters/chapter2/bagging.html#mise-en-pratique-exemple-avec-code",
    "href": "chapters/chapter2/bagging.html#mise-en-pratique-exemple-avec-code",
    "title": "Le bagging",
    "section": "",
    "text": "Ou bien ne commencer les mises en pratique qu’avec les random forest ?",
    "crumbs": [
      "Détail",
      "Le bagging"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "",
    "text": "Introduction\nUne bien belle introduction",
    "crumbs": [
      "Introduction aux méthodes ensemblistes"
    ]
  }
]