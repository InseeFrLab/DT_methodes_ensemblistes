[
  {
    "objectID": "chapters/chapter3/0-intro.html",
    "href": "chapters/chapter3/0-intro.html",
    "title": "Comment bien utiliser les méthodes ensemblistes?",
    "section": "",
    "text": "Comment bien utiliser les méthodes ensemblistes?\nPrincipe: Conseils + mise en oeuvre pratique.",
    "crumbs": [
      "Comment bien utiliser les algorithmes?"
    ]
  },
  {
    "objectID": "chapters/chapter2/0-intro.html",
    "href": "chapters/chapter2/0-intro.html",
    "title": "Présentation formelle des méthodes ensemblistes",
    "section": "",
    "text": "Présentation formelle des méthodes ensemblistes\nPrincipe: cette partie propose une présentation formalisée des méthodes ensemblistes, à destination des personnes souhaitant comprendre en détail le fonctionnement des algorithmes.",
    "crumbs": [
      "Présentation formelle des algorithmes"
    ]
  },
  {
    "objectID": "chapters/chapter2/3-random_forest.html",
    "href": "chapters/chapter2/3-random_forest.html",
    "title": "La forêt aléatoire",
    "section": "",
    "text": "La forêt aléatoire (random forests) est une méthode ensembliste puissante, largement utilisée pour les tâches de classification et de régression. Elle combine la simplicité des arbres de décision avec la puissance de l’agrégation pour améliorer les performances prédictives et réduire le risque de surapprentissage (overfitting).\nLa méthode s’appuie sur la technique du bagging, qui consiste à entraîner chaque arbre sur un échantillon (bootstrap) tiré au hasard à partir du jeu de données initial. Elle introduit un degré supplémentaire de randomisation au moment de la construction d’un arbre, puisqu’à chaque nouvelle division (noeud), un sous-ensemble de variables sur lequel sera fondé le critère de séparation est sélectionné aléatoirement. Cette randomisation supplémentaire réduit la corrélation entre les arbres, ce qui permet de diminuer la variance des prédiction du modèle agrégé.\nObjectifs:\n\ncomprendre les propriétés fondamentales des forêts aléatoires afin de comprendre comment elles améliorent les performances des modèles\ncomprendre les étapes permettant de construire une forêt aléatoire : échantillonnage bootstrap, sélection de variables, partitions, prédiction, évaluation, interprétation\npermettre l’optimisation des performances du modèle: fournir une présentation formelle pour guider le choix des hyperparamètres, la préparation des données, etc.\n\n\n\nLes forêts aléatoires reposent sur plusieurs éléments essentiels :\n\nLes arbres CART: Les modèles élémentaires sont des arbres CART non élagués, c’est-à-dire autorisés à pousser jusqu’à l’atteinte d’un critère d’arrêt défini en amont.\nL’échantillonnage bootstrap: Chaque arbre est construit à partir d’un échantillon aléatoire tiré avec remise du jeu de données d’entraînement.\nLa sélection aléatoire de caractéristiques (variables) : Lors de la construction d’un arbre, à chaque nœud de celui-ci, un sous-ensemble aléatoire de variables est sélectionné. La meilleure division est ensuite choisie parmi ces caractéristiques aléatoires.\nL’agrégation des prédictions : Comme pour le bagging, les prédictions de tous les arbres sont combinées On procède généralement à la moyenne (ou à la médiane) des prédictions dans le cas de la régression, et au vote majoritaire (ou à la moyenne des probabilités prédites pour chaque classe) dans le cas de la classification.\n\n\n\n\nLes propriétés théoriques des forêts aléatoires permettent de comprendre pourquoi (et dans quelles situations) elles sont particulièrement robustes et performantes.\n\n\nL’agrégation de plusieurs arbres dans une forêt aléatoire permet de réduire la variance globale du modèle, ce qui améliore la stabilité et la précision des prédictions lorsque les biais initiaux sont “faibles”. La démonstration est présentée dans la section bagging.\n\n\n\nLes forêts aléatoires sont très performantes en pratique, mais la question de leur convergence vers une solution optimale lorsque la taille de l’échantillon tend vers l’infini reste ouverte (Loupes 2014). Plusieurs travaux théoriques ont toutefois fourni des preuves de consistance pour des versions simplifiées de l’algorithme (par exemple, Biau 2012).\nMême si la convergence vers le modèle optimal n’est pas encore formellement établie, il est possible de montrer, grâce à la Loi des Grands Nombres, que l’erreur de généralisation de l’ensemble diminue lorsque le nombre d’arbres augmente. Cela implique que la forêt aléatoire ne souffre pas d’un surapprentissage (également appelé overfitting) croissant avec le nombre d’arbres inclus dans le modèle, ce qui la rend particulièrement robuste.\n\n\n\nL’erreur de généralisation des forêts aléatoires est influencée par deux facteurs principaux :\n\nLa force (puissance prédictrice) des arbres individuels : La force d’un arbre dépend de sa capacité à faire des prédictions correctes (sans biais). Pour que l’ensemble soit performant, chaque arbre doit être suffisamment prédictif.\nLa corrélation entre les arbres : Une corrélation faible entre les arbres améliore les performances globales. En effet, des arbres fortement corrélés auront tendance à faire des erreurs similaires. La randomisation des caractéristiques à chaque nœud contribue à réduire cette corrélation, ce qui améliore la précision globale.\n\nDans le cas de la régression, où l’objectif est de minimiser l’erreur quadratique moyenne, la décomposition de la variance de l’ensemble (la forêt aléatoire) permet de faire apparaître le rôle de la corrélation entre les arbres:\n\\[\n\\text{Var}(\\hat{f}(x)) = \\rho(x) \\sigma(x)^2 + \\frac{1 - \\rho(x)}{M} \\sigma(x)^2\n\\]\nOù \\(\\rho(x)\\) est le coefficient de corrélation entre les arbres individuels, \\(\\sigma(x)^2\\) est la variance d’un arbre individuel, \\(M\\) est le nombre d’arbres dans la forêt.\nConséquences:\n\nSi \\(\\rho(x)\\) est faible : La variance est significativement réduite avec l’augmentation du nombre d’arbres \\(M\\).\nSi \\(\\rho(x)\\) est élevée : La réduction de variance est moindre, car les arbres sont plus corrélés entre eux.\n\nL’objectif des forêts aléatoires est donc de minimiser la corrélation entre les arbres tout en maximisant leur capacité à prédire correctement, ce qui permet de réduire la variance globale sans augmenter excessivement le biais.\n\n\n\n\n\nNombre d’arbres : plus le nombre d’arbes est élevé, plus la variance est réduite, jusqu’à un certain point de saturation. Souvent, quelques centaines d’arbres suffisent à stabiliser les performances des modèles.\nNombre de variables à sélectionner à chaque noeud: cet hyperparamètre permet de contrôler l’interdépendance entre les arbres. Avec \\(K\\) le nombre total de variables, une règle empirique usuelle est de considérer \\(\\sqrt{K}\\) pour une classification et \\(K/3\\) pour une régression.\nProfondeur des arbres : laisser les arbres se développer pleinement (sans élagage) pour profiter de la réduction de variance par agrégation.\n\n\n\n\n\n\nL’estimation Out-of-Bag (OOB) est une méthode particulièrement efficace pour évaluer les performances des forêts aléatoires sans nécessité une validation croisée ou de réserver une partie des données pour l’étape du test. Cette technique repose sur le fait que chaque arbre dans une forêt aléatoire est construit à partir d’un échantillon bootstrap du jeu de données d’origine, c’est-à-dire un échantillon tiré avec remise. Or, en moyenne, environ 36 % des observations ne sont pas inclus dans chaque échantillon bootstrap, ce qui signifie qu’elles ne sont pas utilisées pour entraîner l’arbre correspondant. Ces observations laissées de côté forment un échantillon out-of-bag. Chaque arbre peut donc être évalué sur son échantillon out-of-bag plutôt que sur un échantillon test.\nProcédure d’Estimation OOB:\n\nConstruction des arbres : Chaque arbre de la forêt est construit à partir d’un échantillon bootstrap tiré avec remise à partir du jeu de données d’origine. Cela signifie que certaines observations seront sélectionnées plusieurs fois, tandis que d’autres ne seront pas sélectionnées du tout.\nPrédiction OOB : Pour chaque observation \\((x_i, y_i)\\) qui n’a pas été inclus dans l’échantillon bootstrap qui a permi de construire un arbre donné, l’arbre est utilisé pour prédire la valeur de \\(y_i\\). Ainsi, chaque observation est prédite par tous les arbres pour lesquels elle fait partie de l’échantillon out-of-bag.\nAgrégation des prédictions : La prédiction finale pour chaque échantillon out-of-bag est obtenue en moyennant les prédictions de tous les arbres pour lesquels cet échantillon était OOB (pour la régression) ou par un vote majoritaire (pour la classification).\nCalcul de l’erreur OOB : L’erreur OOB est ensuite calculée en comparant les prédictions agrégées avec les valeurs réelles des observations \\(y_i\\). Cette erreur est une bonne approximation de l’erreur de généralisation du modèle.\n\nAvantages de l’Estimation OOB:\n\nPas besoin de jeu de validation séparé : L’un des principaux avantages de l’estimation OOB est qu’elle ne nécessite pas de réserver une partie des données pour la validation. Cela est particulièrement utile lorsque la taille du jeu de données est limitée, car toutes les données peuvent être utilisées pour l’entraînement tout en ayant une estimation fiable de la performance.\nEstimation directe et éfficace : Contrairement à la validation croisée qui peut être coûteuse en temps de calcul, l’estimation OOB est disponible “gratuitement” pendant la construction des arbres. Cela permet d’évaluer la performance du modèle sans avoir besoin de réentraîner plusieurs fois le modèle.\nApproximation de l’erreur de généralisation : L’erreur OOB est considérée comme une bonne approximation de l’erreur de généralisation, comparable à celle obtenue par une validation croisée 10-fold.\n\n\n\n\nLa validation croisée est une technique d’évaluation couramment utilisée en apprentissage automatique pour estimer la capacité d’un modèle à généraliser à de nouvelles données. Bien que l’estimation Out-of-Bag (OOB) soit généralement suffisante pour les forêts aléatoires, la validation croisée permet d’obtenir une évaluation plus robuste, notamment sur des jeux de données de petite taille.\nL’idée derrière la validation croisée est de maximiser l’utilisation des données disponibles en réutilisant chaque observation à la fois pour l’entraînement et pour le test. Cela permet d’obtenir une estimation de la performance du modèle qui est moins sensible aux fluctuations dues à la division des données en ensembles d’entraînement et de test. La validation croisée répète cette division plusieurs fois, puis moyenne les résultats pour obtenir une estimation plus fiable.\nProcédure de validation croisée:\nLa validation croisée la plus courante est la validation croisée en k sous-échantillons (k-fold cross-validation):\n\nDivision des données : Le jeu de données est divisé en k sous-échantillons égaux, appelés folds. Typiquement, k est choisi entre 5 et 10, mais il peut être ajusté en fonction de la taille des données.\nEntraînement et test : Le modèle est entraîné sur k - 1 sous-échantillons et testé sur le sous-échantillon restant. Cette opération est répétée k fois, chaque sous-échantillon jouant à tour de rôle le rôle de jeu de test.\nCalcul de la performance : Les k performances obtenues (par exemple, l’erreur quadratique moyenne pour une régression, ou l’accuracy (exactitude) pour une classification) sont moyennées pour obtenir une estimation finale de la performance du modèle.\n\nAvantages de la validation croisée:\n\nUtilisation optimale des données : En particulier lorsque les données sont limitées, la validation croisée maximise l’utilisation de l’ensemble des données en permettant à chaque échantillon de contribuer à la fois à l’entraînement et au test.\nRéduction de la variance : En utilisant plusieurs divisions des données, on obtient une estimation de la performance moins sensible aux particularités d’une seule division.\n\nBien que plus coûteuse en termes de calcul, la validation croisée est souvent préférée lorsque les données sont limitées ou lorsque l’on souhaite évaluer différents modèles ou hyperparamètres avec précision.\nLeave-One-Out Cross-Validation (LOOCV) : Il s’agit d’un cas particulier où le nombre de sous-échantillons est égal à la taille du jeu de données. En d’autres termes, chaque échantillon est utilisé une fois comme jeu de test, et tous les autres échantillons pour l’entraînement. LOOCV fournit une estimation très précise de la performance, mais est très coûteuse en temps de calcul, surtout pour de grands jeux de données.\n\n\n\nL’estimation Out-of-Bag (OOB) et la validation croisée sont deux méthodes clés pour optimiser les hyper-paramètres d’une forêt aléatoire. Les deux approches permettent de comparer différentes combinaisons d’hyper-paramètres et de sélectionner celles qui maximisent les performances prédictives, l’OOB étant souvent plus rapide et moins coûteuse, tandis que la validation croisée est plus fiable dans des situations où le surapprentissage est un risque important.\n\n\n\n\nObjectif: Identifier les variables qui contribuent le plus à la prédiction/influencent le plus la variable cible, extraire des caractéristiques pertinentes pour comprendre les mécanismes de prédiction sous-jacent, établir des règles de décision simplifiées etc.\nMéthodes ususelles (mais biaisées):\n\nRéduction moyenne de l’impureté (Mean Decrease in Impurity) : pour chaque variable, on calcule la moyenne des réductions d’impureté qu’elle a engendrées dans tous les nœuds de tous les arbres où elle est impliquée. Les variables présentant la réduction moyenne d’impureté la plus élevée sont considérées comme les prédicteurs les plus importants.\nPermutation importance (Mean Decrease Accuracy) : Pour chaque variable, les performances du modèle sont comparées avant et après la permutation de ses valeurs. La différence moyenne de performance correspond à la MDA. L’idée est que si l’on permute aléatoirement les valeurs d’une variable (cassant ainsi sa relation avec la cible), une variable importante entraînera une hausse significative de l’erreur de généralisation.\n\nIl est essentiel de noter que ces deux mesures peuvent présenter des biais importants. Elles sont notamment sensibles aux variables catégorielles avec de nombreuses modalités, qui peuvent apparaître artificiellement importantes. Elles sont également fortement biaisée en présence de variables explicatives corrélées ou d’interactions complexes avec la cible.\n\n\n\n\nVariables catégorielles : Encoder correctement (one-hot encoding, ordinal encoding).\nValeurs manquantes : Les forêts aléatoires peuvent gérer les données manquantes, mais une imputation préalable peut améliorer les performances.\nÉchelle des Variables : Pas nécessaire de normaliser, les arbres sont invariants aux transformations monotones.",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "La forêt aléatoire"
    ]
  },
  {
    "objectID": "chapters/chapter2/3-random_forest.html#principe-de-la-forêt-aléatoire",
    "href": "chapters/chapter2/3-random_forest.html#principe-de-la-forêt-aléatoire",
    "title": "La forêt aléatoire",
    "section": "",
    "text": "Les forêts aléatoires reposent sur plusieurs éléments essentiels :\n\nLes arbres CART: Les modèles élémentaires sont des arbres CART non élagués, c’est-à-dire autorisés à pousser jusqu’à l’atteinte d’un critère d’arrêt défini en amont.\nL’échantillonnage bootstrap: Chaque arbre est construit à partir d’un échantillon aléatoire tiré avec remise du jeu de données d’entraînement.\nLa sélection aléatoire de caractéristiques (variables) : Lors de la construction d’un arbre, à chaque nœud de celui-ci, un sous-ensemble aléatoire de variables est sélectionné. La meilleure division est ensuite choisie parmi ces caractéristiques aléatoires.\nL’agrégation des prédictions : Comme pour le bagging, les prédictions de tous les arbres sont combinées On procède généralement à la moyenne (ou à la médiane) des prédictions dans le cas de la régression, et au vote majoritaire (ou à la moyenne des probabilités prédites pour chaque classe) dans le cas de la classification.",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "La forêt aléatoire"
    ]
  },
  {
    "objectID": "chapters/chapter2/3-random_forest.html#pourquoi-et-dans-quelles-situations-la-random-forest-fonctionne-fondements-théoriques-des-forêts-aléatoires",
    "href": "chapters/chapter2/3-random_forest.html#pourquoi-et-dans-quelles-situations-la-random-forest-fonctionne-fondements-théoriques-des-forêts-aléatoires",
    "title": "La forêt aléatoire",
    "section": "",
    "text": "Les propriétés théoriques des forêts aléatoires permettent de comprendre pourquoi (et dans quelles situations) elles sont particulièrement robustes et performantes.\n\n\nL’agrégation de plusieurs arbres dans une forêt aléatoire permet de réduire la variance globale du modèle, ce qui améliore la stabilité et la précision des prédictions lorsque les biais initiaux sont “faibles”. La démonstration est présentée dans la section bagging.\n\n\n\nLes forêts aléatoires sont très performantes en pratique, mais la question de leur convergence vers une solution optimale lorsque la taille de l’échantillon tend vers l’infini reste ouverte (Loupes 2014). Plusieurs travaux théoriques ont toutefois fourni des preuves de consistance pour des versions simplifiées de l’algorithme (par exemple, Biau 2012).\nMême si la convergence vers le modèle optimal n’est pas encore formellement établie, il est possible de montrer, grâce à la Loi des Grands Nombres, que l’erreur de généralisation de l’ensemble diminue lorsque le nombre d’arbres augmente. Cela implique que la forêt aléatoire ne souffre pas d’un surapprentissage (également appelé overfitting) croissant avec le nombre d’arbres inclus dans le modèle, ce qui la rend particulièrement robuste.\n\n\n\nL’erreur de généralisation des forêts aléatoires est influencée par deux facteurs principaux :\n\nLa force (puissance prédictrice) des arbres individuels : La force d’un arbre dépend de sa capacité à faire des prédictions correctes (sans biais). Pour que l’ensemble soit performant, chaque arbre doit être suffisamment prédictif.\nLa corrélation entre les arbres : Une corrélation faible entre les arbres améliore les performances globales. En effet, des arbres fortement corrélés auront tendance à faire des erreurs similaires. La randomisation des caractéristiques à chaque nœud contribue à réduire cette corrélation, ce qui améliore la précision globale.\n\nDans le cas de la régression, où l’objectif est de minimiser l’erreur quadratique moyenne, la décomposition de la variance de l’ensemble (la forêt aléatoire) permet de faire apparaître le rôle de la corrélation entre les arbres:\n\\[\n\\text{Var}(\\hat{f}(x)) = \\rho(x) \\sigma(x)^2 + \\frac{1 - \\rho(x)}{M} \\sigma(x)^2\n\\]\nOù \\(\\rho(x)\\) est le coefficient de corrélation entre les arbres individuels, \\(\\sigma(x)^2\\) est la variance d’un arbre individuel, \\(M\\) est le nombre d’arbres dans la forêt.\nConséquences:\n\nSi \\(\\rho(x)\\) est faible : La variance est significativement réduite avec l’augmentation du nombre d’arbres \\(M\\).\nSi \\(\\rho(x)\\) est élevée : La réduction de variance est moindre, car les arbres sont plus corrélés entre eux.\n\nL’objectif des forêts aléatoires est donc de minimiser la corrélation entre les arbres tout en maximisant leur capacité à prédire correctement, ce qui permet de réduire la variance globale sans augmenter excessivement le biais.",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "La forêt aléatoire"
    ]
  },
  {
    "objectID": "chapters/chapter2/3-random_forest.html#les-hyper-paramètres-clés",
    "href": "chapters/chapter2/3-random_forest.html#les-hyper-paramètres-clés",
    "title": "La forêt aléatoire",
    "section": "",
    "text": "Nombre d’arbres : plus le nombre d’arbes est élevé, plus la variance est réduite, jusqu’à un certain point de saturation. Souvent, quelques centaines d’arbres suffisent à stabiliser les performances des modèles.\nNombre de variables à sélectionner à chaque noeud: cet hyperparamètre permet de contrôler l’interdépendance entre les arbres. Avec \\(K\\) le nombre total de variables, une règle empirique usuelle est de considérer \\(\\sqrt{K}\\) pour une classification et \\(K/3\\) pour une régression.\nProfondeur des arbres : laisser les arbres se développer pleinement (sans élagage) pour profiter de la réduction de variance par agrégation.",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "La forêt aléatoire"
    ]
  },
  {
    "objectID": "chapters/chapter2/3-random_forest.html#evaluation-des-performances-du-modèle-et-choix-des-hyper-paramètres",
    "href": "chapters/chapter2/3-random_forest.html#evaluation-des-performances-du-modèle-et-choix-des-hyper-paramètres",
    "title": "La forêt aléatoire",
    "section": "",
    "text": "L’estimation Out-of-Bag (OOB) est une méthode particulièrement efficace pour évaluer les performances des forêts aléatoires sans nécessité une validation croisée ou de réserver une partie des données pour l’étape du test. Cette technique repose sur le fait que chaque arbre dans une forêt aléatoire est construit à partir d’un échantillon bootstrap du jeu de données d’origine, c’est-à-dire un échantillon tiré avec remise. Or, en moyenne, environ 36 % des observations ne sont pas inclus dans chaque échantillon bootstrap, ce qui signifie qu’elles ne sont pas utilisées pour entraîner l’arbre correspondant. Ces observations laissées de côté forment un échantillon out-of-bag. Chaque arbre peut donc être évalué sur son échantillon out-of-bag plutôt que sur un échantillon test.\nProcédure d’Estimation OOB:\n\nConstruction des arbres : Chaque arbre de la forêt est construit à partir d’un échantillon bootstrap tiré avec remise à partir du jeu de données d’origine. Cela signifie que certaines observations seront sélectionnées plusieurs fois, tandis que d’autres ne seront pas sélectionnées du tout.\nPrédiction OOB : Pour chaque observation \\((x_i, y_i)\\) qui n’a pas été inclus dans l’échantillon bootstrap qui a permi de construire un arbre donné, l’arbre est utilisé pour prédire la valeur de \\(y_i\\). Ainsi, chaque observation est prédite par tous les arbres pour lesquels elle fait partie de l’échantillon out-of-bag.\nAgrégation des prédictions : La prédiction finale pour chaque échantillon out-of-bag est obtenue en moyennant les prédictions de tous les arbres pour lesquels cet échantillon était OOB (pour la régression) ou par un vote majoritaire (pour la classification).\nCalcul de l’erreur OOB : L’erreur OOB est ensuite calculée en comparant les prédictions agrégées avec les valeurs réelles des observations \\(y_i\\). Cette erreur est une bonne approximation de l’erreur de généralisation du modèle.\n\nAvantages de l’Estimation OOB:\n\nPas besoin de jeu de validation séparé : L’un des principaux avantages de l’estimation OOB est qu’elle ne nécessite pas de réserver une partie des données pour la validation. Cela est particulièrement utile lorsque la taille du jeu de données est limitée, car toutes les données peuvent être utilisées pour l’entraînement tout en ayant une estimation fiable de la performance.\nEstimation directe et éfficace : Contrairement à la validation croisée qui peut être coûteuse en temps de calcul, l’estimation OOB est disponible “gratuitement” pendant la construction des arbres. Cela permet d’évaluer la performance du modèle sans avoir besoin de réentraîner plusieurs fois le modèle.\nApproximation de l’erreur de généralisation : L’erreur OOB est considérée comme une bonne approximation de l’erreur de généralisation, comparable à celle obtenue par une validation croisée 10-fold.\n\n\n\n\nLa validation croisée est une technique d’évaluation couramment utilisée en apprentissage automatique pour estimer la capacité d’un modèle à généraliser à de nouvelles données. Bien que l’estimation Out-of-Bag (OOB) soit généralement suffisante pour les forêts aléatoires, la validation croisée permet d’obtenir une évaluation plus robuste, notamment sur des jeux de données de petite taille.\nL’idée derrière la validation croisée est de maximiser l’utilisation des données disponibles en réutilisant chaque observation à la fois pour l’entraînement et pour le test. Cela permet d’obtenir une estimation de la performance du modèle qui est moins sensible aux fluctuations dues à la division des données en ensembles d’entraînement et de test. La validation croisée répète cette division plusieurs fois, puis moyenne les résultats pour obtenir une estimation plus fiable.\nProcédure de validation croisée:\nLa validation croisée la plus courante est la validation croisée en k sous-échantillons (k-fold cross-validation):\n\nDivision des données : Le jeu de données est divisé en k sous-échantillons égaux, appelés folds. Typiquement, k est choisi entre 5 et 10, mais il peut être ajusté en fonction de la taille des données.\nEntraînement et test : Le modèle est entraîné sur k - 1 sous-échantillons et testé sur le sous-échantillon restant. Cette opération est répétée k fois, chaque sous-échantillon jouant à tour de rôle le rôle de jeu de test.\nCalcul de la performance : Les k performances obtenues (par exemple, l’erreur quadratique moyenne pour une régression, ou l’accuracy (exactitude) pour une classification) sont moyennées pour obtenir une estimation finale de la performance du modèle.\n\nAvantages de la validation croisée:\n\nUtilisation optimale des données : En particulier lorsque les données sont limitées, la validation croisée maximise l’utilisation de l’ensemble des données en permettant à chaque échantillon de contribuer à la fois à l’entraînement et au test.\nRéduction de la variance : En utilisant plusieurs divisions des données, on obtient une estimation de la performance moins sensible aux particularités d’une seule division.\n\nBien que plus coûteuse en termes de calcul, la validation croisée est souvent préférée lorsque les données sont limitées ou lorsque l’on souhaite évaluer différents modèles ou hyperparamètres avec précision.\nLeave-One-Out Cross-Validation (LOOCV) : Il s’agit d’un cas particulier où le nombre de sous-échantillons est égal à la taille du jeu de données. En d’autres termes, chaque échantillon est utilisé une fois comme jeu de test, et tous les autres échantillons pour l’entraînement. LOOCV fournit une estimation très précise de la performance, mais est très coûteuse en temps de calcul, surtout pour de grands jeux de données.\n\n\n\nL’estimation Out-of-Bag (OOB) et la validation croisée sont deux méthodes clés pour optimiser les hyper-paramètres d’une forêt aléatoire. Les deux approches permettent de comparer différentes combinaisons d’hyper-paramètres et de sélectionner celles qui maximisent les performances prédictives, l’OOB étant souvent plus rapide et moins coûteuse, tandis que la validation croisée est plus fiable dans des situations où le surapprentissage est un risque important.",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "La forêt aléatoire"
    ]
  },
  {
    "objectID": "chapters/chapter2/3-random_forest.html#interprétation-et-importance-des-variables",
    "href": "chapters/chapter2/3-random_forest.html#interprétation-et-importance-des-variables",
    "title": "La forêt aléatoire",
    "section": "",
    "text": "Objectif: Identifier les variables qui contribuent le plus à la prédiction/influencent le plus la variable cible, extraire des caractéristiques pertinentes pour comprendre les mécanismes de prédiction sous-jacent, établir des règles de décision simplifiées etc.\nMéthodes ususelles (mais biaisées):\n\nRéduction moyenne de l’impureté (Mean Decrease in Impurity) : pour chaque variable, on calcule la moyenne des réductions d’impureté qu’elle a engendrées dans tous les nœuds de tous les arbres où elle est impliquée. Les variables présentant la réduction moyenne d’impureté la plus élevée sont considérées comme les prédicteurs les plus importants.\nPermutation importance (Mean Decrease Accuracy) : Pour chaque variable, les performances du modèle sont comparées avant et après la permutation de ses valeurs. La différence moyenne de performance correspond à la MDA. L’idée est que si l’on permute aléatoirement les valeurs d’une variable (cassant ainsi sa relation avec la cible), une variable importante entraînera une hausse significative de l’erreur de généralisation.\n\nIl est essentiel de noter que ces deux mesures peuvent présenter des biais importants. Elles sont notamment sensibles aux variables catégorielles avec de nombreuses modalités, qui peuvent apparaître artificiellement importantes. Elles sont également fortement biaisée en présence de variables explicatives corrélées ou d’interactions complexes avec la cible.",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "La forêt aléatoire"
    ]
  },
  {
    "objectID": "chapters/chapter2/3-random_forest.html#préparation-des-données-feature-engineering",
    "href": "chapters/chapter2/3-random_forest.html#préparation-des-données-feature-engineering",
    "title": "La forêt aléatoire",
    "section": "",
    "text": "Variables catégorielles : Encoder correctement (one-hot encoding, ordinal encoding).\nValeurs manquantes : Les forêts aléatoires peuvent gérer les données manquantes, mais une imputation préalable peut améliorer les performances.\nÉchelle des Variables : Pas nécessaire de normaliser, les arbres sont invariants aux transformations monotones.",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "La forêt aléatoire"
    ]
  },
  {
    "objectID": "chapters/chapter2/1-CART.html",
    "href": "chapters/chapter2/1-CART.html",
    "title": "La brique élémentaire: l’arbre de décision",
    "section": "",
    "text": "Les arbres de décision sont des outils puissants en apprentissage automatique, utilisés pour des tâches de classification et de régression. Ces algorithmes non paramétriques consistent à diviser l’espace des caractéristiques en sous-ensembles homogènes à l’aide de règles simples, afin de faire des prédictions. Malgré leur simplicité apparente, les arbres de décision sont capable de saisir des relations complexes et non linéaires entre les variables (ou caractéristiques) d’un jeu de données.\n\n\nImaginez que vous souhaitiez prédire le prix d’une maison en fonction de sa superficie et de son nombre de pièces. L’espace des caractéristiques (superficie et nombre de pièces) est vaste, et les prix des maisons (la réponse à prédire) sont très variables. Pour prédire le prix des maisons, l’idée est de diviser cet espace en zones plus petites, où les maisons ont des prix similaires, et d’attribuer une prédiction identique à toutes les maisons situées dans la même zone.\n\n\nL’objectif principal est de trouver la partition de l’espace des caractéristiques qui offre les meilleures prédictions possibles. Cependant, cet objectif se heurte à plusieurs difficultés, et la complexité du problème augmente rapidement avec le nombre de caractéristiques et la taille de l’échantillon:\n\nInfinité des découpages possibles : Il existe une infinité de façons de diviser l’espace des caractéristiques.\nComplexité de la paramétrisation : Il est difficile de représenter tous ces découpages avec un nombre limité de paramètres.\nOptimisation complexe : Même avec une paramétrisation, trouver le meilleur découpage nécessite une optimisation complexe, souvent irréaliste en pratique.\n\n\n\n\nPour surmonter ces difficultés, les méthodes d’arbres de décision, et notamment la plus célèbre, l’algorithme CART (Classication And Regression Tree, Breiman et al. (1984)), adoptent deux approches clés :\n\nSimplification du partitionnement de l’espace\n\nAu lieu d’explorer tous les découpages possibles, les arbres de décision partitionnent l’espace des caractéristiques en plusieurs régions distinctes (non chevauchantes) en appliquant des règles de décision simples. Les règles suivantes sont communément adoptées:\n\nDécoupages binaires simples : À chaque étape, l’algorithme divise une région de l’espace en deux sous-régions en se basant sur une seule caractéristique (ou variable) et en définissant un seul seuil (ou critère) pour cette segmentation. Concrètement, cela revient à poser une question du type : “La valeur de la caractéristique X dépasse-t-elle un certain seuil ?” Par exemple : “La superficie de la maison est-elle supérieure à 100 m² ?”. Les deux réponses possibles (“Oui” ou “Non”) génèrent deux nouvelles sous-régions distinctes de l’espace, chacune correspondant à un sous-ensemble de données plus homogène.\nPrédictions locales : Lorsque l’algorithme s’arrête, une prédiction simple est faite dans chaque région. Il s’agit souvent de la moyenne des valeurs cibles dans cette région (régression) ou de la classe majoritaire (classification).\n\nCes règles de découpage rendent le problème d’optimisation plus simple mais également plus interprétable.\n\nOptimisation gloutonne (greedy)\n\nPlutôt que d’optimiser toutes les divisions simultanément, les arbres de décision utilisent une approche simplifiée, récursive et séquentielle :\n\nDivision étape par étape : À chaque étape, l’arbre choisit la meilleure division possible sur la base d’un critère de réduction de l’hétérogénéité intra-région. En revanche, il ne prend pas en compte les étapes d’optimisation futures.\nCritère local : La décision est basée sur la réduction immédiate de l’impureté ou de l’erreur de prédiction (par exemple, la réduction de la variance pour la régression). Ce processus est répété pour chaque sous-région, ce qui permet d’affiner progressivement la partition de l’espace en fonction des caractéristiques les plus discriminantes.\n\nCette méthode dite “gloutonne” (greedy) s’avère efficace pour construire un partitionnement de l’espace des caractéristiques, car elle décompose un problème d’optimisation complexe en une succession de problèmes plus simples et plus rapides à résoudre. Le résultat obtenu n’est pas nécessairement un optimum global, mais il s’en approche raisonnablement et surtout rapidement.\nLe terme “arbre de décision” provient de la structure descendante en forme d’arbre inversé qui émerge lorsqu’on utilise un algorithme glouton pour découper l’espace des caractéristiques en sous-ensemble de réponses homogènes de manière récursive. A chaque étape, deux nouvelles branches sont créées et forment une nouvelle partition de l’espace des caractéristiques.\nUne fois entraîné, un arbre de décision est une fonction constante par morceaux défini sur l’espace des caractéristiques. En raison de leur nature non-continue et non-différentiable, il est impossible d’utiliser des méthodes d’optimisation classiques reposant sur le calcul de gradients.\n\n\n\nNous présentons la structure d’un arbre de décision et les principaux éléments qui le composent.\n\nNœud Racine (Root Node) : Le nœud racine est le point de départ de l’arbre de décision, il est situé au sommet de l’arbre. Il contient l’ensemble des données d’entraînement avant toute division. À ce niveau, l’algorithme cherche la caractéristique la plus discriminante, c’est-à-dire celle qui permet de diviser les données de manière à optimiser une fonction de perte (comme l’indice de Gini pour la classification ou la variance pour la régression).\nNœuds Internes (Internal Nodes) : Les nœuds internes sont les points intermédiaires où l’algorithme CART applique des règles de décision pour diviser les données en sous-ensembles plus petits. Chaque nœud interne représente une question ou condition basée sur une caractéristique particulière (par exemple, “La superficie de la maison est-elle supérieure à 100 m² ?”). À chaque étape, une seule caractéristique (la superficie) et un seul seuil (supérieur à 100) sont utilisés pour faire la division.\nBranches: Les branches sont les connexions entre les nœuds, elles illustrent le chemin que les données suivent en fonction des réponses aux questions posées dans les nœuds internes. Chaque branche correspond à une décision binaire, “Oui” ou “Non”, qui oriente les observations vers une nouvelle subdivision de l’espace des caractéristiques.\nNœuds Terminaux ou Feuilles (Leaf Nodes ou Terminal Nodes) : Les nœuds terminaux, situés à l’extrémité des branches, sont les points où le processus de division s’arrête. Ils fournissent la prédiction finale.\n\nEn classification, chaque feuille correspond à une classe prédite (par exemple, “Oui” ou “Non”).\nEn régression, chaque feuille fournit une valeur numérique prédite (comme le prix estimé d’une maison).\n\n\nFigure illustrative : Une représentation visuelle de la structure de l’arbre peut être utile ici pour illustrer les concepts de nœuds, branches et feuilles.\n\n\n\nSupposons que nous souhaitions prédire le prix d’une maison en fonction de sa superficie et de son nombre de pièces. Un arbre de décision pourrait procéder ainsi :\n\nPremière division : “La superficie de la maison est-elle supérieure à 100 m² ?”\n\nOui : Aller à la branche de gauche.\nNon : Aller à la branche de droite.\n\nDeuxième division (branche de gauche) : “Le nombre de pièces est-il supérieur à 4 ?”\n\nOui : Prix élevé (par exemple, plus de 300 000 €).\nNon : Prix moyen (par exemple, entre 200 000 € et 300 000 €).\n\nDeuxième division (branche de droite) : “Le nombre de pièces est-il supérieur à 2 ?”\n\nOui : Prix moyen (par exemple, entre 150 000 € et 200 000 €).\nNon : Prix bas (par exemple, moins de 150 000 €).\n\n\nCet arbre utilise des règles simples pour diviser l’espace des caractéristiques (superficie et nombre de pièces) en sous-groupes homogènes et fournir une prédiction (estimer le prix d’une maison).\nFigure illustrative\n\n\n\n\nL’algorithme CART (Classification and Regression Trees) proposé par Breiman et al. (1984) est une méthode utilisée pour construire des arbres de décision, que ce soit pour des tâches de classification ou de régression. L’algorithme CART fonctionne en partitionnant l’espace des caractéristiques en sous-ensembles de manière récursive, en suivant une logique de décisions binaires à chaque étape. Ce processus est itératif et suit plusieurs étapes clés.\n\n\nLa fonction d’impureté est une mesure locale utilisée dans la construction des arbres de décision pour évaluer la qualité des divisions à chaque nœud. Elle quantifie le degré d’hétérogénéité des observations dans un nœud par rapport à la variable cible (classe pour la classification, ou valeur continue pour la régression). Plus précisément, une mesure d’impureté est conçue pour croître avec la dispersion dans un nœud. Un nœud est dit pur lorsque toutes les observations qu’il contient appartiennent à la même classe (classification) ou présentent des valeurs similaires/identiques (régression).\nL’algorithme CART utilise ce type de mesure pour choisir les divisions qui créent des sous-ensembles plus homogènes que le nœud parent. À chaque étape de construction, l’algorithme sélectionne la division qui réduit le plus l’impureté, afin de garantir des nœuds de plus en plus homogènes au fur et à mesure que l’arbre se développe.\nLe choix de la fonction d’impureté dépend du type de problème :\n\nClassification : L’indice de Gini ou l’entropie sont très souvent utilisées pour évaluer la dispersion des classes dans chaque nœud.\nRégression : La somme des erreurs quadratiques (SSE) est souvent utilisée pour mesurer la variance des valeurs cibles dans chaque nœud.\n\n\n\nDans le cadre de la classification, l’objectif est de partitionner les données de manière à ce que chaque sous-ensemble (ou région) soit le plus homogène possible en termes de classe prédite. Plusieurs mesures d’impureté sont couramment utilisées pour évaluer la qualité des divisions.\nPropriété-définition d’une mesure d’impureté\nPour un nœud \\(t\\) contenant \\(K\\) classes, une mesure d’impureté \\(I(t)\\) est une fonction qui quantifie l’hétérogénéité des classes dans ce nœud. Elle doit satisfaire les propriétés suivantes :\n\nPureté maximale : Lorsque toutes les observations du nœud appartiennent à une seule classe, c’est-à-dire que la proportion \\(p_k = 1\\) pour une classe \\(k\\) et \\(p_j = 0\\) pour toutes les autres classes \\(j \\neq k\\), l’impureté est minimale et \\(I(t) = 0\\). Cela indique que le nœud est entièrement pur, ou homogène.\nImpureté maximale : Lorsque les observations sont réparties de manière uniforme entre toutes les classes, c’est-à-dire que \\(p_k = \\frac{1}{K}\\) pour chaque classe \\(k\\), l’impureté atteint son maximum. Cette situation reflète une impureté élevée, car le nœud est très hétérogène et contient une forte incertitude sur la classe des observations.\n\n1. L’indice de Gini\nL’indice de Gini est l’une des fonctions de perte les plus couramment utilisées pour la classification. Il mesure la probabilité qu’un individu sélectionné au hasard dans un nœud soit mal classé si on lui attribue une classe au hasard, en fonction de la distribution des classes dans ce nœud.\nPour un nœud \\(t\\) contenant \\(K\\) classes, l’indice de Gini \\(G(t)\\) est donné par :\n\\[\nG(t) = 1 - \\sum_{k=1}^{K} p_k^2\n\\]\noù \\(p_k\\) est la proportion d’observations appartenant à la classe \\(k\\) dans le nœud \\(t\\).\n\nCritère de choix : L’indice de Gini est souvent utilisé parce qu’il est simple à calculer et capture bien l’homogénéité des classes au sein d’un nœud. Il privilégie les partitions où une classe domine fortement dans chaque sous-ensemble.\n2. L’entropie (ou entropie de Shannon)\nL’entropie est une autre mesure de l’impureté utilisée dans les arbres de décision. Elle mesure la quantité d’incertitude ou de désordre dans un nœud, en s’appuyant sur la théorie de l’information.\nPour un nœud \\(t\\) contenant \\(K\\) classes, l’entropie \\(E(t)\\) est définie par :\n\\[\nE(t) = - \\sum_{k=1}^{K} p_k \\log(p_k)\n\\]\noù \\(p_k\\) est la proportion d’observations de la classe \\(k\\) dans le nœud \\(t\\).\n\nCritère de choix : L’entropie a tendance à être plus sensible aux changements dans les distributions des classes que l’indice de Gini, car elle attribut un poids plus élevé aux événements rares (valeurs de \\(p_k\\) très faibles). Elle est souvent utilisée lorsque l’erreur de classification des classes minoritaires est particulièrement importante.\n3. Taux d’erreur\nLe taux d’erreur est une autre mesure de l’impureté parfois utilisée dans les arbres de décision. Il représente la proportion d’observations mal classées dans un nœud.\nPour un nœud \\(t\\), le taux d’erreur \\(\\text{TE}(t)\\) est donné par :\n\\[\n\\text{TE}(t) = 1 - \\max(p_k)\n\\]\noù \\(\\max(p_k)\\) est la proportion d’observations appartenant à la classe majoritaire dans le nœud.\n\nCritère de choix : Bien que le taux d’erreur soit simple à comprendre, il est moins souvent utilisé dans la construction des arbres de décision parce qu’il est moins sensible que l’indice de Gini ou l’entropie aux petits changements dans la distribution des classes.\n\n\n\nDans les problèmes de régression, l’objectif est de partitionner les données de manière à réduire au maximum la variabilité des valeurs au sein de chaque sous-ensemble. Pour mesurer cette variabilité, la somme des erreurs quadratiques (SSE) est la fonction d’impureté la plus couramment employée. Elle évalue l’impureté d’une région en quantifiant à quel point les valeurs de cette région s’écartent de la moyenne locale.\n1.Somme des erreurs quadratiques (SSE) ou variance\nLa somme des erreurs quadratiques (ou SSE, pour Sum of Squared Errors) est une mesure qui quantifie la dispersion des valeurs dans un nœud par rapport à la moyenne des valeurs dans ce nœud.\nFormule : Pour un nœud \\(t\\), contenant \\(N\\) observations avec des valeurs \\(y_i\\), la SSE est donnée par :\n\\[\n\\text{SSE}(t) = \\sum_{i=1}^{N} (y_i - \\hat{y})^2\n\\]\noù \\(\\hat{y}\\) est la moyenne des valeurs \\(y_i\\) dans le nœud \\(t\\).\nPropriété :\n\nSi toutes les valeurs de \\(y_i\\) dans un nœud sont proches de la moyenne \\(\\hat{y}\\), la SSE sera faible, indiquant une homogénéité élevée dans le nœud.\nEn revanche, une SSE élevée indique une grande variabilité dans les valeurs, donc un nœud impur.\n\nCritère de choix : La somme des erreurs quadratiques (SSE) est particulièrement sensible aux écarts élevés entre les valeurs observées et la moyenne prédite. En cherchant à minimiser la SSE, les modèles visent à former des nœuds dans lesquels les valeurs des observations sont aussi proches que possible de la moyenne locale.\n\n\n\n\nUne fois la mesure d’impureté définie, l’algorithme CART examine toutes les divisions binaires possibles de l’espace des caractéristiques. À chaque nœud, et pour chaque caractéristique, il cherche à identifier le seuil optimal, c’est-à-dire le seuil qui minimise le plus efficacement l’impureté des deux sous-ensembles générés. L’algorithme compare ensuite toutes les divisions potentielles (caractéristiques et seuils optimaux associés à chaque nœud) et sélectionne celle qui entraîne la réduction maximale de l’impureté.\nPrenons l’exemple d’une caractéristique continue, telle que la superficie d’une maison :\n\nSi l’algorithme teste la règle “Superficie &gt; 100 m²”, il calcule la fonction de perte pour les deux sous-ensembles générés par cette règle (“Oui” et “Non”).\nCe processus est répété pour différentes valeurs seuils afin de trouver la partition qui minimise le plus efficacement l’impureté au sein des sous-ensembles.\n\n\n\n\nL’algorithme CART poursuit le partitionnement de l’espace des caractéristiques en appliquant de manière récursive les mêmes étapes : identification de la caractéristique et du seuil optimal pour chaque nœud, puis sélection du partitionnement binaire qui maximise la réduction de l’impureté. Ce processus est répété jusqu’à ce qu’un critère d’arrêt soit atteint, par exemple :\n\nProfondeur maximale de l’arbre : Limiter le nombre de divisions successives pour éviter un arbre trop complexe.\nNombre minimum d’observations par feuille : Empêcher la création de feuilles contenant très peu d’observations, ce qui réduirait la capacité du modèle à généraliser.\nRéduction minimale de l’impureté à chaque étape\n\n\n\n\n\n\n\nUne fois l’arbre construit, la prédiction pour une nouvelle observation s’effectue en suivant les branches de l’arbre, en partant du nœud racine jusqu’à un nœud terminal (ou feuille). À chaque nœud interne, une décision est prise en fonction des valeurs des caractéristiques de l’observation, ce qui détermine la direction à suivre vers l’un des sous-ensembles. Ce cheminement se poursuit jusqu’à ce que l’observation atteigne une feuille, où la prédiction finale est effectuée.\n\nEn classification, la classe attribuée est celle majoritaire dans la feuille atteinte.\nEn régression, la valeur prédite est généralement la moyenne des valeurs cibles des observations dans la feuille.\n\n\n\n\nPour améliorer la performance de l’arbre, on peut ajuster les hyperparamètres tels que la profondeur maximale ou le nombre minimum d’observations dans une feuille. De plus, des techniques comme la prédiction avec arbres multiples (bagging, forêts aléatoires) permettent de surmonter les limites des arbres individuels, souvent sujets au surapprentissage.\n\n\n\n\n\n\n\nInterprétabilité : Les arbres de décision sont faciles à comprendre et à visualiser.\nSimplicité : Pas besoin de transformations complexes des données.\nFlexibilité : Ils peuvent gérer des caractéristiques numériques et catégorielles, ainsi que les valeurs manquantes.\nGestion des interactions : Modèles non paramétriques, pas d’hypothèses sur les lois par les variables. Ils capturent naturellement les interactions entre les caractéristiques.\n\n\n\n\n\nSurapprentissage : Les arbres trop profonds peuvent surapprendre les données d’entraînement.\nOptimisation locale : L’approche gloutonne peut conduire à des solutions sous-optimales globalement (optimum local).\nStabilité : De petits changements dans les données peuvent entraîner des changements significatifs dans la structure de l’arbre (manque de robustesse).",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "La brique élémentaire: l'arbre de décision"
    ]
  },
  {
    "objectID": "chapters/chapter2/1-CART.html#le-principe-fondamental-partitionner-pour-prédire",
    "href": "chapters/chapter2/1-CART.html#le-principe-fondamental-partitionner-pour-prédire",
    "title": "La brique élémentaire: l’arbre de décision",
    "section": "",
    "text": "Imaginez que vous souhaitiez prédire le prix d’une maison en fonction de sa superficie et de son nombre de pièces. L’espace des caractéristiques (superficie et nombre de pièces) est vaste, et les prix des maisons (la réponse à prédire) sont très variables. Pour prédire le prix des maisons, l’idée est de diviser cet espace en zones plus petites, où les maisons ont des prix similaires, et d’attribuer une prédiction identique à toutes les maisons situées dans la même zone.\n\n\nL’objectif principal est de trouver la partition de l’espace des caractéristiques qui offre les meilleures prédictions possibles. Cependant, cet objectif se heurte à plusieurs difficultés, et la complexité du problème augmente rapidement avec le nombre de caractéristiques et la taille de l’échantillon:\n\nInfinité des découpages possibles : Il existe une infinité de façons de diviser l’espace des caractéristiques.\nComplexité de la paramétrisation : Il est difficile de représenter tous ces découpages avec un nombre limité de paramètres.\nOptimisation complexe : Même avec une paramétrisation, trouver le meilleur découpage nécessite une optimisation complexe, souvent irréaliste en pratique.\n\n\n\n\nPour surmonter ces difficultés, les méthodes d’arbres de décision, et notamment la plus célèbre, l’algorithme CART (Classication And Regression Tree, Breiman et al. (1984)), adoptent deux approches clés :\n\nSimplification du partitionnement de l’espace\n\nAu lieu d’explorer tous les découpages possibles, les arbres de décision partitionnent l’espace des caractéristiques en plusieurs régions distinctes (non chevauchantes) en appliquant des règles de décision simples. Les règles suivantes sont communément adoptées:\n\nDécoupages binaires simples : À chaque étape, l’algorithme divise une région de l’espace en deux sous-régions en se basant sur une seule caractéristique (ou variable) et en définissant un seul seuil (ou critère) pour cette segmentation. Concrètement, cela revient à poser une question du type : “La valeur de la caractéristique X dépasse-t-elle un certain seuil ?” Par exemple : “La superficie de la maison est-elle supérieure à 100 m² ?”. Les deux réponses possibles (“Oui” ou “Non”) génèrent deux nouvelles sous-régions distinctes de l’espace, chacune correspondant à un sous-ensemble de données plus homogène.\nPrédictions locales : Lorsque l’algorithme s’arrête, une prédiction simple est faite dans chaque région. Il s’agit souvent de la moyenne des valeurs cibles dans cette région (régression) ou de la classe majoritaire (classification).\n\nCes règles de découpage rendent le problème d’optimisation plus simple mais également plus interprétable.\n\nOptimisation gloutonne (greedy)\n\nPlutôt que d’optimiser toutes les divisions simultanément, les arbres de décision utilisent une approche simplifiée, récursive et séquentielle :\n\nDivision étape par étape : À chaque étape, l’arbre choisit la meilleure division possible sur la base d’un critère de réduction de l’hétérogénéité intra-région. En revanche, il ne prend pas en compte les étapes d’optimisation futures.\nCritère local : La décision est basée sur la réduction immédiate de l’impureté ou de l’erreur de prédiction (par exemple, la réduction de la variance pour la régression). Ce processus est répété pour chaque sous-région, ce qui permet d’affiner progressivement la partition de l’espace en fonction des caractéristiques les plus discriminantes.\n\nCette méthode dite “gloutonne” (greedy) s’avère efficace pour construire un partitionnement de l’espace des caractéristiques, car elle décompose un problème d’optimisation complexe en une succession de problèmes plus simples et plus rapides à résoudre. Le résultat obtenu n’est pas nécessairement un optimum global, mais il s’en approche raisonnablement et surtout rapidement.\nLe terme “arbre de décision” provient de la structure descendante en forme d’arbre inversé qui émerge lorsqu’on utilise un algorithme glouton pour découper l’espace des caractéristiques en sous-ensemble de réponses homogènes de manière récursive. A chaque étape, deux nouvelles branches sont créées et forment une nouvelle partition de l’espace des caractéristiques.\nUne fois entraîné, un arbre de décision est une fonction constante par morceaux défini sur l’espace des caractéristiques. En raison de leur nature non-continue et non-différentiable, il est impossible d’utiliser des méthodes d’optimisation classiques reposant sur le calcul de gradients.\n\n\n\nNous présentons la structure d’un arbre de décision et les principaux éléments qui le composent.\n\nNœud Racine (Root Node) : Le nœud racine est le point de départ de l’arbre de décision, il est situé au sommet de l’arbre. Il contient l’ensemble des données d’entraînement avant toute division. À ce niveau, l’algorithme cherche la caractéristique la plus discriminante, c’est-à-dire celle qui permet de diviser les données de manière à optimiser une fonction de perte (comme l’indice de Gini pour la classification ou la variance pour la régression).\nNœuds Internes (Internal Nodes) : Les nœuds internes sont les points intermédiaires où l’algorithme CART applique des règles de décision pour diviser les données en sous-ensembles plus petits. Chaque nœud interne représente une question ou condition basée sur une caractéristique particulière (par exemple, “La superficie de la maison est-elle supérieure à 100 m² ?”). À chaque étape, une seule caractéristique (la superficie) et un seul seuil (supérieur à 100) sont utilisés pour faire la division.\nBranches: Les branches sont les connexions entre les nœuds, elles illustrent le chemin que les données suivent en fonction des réponses aux questions posées dans les nœuds internes. Chaque branche correspond à une décision binaire, “Oui” ou “Non”, qui oriente les observations vers une nouvelle subdivision de l’espace des caractéristiques.\nNœuds Terminaux ou Feuilles (Leaf Nodes ou Terminal Nodes) : Les nœuds terminaux, situés à l’extrémité des branches, sont les points où le processus de division s’arrête. Ils fournissent la prédiction finale.\n\nEn classification, chaque feuille correspond à une classe prédite (par exemple, “Oui” ou “Non”).\nEn régression, chaque feuille fournit une valeur numérique prédite (comme le prix estimé d’une maison).\n\n\nFigure illustrative : Une représentation visuelle de la structure de l’arbre peut être utile ici pour illustrer les concepts de nœuds, branches et feuilles.\n\n\n\nSupposons que nous souhaitions prédire le prix d’une maison en fonction de sa superficie et de son nombre de pièces. Un arbre de décision pourrait procéder ainsi :\n\nPremière division : “La superficie de la maison est-elle supérieure à 100 m² ?”\n\nOui : Aller à la branche de gauche.\nNon : Aller à la branche de droite.\n\nDeuxième division (branche de gauche) : “Le nombre de pièces est-il supérieur à 4 ?”\n\nOui : Prix élevé (par exemple, plus de 300 000 €).\nNon : Prix moyen (par exemple, entre 200 000 € et 300 000 €).\n\nDeuxième division (branche de droite) : “Le nombre de pièces est-il supérieur à 2 ?”\n\nOui : Prix moyen (par exemple, entre 150 000 € et 200 000 €).\nNon : Prix bas (par exemple, moins de 150 000 €).\n\n\nCet arbre utilise des règles simples pour diviser l’espace des caractéristiques (superficie et nombre de pièces) en sous-groupes homogènes et fournir une prédiction (estimer le prix d’une maison).\nFigure illustrative",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "La brique élémentaire: l'arbre de décision"
    ]
  },
  {
    "objectID": "chapters/chapter2/1-CART.html#lalgorithme-cart-un-partitionnement-binaire-récursif",
    "href": "chapters/chapter2/1-CART.html#lalgorithme-cart-un-partitionnement-binaire-récursif",
    "title": "La brique élémentaire: l’arbre de décision",
    "section": "",
    "text": "L’algorithme CART (Classification and Regression Trees) proposé par Breiman et al. (1984) est une méthode utilisée pour construire des arbres de décision, que ce soit pour des tâches de classification ou de régression. L’algorithme CART fonctionne en partitionnant l’espace des caractéristiques en sous-ensembles de manière récursive, en suivant une logique de décisions binaires à chaque étape. Ce processus est itératif et suit plusieurs étapes clés.\n\n\nLa fonction d’impureté est une mesure locale utilisée dans la construction des arbres de décision pour évaluer la qualité des divisions à chaque nœud. Elle quantifie le degré d’hétérogénéité des observations dans un nœud par rapport à la variable cible (classe pour la classification, ou valeur continue pour la régression). Plus précisément, une mesure d’impureté est conçue pour croître avec la dispersion dans un nœud. Un nœud est dit pur lorsque toutes les observations qu’il contient appartiennent à la même classe (classification) ou présentent des valeurs similaires/identiques (régression).\nL’algorithme CART utilise ce type de mesure pour choisir les divisions qui créent des sous-ensembles plus homogènes que le nœud parent. À chaque étape de construction, l’algorithme sélectionne la division qui réduit le plus l’impureté, afin de garantir des nœuds de plus en plus homogènes au fur et à mesure que l’arbre se développe.\nLe choix de la fonction d’impureté dépend du type de problème :\n\nClassification : L’indice de Gini ou l’entropie sont très souvent utilisées pour évaluer la dispersion des classes dans chaque nœud.\nRégression : La somme des erreurs quadratiques (SSE) est souvent utilisée pour mesurer la variance des valeurs cibles dans chaque nœud.\n\n\n\nDans le cadre de la classification, l’objectif est de partitionner les données de manière à ce que chaque sous-ensemble (ou région) soit le plus homogène possible en termes de classe prédite. Plusieurs mesures d’impureté sont couramment utilisées pour évaluer la qualité des divisions.\nPropriété-définition d’une mesure d’impureté\nPour un nœud \\(t\\) contenant \\(K\\) classes, une mesure d’impureté \\(I(t)\\) est une fonction qui quantifie l’hétérogénéité des classes dans ce nœud. Elle doit satisfaire les propriétés suivantes :\n\nPureté maximale : Lorsque toutes les observations du nœud appartiennent à une seule classe, c’est-à-dire que la proportion \\(p_k = 1\\) pour une classe \\(k\\) et \\(p_j = 0\\) pour toutes les autres classes \\(j \\neq k\\), l’impureté est minimale et \\(I(t) = 0\\). Cela indique que le nœud est entièrement pur, ou homogène.\nImpureté maximale : Lorsque les observations sont réparties de manière uniforme entre toutes les classes, c’est-à-dire que \\(p_k = \\frac{1}{K}\\) pour chaque classe \\(k\\), l’impureté atteint son maximum. Cette situation reflète une impureté élevée, car le nœud est très hétérogène et contient une forte incertitude sur la classe des observations.\n\n1. L’indice de Gini\nL’indice de Gini est l’une des fonctions de perte les plus couramment utilisées pour la classification. Il mesure la probabilité qu’un individu sélectionné au hasard dans un nœud soit mal classé si on lui attribue une classe au hasard, en fonction de la distribution des classes dans ce nœud.\nPour un nœud \\(t\\) contenant \\(K\\) classes, l’indice de Gini \\(G(t)\\) est donné par :\n\\[\nG(t) = 1 - \\sum_{k=1}^{K} p_k^2\n\\]\noù \\(p_k\\) est la proportion d’observations appartenant à la classe \\(k\\) dans le nœud \\(t\\).\n\nCritère de choix : L’indice de Gini est souvent utilisé parce qu’il est simple à calculer et capture bien l’homogénéité des classes au sein d’un nœud. Il privilégie les partitions où une classe domine fortement dans chaque sous-ensemble.\n2. L’entropie (ou entropie de Shannon)\nL’entropie est une autre mesure de l’impureté utilisée dans les arbres de décision. Elle mesure la quantité d’incertitude ou de désordre dans un nœud, en s’appuyant sur la théorie de l’information.\nPour un nœud \\(t\\) contenant \\(K\\) classes, l’entropie \\(E(t)\\) est définie par :\n\\[\nE(t) = - \\sum_{k=1}^{K} p_k \\log(p_k)\n\\]\noù \\(p_k\\) est la proportion d’observations de la classe \\(k\\) dans le nœud \\(t\\).\n\nCritère de choix : L’entropie a tendance à être plus sensible aux changements dans les distributions des classes que l’indice de Gini, car elle attribut un poids plus élevé aux événements rares (valeurs de \\(p_k\\) très faibles). Elle est souvent utilisée lorsque l’erreur de classification des classes minoritaires est particulièrement importante.\n3. Taux d’erreur\nLe taux d’erreur est une autre mesure de l’impureté parfois utilisée dans les arbres de décision. Il représente la proportion d’observations mal classées dans un nœud.\nPour un nœud \\(t\\), le taux d’erreur \\(\\text{TE}(t)\\) est donné par :\n\\[\n\\text{TE}(t) = 1 - \\max(p_k)\n\\]\noù \\(\\max(p_k)\\) est la proportion d’observations appartenant à la classe majoritaire dans le nœud.\n\nCritère de choix : Bien que le taux d’erreur soit simple à comprendre, il est moins souvent utilisé dans la construction des arbres de décision parce qu’il est moins sensible que l’indice de Gini ou l’entropie aux petits changements dans la distribution des classes.\n\n\n\nDans les problèmes de régression, l’objectif est de partitionner les données de manière à réduire au maximum la variabilité des valeurs au sein de chaque sous-ensemble. Pour mesurer cette variabilité, la somme des erreurs quadratiques (SSE) est la fonction d’impureté la plus couramment employée. Elle évalue l’impureté d’une région en quantifiant à quel point les valeurs de cette région s’écartent de la moyenne locale.\n1.Somme des erreurs quadratiques (SSE) ou variance\nLa somme des erreurs quadratiques (ou SSE, pour Sum of Squared Errors) est une mesure qui quantifie la dispersion des valeurs dans un nœud par rapport à la moyenne des valeurs dans ce nœud.\nFormule : Pour un nœud \\(t\\), contenant \\(N\\) observations avec des valeurs \\(y_i\\), la SSE est donnée par :\n\\[\n\\text{SSE}(t) = \\sum_{i=1}^{N} (y_i - \\hat{y})^2\n\\]\noù \\(\\hat{y}\\) est la moyenne des valeurs \\(y_i\\) dans le nœud \\(t\\).\nPropriété :\n\nSi toutes les valeurs de \\(y_i\\) dans un nœud sont proches de la moyenne \\(\\hat{y}\\), la SSE sera faible, indiquant une homogénéité élevée dans le nœud.\nEn revanche, une SSE élevée indique une grande variabilité dans les valeurs, donc un nœud impur.\n\nCritère de choix : La somme des erreurs quadratiques (SSE) est particulièrement sensible aux écarts élevés entre les valeurs observées et la moyenne prédite. En cherchant à minimiser la SSE, les modèles visent à former des nœuds dans lesquels les valeurs des observations sont aussi proches que possible de la moyenne locale.\n\n\n\n\nUne fois la mesure d’impureté définie, l’algorithme CART examine toutes les divisions binaires possibles de l’espace des caractéristiques. À chaque nœud, et pour chaque caractéristique, il cherche à identifier le seuil optimal, c’est-à-dire le seuil qui minimise le plus efficacement l’impureté des deux sous-ensembles générés. L’algorithme compare ensuite toutes les divisions potentielles (caractéristiques et seuils optimaux associés à chaque nœud) et sélectionne celle qui entraîne la réduction maximale de l’impureté.\nPrenons l’exemple d’une caractéristique continue, telle que la superficie d’une maison :\n\nSi l’algorithme teste la règle “Superficie &gt; 100 m²”, il calcule la fonction de perte pour les deux sous-ensembles générés par cette règle (“Oui” et “Non”).\nCe processus est répété pour différentes valeurs seuils afin de trouver la partition qui minimise le plus efficacement l’impureté au sein des sous-ensembles.\n\n\n\n\nL’algorithme CART poursuit le partitionnement de l’espace des caractéristiques en appliquant de manière récursive les mêmes étapes : identification de la caractéristique et du seuil optimal pour chaque nœud, puis sélection du partitionnement binaire qui maximise la réduction de l’impureté. Ce processus est répété jusqu’à ce qu’un critère d’arrêt soit atteint, par exemple :\n\nProfondeur maximale de l’arbre : Limiter le nombre de divisions successives pour éviter un arbre trop complexe.\nNombre minimum d’observations par feuille : Empêcher la création de feuilles contenant très peu d’observations, ce qui réduirait la capacité du modèle à généraliser.\nRéduction minimale de l’impureté à chaque étape\n\n\n\n\n\n\n\nUne fois l’arbre construit, la prédiction pour une nouvelle observation s’effectue en suivant les branches de l’arbre, en partant du nœud racine jusqu’à un nœud terminal (ou feuille). À chaque nœud interne, une décision est prise en fonction des valeurs des caractéristiques de l’observation, ce qui détermine la direction à suivre vers l’un des sous-ensembles. Ce cheminement se poursuit jusqu’à ce que l’observation atteigne une feuille, où la prédiction finale est effectuée.\n\nEn classification, la classe attribuée est celle majoritaire dans la feuille atteinte.\nEn régression, la valeur prédite est généralement la moyenne des valeurs cibles des observations dans la feuille.\n\n\n\n\nPour améliorer la performance de l’arbre, on peut ajuster les hyperparamètres tels que la profondeur maximale ou le nombre minimum d’observations dans une feuille. De plus, des techniques comme la prédiction avec arbres multiples (bagging, forêts aléatoires) permettent de surmonter les limites des arbres individuels, souvent sujets au surapprentissage.",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "La brique élémentaire: l'arbre de décision"
    ]
  },
  {
    "objectID": "chapters/chapter2/1-CART.html#avantages-et-limites-de-cette-approche",
    "href": "chapters/chapter2/1-CART.html#avantages-et-limites-de-cette-approche",
    "title": "La brique élémentaire: l’arbre de décision",
    "section": "",
    "text": "Interprétabilité : Les arbres de décision sont faciles à comprendre et à visualiser.\nSimplicité : Pas besoin de transformations complexes des données.\nFlexibilité : Ils peuvent gérer des caractéristiques numériques et catégorielles, ainsi que les valeurs manquantes.\nGestion des interactions : Modèles non paramétriques, pas d’hypothèses sur les lois par les variables. Ils capturent naturellement les interactions entre les caractéristiques.\n\n\n\n\n\nSurapprentissage : Les arbres trop profonds peuvent surapprendre les données d’entraînement.\nOptimisation locale : L’approche gloutonne peut conduire à des solutions sous-optimales globalement (optimum local).\nStabilité : De petits changements dans les données peuvent entraîner des changements significatifs dans la structure de l’arbre (manque de robustesse).",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "La brique élémentaire: l'arbre de décision"
    ]
  },
  {
    "objectID": "chapters/chapter1/1-survol.html",
    "href": "chapters/chapter1/1-survol.html",
    "title": "Aperçu des méthodes ensemblistes",
    "section": "",
    "text": "Principe: cette partie propose une présentation intuitive des méthodes ensemblistes, à destination notamment des managers sans bagage en machine learning. Elle ne contient aucune formalisation mathématique.\n\n\nLes approches ensemblistes désignent un ensemble d’algorithmes de machine learning supervisé développés depuis le début des années 1990, c’est-à-dire des méthodes statistiques permettant de prédire une variable-cible \\(y\\) à partir d’un ensemble de variables \\(\\mathbf{X}\\) (features). Elles peuvent par exemple être utilisées pour prédire le salaire d’un salarié, la probabilité de réponse dans une enquête, le niveau de diplôme… Ces approches se définissent par un point commun: plutôt que de tenter de construire d’emblée un unique modèle très complexe et très performant, elles visent à obtenir un modèle très performant en combinant intelligemment un ensemble de modèles simples et peu performants, dits “apprenants faibles” (weak learner ou base learner). Le choix de ces modèles de base (des arbres de décision dans la plupart des cas) et la manière dont leurs prédictions sont combinées sont des facteurs déterminants pour la performance de ces approches. Le présent document se concentre sur trois approches ensemblistes à base d’arbres de décisions qui sont fréquemment utilisées: le bagging, la forêt aléatoire (random forest) et le boosting.\nOn distingue deux grandes familles de méthodes ensemblistes, selon qu’elles s’appuient sur des modèles de base entrainés en parallèle indépendamment les uns des autres, ou au contraire de façon séquentielle. Lorsque les modèles sont entrainés en parallèle, chaque modèle de base est entraîné en utilisant soit un échantillon aléatoire des données d’entraînement, soit un sous-ensemble aléatoires des variables disponibles, et le plus souvent une combinaison des deux, auquel cas on parle de forêt aléatoire (random forest). Les implémentations les plus courantes des forêts aléatoires sont les packages ranger en R et scikit-learn en Python. Lorsque les modèles de base sont entrainés de manière séquentielle, chaque modèle de base vise à améliorer la prédiction proposée par l’ensemble des modèles de base précédents. Les implémentations les plus courantes du boosting sont actuellement XGBoost, CatBoost et LightGBM.\n\n\n\nLes méthodes ensemblistes sont particulièrement bien adaptées à de nombreux cas d’usage de la statistique publique, pour deux raisons. D’une, elles sont conçues pour s’appliquer à des données tabulaires (enregistrements en lignes, variables en colonnes), structure de données omniprésente dans la statistique publique. D’autre part, elles peuvent être mobilisées dans toutes les situations où le statisticien mobilise une régression linéaire ou une régression logistisque (imputation, repondération…).\nLes méthodes ensemblistes présentent trois avantages par rapport aux méthodes économétriques traditionnelles (régression linéaire et régression logistique):\n\nElles ont une puissance prédictive supérieure: alors que les méthodes traditionnelles supposent fréquemment l’existence d’une relation linéaire ou log-linéaire entre \\(y\\) et \\(\\mathbf{X}\\), les méthodes ensemblistes ne font quasiment aucune hypothèse sur la relation entre \\(y\\) et \\(\\mathbf{X}\\), et se contentent d’approximer le mieux possible cette relation à partir des données disponibles. En particulier, les modèles ensemblistes peuvent facilement modéliser des non-linéarités de la relation entre \\(y\\) et \\(\\mathbf{X}\\) et des interactions entre variables explicatives sans avoir à les spécifier explicitement au préalable, alors que les méthodes traditionnelles supposent fréquemment l’existence d’une relation linéaire ou log-linéaire entre \\(y\\) et \\(\\mathbf{X}\\).\nElles nécessitent moins de préparation des données: elles ne requièrent pas de normalisation des variables explicatives et peuvent s’accommoder des valeurs manquantes (selon des techniques variables selon les algorithmes).\nElles sont généralement moins sensibles aux valeurs extrêmes et à l’hétéroscédasticité des variables explicatives que les approches traditionnelles.\n\nElles présentent par ailleurs deux inconvénients rapport aux méthodes économétriques traditionnelles. Premièrement, bien qu’il existe désormais de multiples approches permettent d’interpétrer partiellement les modèles ensemblistes, leur interprétabilité reste globalement moindre que celle d’une régression linéaire ou logistique. Deuxièmement, les modèles ensemblistes sont plus complexes que les approches traditionnelles, et leurs hyperparamètres doivent faire l’objet d’une optimisation, par exemple au travers d’une validation croisée. Ce processus d’optimisation est généralement plus complexe et plus long que l’estimation d’une régression linéaire ou logistique. En revanche, les méthodes ensemblistes sont relativement simples à prendre en main, et ne requièrent pas nécessairement une puissance de calcul importante.\n\n\n\n\n\n\nEt par rapport au deep learning?\n\n\n\nSi les approches de deep learning sont sans conteste très performantes pour le traitement du langage naturel et le traitement d’image, leur supériorité n’est pas établie pour les applications reposant sur des données tabulaires. Les comparaisons disponibles dans la littérature concluent en effet que les méthodes ensemblistes à base d’arbres sont soit plus performantes que les approches de deep learning (Grinsztajn, Oyallon, and Varoquaux (2022), Shwartz-Ziv and Armon (2022)), soit font jeu égal avec elles (McElfresh et al. (2024)). Ces études ont identifié trois avantages des méthodes ensemblistes: elles sont peu sensibles aux variables explicatives non pertinentes, robustes aux valeurs extrêmes des variables explicatives, et capables d’approximer des fonctions très irrégulières. De plus, dans la pratique les méthodes ensemblistes sont souvent plus rapides à entraîner et moins gourmandes en ressources informatiques, et l’optimisation des hyperparamètres s’avère souvent moins complexe (Shwartz-Ziv and Armon (2022)).\n\n\n\n\n\n\n\n\n\nLe modèle de base des méthodes ensemblistes est le plus souvent un arbre de classification et de régression (CART, Breiman et al. (1984)). Un arbre CART est un algorithme prédictif assez simple avec trois caractéristiques essentielles:\n\nL’arbre partitionne l’espace des variables explicatives \\(X\\) en régions (appelées feuilles ou leaves) homogènes au sens d’une certaine mesure de l’hétérogénéité;\nChaque région est définie par un ensemble de conditions, appelées régles de décision (splitting rules), qui portent sur les valeurs des variables explicatives (par exemple, une région peut être définie par la condition: \\(age &gt; 40 & statut = 'Cadre'\\));\nUne fois l’arbre construit, les prédictions de l’arbre pour chaque région se déduisent des données d’entraînement de façon intuitive: il s’agira de la classe la plus fréquente parmi les observations situées dans cette région dans le cas d’une classification, et de la moyenne des observations situées dans cette région dans le cas d’une régression.\n\nLa structure de cet algorithme a trois conséquences importantes:\n\nl’arbre final est une fonction constante par morceaux: la prédiction est identique pour toutes les observations situées dans la même région, et ne varie que d’une région à l’autre.\naucune hypothèse sur le lien entre \\(X\\) et \\(y\\) forme fonctionnelle précise;\nprésence d’interactions entre variables.\n\nIllustration, et représentation graphique (sous forme d’arbre et de graphique).\n\n\n\nSi son principe est simple, la construction d’un arbre de décision se heurte à trois difficultés pratiques.\nPremière difficulté: comment trouver le partitionnement optimal en un temps raisonnable? Les arbres CART proposent une solution efficace à ce problème en faisant deux hypothèses simplificatrices. D’une part, la procédure de construction de l’arbre ne s’intéresse qu’à des critères de décision binaires très simples, mobilisant à chaque fois une seule variable et un seul seuil (exemples: \\(age &gt; 40?\\), \\(diplome = 'Licence'\\)…). Autrement dit, les critères complexes mobilisant des combinaisons de variables et de seuils sont exclus a priori. D’autre part, la construction de l’arbre se fait de façon itérative, une règle de décision à la fois: la procédure se contente à chaque étape de chercher la règle de décision qui réduit le plus l’hétérogénéité des groupes, conditionnellement aux régles de décision qui ont été choisies au préalable. Cette procédure ne garantit donc pas que l’arbre final soit optimal, mais elle permet d’obtenir rapidement un arbre raisonnablement performant.\nDeuxième difficulté: comment mesurer l’homogénéité des régions?\nTroisième difficulté: à quel moment faut-il s’arrêter?\n\n\n\nLes arbres CART présentent plusieurs avantages: leur principe est simple, ils sont aisément interprétables et peuvent faire l’objet de représentations graphiques intuitives. Par ailleurs, la flexibilité offerte par le partitionnement récursif assure que les arbres obtenus reflètent les corrélations observées dans les données d’entraînement.\nIls souffrent néanmoins de deux limites. D’une part, les arbres CART ont souvent un pouvoir prédictif faible qui en limite l’usage. D’autre part, ils sont peu robustes et instables. Ainsi, un léger changement dans les données (par exemple l’ajout ou la suppression de quelques observations) peut entraîner des modifications significatives dans la structure de l’arbre et dans la définition des feuilles. De plus, les prédictions des arbres CART sont sensibles à de petites fluctuations des données: celles-ci peuvent aboutir à ce qu’une partie des observations change brutalement de feuille et donc de valeur prédite.\nToute les méthodes ensemblistes présentées ci-dessous (bagging, random forests et boosting) combinent un grand nombre d’arbres de décision pour en surmonter les deux limites: il s’agit d’obtenir un modèle dont le pouvoir prédictif est élevé et dont les prédictions sont stables.\n\n\n\n\nPrésenter le bagging en reprenant des éléments du chapitre 10 de https://bradleyboehmke.github.io/HOML. Mettre une description de l’algorithme en pseudo-code?\n\nPrésentation avec la figure en SVG;\nIllustration avec un cas d’usage de classification en deux dimensions.\n\n\n\n\n\n\n\nFigure 1: Représentation schématique d’un algorithme de bagging\n\n\n\nLe bagging, ou Bootstrap Aggregating (Breiman (1996)), est une méthode ensembliste qui comporte trois étapes principales:\n\nTirage de sous-échantillons aléatoires: À partir du jeu de données initial, plusieurs sous-échantillons sont générés par échantillonnage aléatoire avec remise (bootstrapping). Chaque sous-échantillon a la même taille que le jeu de données original, mais peut contenir des observations répétées, tandis que d’autres peuvent être omises.\nEntraînement parallèle: Un arbre est entraîné sur chaque sous-échantillon de manière indépendante. Cette technique permet un gain d’efficacité et un meilleur contrôle du surapprentissage (overfitting).\nAgrégation des prédictions: Les prédictions des modèles sont combinées pour produire le résultat final. En classification, la prédiction finale est souvent déterminée par un vote majoritaire, tandis qu’en régression, elle correspond généralement à la moyenne des prédictions.\n\nCette technique permet de diversifier les données d’entraînement en créant des échantillons variés, ce qui aide à réduire la variance et à améliorer la robustesse du modèle.\nEn combinant les prédictions de plusieurs modèles, le bagging renforce la stabilité et la performance globale de l’algorithme, notamment en réduisant la variance des prédictions.\nAvantages: pouvoir prédictif, entraînement hautement parallélisable. Inconvénients: malgré l’échantillonnage des données, les arbres ont souvent une structure similaire car les variables hautement prédictives restent approximativement les mêmes dans les différents sous-échantillons. Ce phénomène de corrélation entre arbres est le principal frein à la puissance prédictive du bagging et explique pourquoi il est très peu utilisé en pratique aujourd’hui.\nLe bagging est particulièrement efficace pour réduire la variance des modèles, ce qui les rend moins vulnérables au surapprentissage. Cette caractéristique est particulièrement utile dans les situations où la robustesse et la capacité de généralisation des modèles sont cruciales. De plus, comme le bagging repose sur des processus indépendants, l’exécution est plus plus rapide dans des environnements distribués.\nCependant, bien que chaque modèle de base soit construit indépendamment sur des sous-échantillons distincts, les variables utilisées pour générer ces modèles ne sont pas forcément indépendantes d’un modèle à l’autre. Dans le cas du bagging appliqué aux arbres de décision, cela conduit souvent à des arbres ayant une structure similaire.\nLes forêts aléatoires apportent une amélioration à cette approche en réduisant cette corrélation entre les arbres, ce qui permet d’augmenter la précision de l’ensemble du modèle.\n\n\n\nExpliquer que les random forests sont une amélioration du bagging, en reprenant des éléments du chapitre 11 de https://bradleyboehmke.github.io/HOML/\n\n\n\n\n\n\nFigure 2: Représentation schématique d’un algorithme de forêt aléatoire\n\n\n\n\n\n\nPrésentation avec la figure en SVG;\nDifficile d’illustrer avec un exemple (car on ne peut pas vraiment représenter le feature sampling);\nBien insister sur les avantages des RF: 1/ faible nombre d’hyperparamètres; 2/ faible sensibilité aux hyperparamètres; 3/ limite intrinsèque à l’overfitting.\n\n\n\n\nReprendre des éléments du chapitre 12 de https://bradleyboehmke.github.io/HOML/ et des éléments de la formation boosting.\nLe boosting combine l’approche ensembliste avec une modélisation additive par étapes (forward stagewise additive modeling).\n\nPrésentation;\nAvantage du boosting: performances particulièrement élevées.\nInconvénients: 1/ nombre élevé d’hyperparamètres; 2/ sensibilité des performances aux hyperparamètres; 3/ risque élevé d’overfitting.\nPréciser qu’il est possible d’utiliser du subsampling par lignes et colonnes pour un algoithme de boosting. Ce point est abordé plus en détail dans la partie sur les hyperparamètres.\n\n\n\n\n\n\n\nFigure 3: Représentation schématique d’un algorithme de boosting\n\n\n\nUn arbre CART (Classification And Regression Tree) est construit en utilisant une approche hiérarchique pour diviser un ensemble de données en sous-groupes de plus en plus homogènes. Intuitivement, voici comment cela se passe :\n\nChoix de la meilleure coupure :\n\nL’arbre commence à la racine, c’est-à-dire l’ensemble complet des données.\n\nÀ chaque étape, on cherche la variable et la valeur de seuil qui divisent le mieux les données en deux groupes selon un critère spécifique (comme l’entropie, l’indice de Gini pour la classification, ou la variance pour la régression).\n\nL’objectif est de minimiser l’hétérogénéité (ou maximiser l’homogénéité) au sein des groupes créés par la division.\n\nDivision récursive :\n\nUne fois la meilleure coupure trouvée, les données sont séparées en deux sous-groupes : un groupe pour les observations qui satisfont la condition de la coupure, et l’autre pour celles qui ne la satisfont pas.\n\nCe processus est répété récursivement sur chaque sous-groupe, formant ainsi de nouveaux “nœuds” dans l’arbre.\n\nArrêt de la croissance de l’arbre :\n\nL’arbre ne continue pas à se développer indéfiniment. La division s’arrête lorsque l’un des critères de fin est atteint, par exemple :\n\nUn nombre minimal d’observations dans un nœud.\n\nUne amélioration trop faible dans le critère de division.\n\nUne profondeur maximale spécifiée.\n\n\nAssignation des prédictions :\n\nUne fois l’arbre construit, chaque feuille (nœud terminal) contient un sous-ensemble de données.\n\nPour la classification, la classe prédominante dans une feuille est assignée comme prédiction pour toutes les observations appartenant à cette feuille.\n\nPour la régression, la moyenne (ou médiane) des valeurs dans une feuille est utilisée comme prédiction.\n\n\nExemple intuitif :\nImaginez que vous essayez de deviner si une personne préfère le café ou le thé. Vous commencez par poser une question générale, comme “Préfères-tu les boissons chaudes ?” Selon la réponse, vous posez d’autres questions plus spécifiques (comme “Ajoutes-tu du lait ?” ou “Aimes-tu les boissons amères ?”), jusqu’à ce que vous puissiez deviner leur préférence avec un haut degré de certitude.\nEn résumé, construire un arbre CART revient à poser des questions successives qui divisent les données de manière optimale pour parvenir à une prédiction claire et précise.",
    "crumbs": [
      "Survol des méthodes ensemblistes",
      "Aperçu des méthodes ensemblistes"
    ]
  },
  {
    "objectID": "chapters/chapter1/1-survol.html#que-sont-les-méthodes-ensemblistes",
    "href": "chapters/chapter1/1-survol.html#que-sont-les-méthodes-ensemblistes",
    "title": "Aperçu des méthodes ensemblistes",
    "section": "",
    "text": "Les approches ensemblistes désignent un ensemble d’algorithmes de machine learning supervisé développés depuis le début des années 1990, c’est-à-dire des méthodes statistiques permettant de prédire une variable-cible \\(y\\) à partir d’un ensemble de variables \\(\\mathbf{X}\\) (features). Elles peuvent par exemple être utilisées pour prédire le salaire d’un salarié, la probabilité de réponse dans une enquête, le niveau de diplôme… Ces approches se définissent par un point commun: plutôt que de tenter de construire d’emblée un unique modèle très complexe et très performant, elles visent à obtenir un modèle très performant en combinant intelligemment un ensemble de modèles simples et peu performants, dits “apprenants faibles” (weak learner ou base learner). Le choix de ces modèles de base (des arbres de décision dans la plupart des cas) et la manière dont leurs prédictions sont combinées sont des facteurs déterminants pour la performance de ces approches. Le présent document se concentre sur trois approches ensemblistes à base d’arbres de décisions qui sont fréquemment utilisées: le bagging, la forêt aléatoire (random forest) et le boosting.\nOn distingue deux grandes familles de méthodes ensemblistes, selon qu’elles s’appuient sur des modèles de base entrainés en parallèle indépendamment les uns des autres, ou au contraire de façon séquentielle. Lorsque les modèles sont entrainés en parallèle, chaque modèle de base est entraîné en utilisant soit un échantillon aléatoire des données d’entraînement, soit un sous-ensemble aléatoires des variables disponibles, et le plus souvent une combinaison des deux, auquel cas on parle de forêt aléatoire (random forest). Les implémentations les plus courantes des forêts aléatoires sont les packages ranger en R et scikit-learn en Python. Lorsque les modèles de base sont entrainés de manière séquentielle, chaque modèle de base vise à améliorer la prédiction proposée par l’ensemble des modèles de base précédents. Les implémentations les plus courantes du boosting sont actuellement XGBoost, CatBoost et LightGBM.",
    "crumbs": [
      "Survol des méthodes ensemblistes",
      "Aperçu des méthodes ensemblistes"
    ]
  },
  {
    "objectID": "chapters/chapter1/1-survol.html#pourquoi-utiliser-des-méthodes-ensemblistes",
    "href": "chapters/chapter1/1-survol.html#pourquoi-utiliser-des-méthodes-ensemblistes",
    "title": "Aperçu des méthodes ensemblistes",
    "section": "",
    "text": "Les méthodes ensemblistes sont particulièrement bien adaptées à de nombreux cas d’usage de la statistique publique, pour deux raisons. D’une, elles sont conçues pour s’appliquer à des données tabulaires (enregistrements en lignes, variables en colonnes), structure de données omniprésente dans la statistique publique. D’autre part, elles peuvent être mobilisées dans toutes les situations où le statisticien mobilise une régression linéaire ou une régression logistisque (imputation, repondération…).\nLes méthodes ensemblistes présentent trois avantages par rapport aux méthodes économétriques traditionnelles (régression linéaire et régression logistique):\n\nElles ont une puissance prédictive supérieure: alors que les méthodes traditionnelles supposent fréquemment l’existence d’une relation linéaire ou log-linéaire entre \\(y\\) et \\(\\mathbf{X}\\), les méthodes ensemblistes ne font quasiment aucune hypothèse sur la relation entre \\(y\\) et \\(\\mathbf{X}\\), et se contentent d’approximer le mieux possible cette relation à partir des données disponibles. En particulier, les modèles ensemblistes peuvent facilement modéliser des non-linéarités de la relation entre \\(y\\) et \\(\\mathbf{X}\\) et des interactions entre variables explicatives sans avoir à les spécifier explicitement au préalable, alors que les méthodes traditionnelles supposent fréquemment l’existence d’une relation linéaire ou log-linéaire entre \\(y\\) et \\(\\mathbf{X}\\).\nElles nécessitent moins de préparation des données: elles ne requièrent pas de normalisation des variables explicatives et peuvent s’accommoder des valeurs manquantes (selon des techniques variables selon les algorithmes).\nElles sont généralement moins sensibles aux valeurs extrêmes et à l’hétéroscédasticité des variables explicatives que les approches traditionnelles.\n\nElles présentent par ailleurs deux inconvénients rapport aux méthodes économétriques traditionnelles. Premièrement, bien qu’il existe désormais de multiples approches permettent d’interpétrer partiellement les modèles ensemblistes, leur interprétabilité reste globalement moindre que celle d’une régression linéaire ou logistique. Deuxièmement, les modèles ensemblistes sont plus complexes que les approches traditionnelles, et leurs hyperparamètres doivent faire l’objet d’une optimisation, par exemple au travers d’une validation croisée. Ce processus d’optimisation est généralement plus complexe et plus long que l’estimation d’une régression linéaire ou logistique. En revanche, les méthodes ensemblistes sont relativement simples à prendre en main, et ne requièrent pas nécessairement une puissance de calcul importante.\n\n\n\n\n\n\nEt par rapport au deep learning?\n\n\n\nSi les approches de deep learning sont sans conteste très performantes pour le traitement du langage naturel et le traitement d’image, leur supériorité n’est pas établie pour les applications reposant sur des données tabulaires. Les comparaisons disponibles dans la littérature concluent en effet que les méthodes ensemblistes à base d’arbres sont soit plus performantes que les approches de deep learning (Grinsztajn, Oyallon, and Varoquaux (2022), Shwartz-Ziv and Armon (2022)), soit font jeu égal avec elles (McElfresh et al. (2024)). Ces études ont identifié trois avantages des méthodes ensemblistes: elles sont peu sensibles aux variables explicatives non pertinentes, robustes aux valeurs extrêmes des variables explicatives, et capables d’approximer des fonctions très irrégulières. De plus, dans la pratique les méthodes ensemblistes sont souvent plus rapides à entraîner et moins gourmandes en ressources informatiques, et l’optimisation des hyperparamètres s’avère souvent moins complexe (Shwartz-Ziv and Armon (2022)).",
    "crumbs": [
      "Survol des méthodes ensemblistes",
      "Aperçu des méthodes ensemblistes"
    ]
  },
  {
    "objectID": "chapters/chapter1/1-survol.html#comment-fonctionnent-les-méthodes-ensemblistes",
    "href": "chapters/chapter1/1-survol.html#comment-fonctionnent-les-méthodes-ensemblistes",
    "title": "Aperçu des méthodes ensemblistes",
    "section": "",
    "text": "Le modèle de base des méthodes ensemblistes est le plus souvent un arbre de classification et de régression (CART, Breiman et al. (1984)). Un arbre CART est un algorithme prédictif assez simple avec trois caractéristiques essentielles:\n\nL’arbre partitionne l’espace des variables explicatives \\(X\\) en régions (appelées feuilles ou leaves) homogènes au sens d’une certaine mesure de l’hétérogénéité;\nChaque région est définie par un ensemble de conditions, appelées régles de décision (splitting rules), qui portent sur les valeurs des variables explicatives (par exemple, une région peut être définie par la condition: \\(age &gt; 40 & statut = 'Cadre'\\));\nUne fois l’arbre construit, les prédictions de l’arbre pour chaque région se déduisent des données d’entraînement de façon intuitive: il s’agira de la classe la plus fréquente parmi les observations situées dans cette région dans le cas d’une classification, et de la moyenne des observations situées dans cette région dans le cas d’une régression.\n\nLa structure de cet algorithme a trois conséquences importantes:\n\nl’arbre final est une fonction constante par morceaux: la prédiction est identique pour toutes les observations situées dans la même région, et ne varie que d’une région à l’autre.\naucune hypothèse sur le lien entre \\(X\\) et \\(y\\) forme fonctionnelle précise;\nprésence d’interactions entre variables.\n\nIllustration, et représentation graphique (sous forme d’arbre et de graphique).\n\n\n\nSi son principe est simple, la construction d’un arbre de décision se heurte à trois difficultés pratiques.\nPremière difficulté: comment trouver le partitionnement optimal en un temps raisonnable? Les arbres CART proposent une solution efficace à ce problème en faisant deux hypothèses simplificatrices. D’une part, la procédure de construction de l’arbre ne s’intéresse qu’à des critères de décision binaires très simples, mobilisant à chaque fois une seule variable et un seul seuil (exemples: \\(age &gt; 40?\\), \\(diplome = 'Licence'\\)…). Autrement dit, les critères complexes mobilisant des combinaisons de variables et de seuils sont exclus a priori. D’autre part, la construction de l’arbre se fait de façon itérative, une règle de décision à la fois: la procédure se contente à chaque étape de chercher la règle de décision qui réduit le plus l’hétérogénéité des groupes, conditionnellement aux régles de décision qui ont été choisies au préalable. Cette procédure ne garantit donc pas que l’arbre final soit optimal, mais elle permet d’obtenir rapidement un arbre raisonnablement performant.\nDeuxième difficulté: comment mesurer l’homogénéité des régions?\nTroisième difficulté: à quel moment faut-il s’arrêter?\n\n\n\nLes arbres CART présentent plusieurs avantages: leur principe est simple, ils sont aisément interprétables et peuvent faire l’objet de représentations graphiques intuitives. Par ailleurs, la flexibilité offerte par le partitionnement récursif assure que les arbres obtenus reflètent les corrélations observées dans les données d’entraînement.\nIls souffrent néanmoins de deux limites. D’une part, les arbres CART ont souvent un pouvoir prédictif faible qui en limite l’usage. D’autre part, ils sont peu robustes et instables. Ainsi, un léger changement dans les données (par exemple l’ajout ou la suppression de quelques observations) peut entraîner des modifications significatives dans la structure de l’arbre et dans la définition des feuilles. De plus, les prédictions des arbres CART sont sensibles à de petites fluctuations des données: celles-ci peuvent aboutir à ce qu’une partie des observations change brutalement de feuille et donc de valeur prédite.\nToute les méthodes ensemblistes présentées ci-dessous (bagging, random forests et boosting) combinent un grand nombre d’arbres de décision pour en surmonter les deux limites: il s’agit d’obtenir un modèle dont le pouvoir prédictif est élevé et dont les prédictions sont stables.\n\n\n\n\nPrésenter le bagging en reprenant des éléments du chapitre 10 de https://bradleyboehmke.github.io/HOML. Mettre une description de l’algorithme en pseudo-code?\n\nPrésentation avec la figure en SVG;\nIllustration avec un cas d’usage de classification en deux dimensions.\n\n\n\n\n\n\n\nFigure 1: Représentation schématique d’un algorithme de bagging\n\n\n\nLe bagging, ou Bootstrap Aggregating (Breiman (1996)), est une méthode ensembliste qui comporte trois étapes principales:\n\nTirage de sous-échantillons aléatoires: À partir du jeu de données initial, plusieurs sous-échantillons sont générés par échantillonnage aléatoire avec remise (bootstrapping). Chaque sous-échantillon a la même taille que le jeu de données original, mais peut contenir des observations répétées, tandis que d’autres peuvent être omises.\nEntraînement parallèle: Un arbre est entraîné sur chaque sous-échantillon de manière indépendante. Cette technique permet un gain d’efficacité et un meilleur contrôle du surapprentissage (overfitting).\nAgrégation des prédictions: Les prédictions des modèles sont combinées pour produire le résultat final. En classification, la prédiction finale est souvent déterminée par un vote majoritaire, tandis qu’en régression, elle correspond généralement à la moyenne des prédictions.\n\nCette technique permet de diversifier les données d’entraînement en créant des échantillons variés, ce qui aide à réduire la variance et à améliorer la robustesse du modèle.\nEn combinant les prédictions de plusieurs modèles, le bagging renforce la stabilité et la performance globale de l’algorithme, notamment en réduisant la variance des prédictions.\nAvantages: pouvoir prédictif, entraînement hautement parallélisable. Inconvénients: malgré l’échantillonnage des données, les arbres ont souvent une structure similaire car les variables hautement prédictives restent approximativement les mêmes dans les différents sous-échantillons. Ce phénomène de corrélation entre arbres est le principal frein à la puissance prédictive du bagging et explique pourquoi il est très peu utilisé en pratique aujourd’hui.\nLe bagging est particulièrement efficace pour réduire la variance des modèles, ce qui les rend moins vulnérables au surapprentissage. Cette caractéristique est particulièrement utile dans les situations où la robustesse et la capacité de généralisation des modèles sont cruciales. De plus, comme le bagging repose sur des processus indépendants, l’exécution est plus plus rapide dans des environnements distribués.\nCependant, bien que chaque modèle de base soit construit indépendamment sur des sous-échantillons distincts, les variables utilisées pour générer ces modèles ne sont pas forcément indépendantes d’un modèle à l’autre. Dans le cas du bagging appliqué aux arbres de décision, cela conduit souvent à des arbres ayant une structure similaire.\nLes forêts aléatoires apportent une amélioration à cette approche en réduisant cette corrélation entre les arbres, ce qui permet d’augmenter la précision de l’ensemble du modèle.\n\n\n\nExpliquer que les random forests sont une amélioration du bagging, en reprenant des éléments du chapitre 11 de https://bradleyboehmke.github.io/HOML/\n\n\n\n\n\n\nFigure 2: Représentation schématique d’un algorithme de forêt aléatoire\n\n\n\n\n\n\nPrésentation avec la figure en SVG;\nDifficile d’illustrer avec un exemple (car on ne peut pas vraiment représenter le feature sampling);\nBien insister sur les avantages des RF: 1/ faible nombre d’hyperparamètres; 2/ faible sensibilité aux hyperparamètres; 3/ limite intrinsèque à l’overfitting.\n\n\n\n\nReprendre des éléments du chapitre 12 de https://bradleyboehmke.github.io/HOML/ et des éléments de la formation boosting.\nLe boosting combine l’approche ensembliste avec une modélisation additive par étapes (forward stagewise additive modeling).\n\nPrésentation;\nAvantage du boosting: performances particulièrement élevées.\nInconvénients: 1/ nombre élevé d’hyperparamètres; 2/ sensibilité des performances aux hyperparamètres; 3/ risque élevé d’overfitting.\nPréciser qu’il est possible d’utiliser du subsampling par lignes et colonnes pour un algoithme de boosting. Ce point est abordé plus en détail dans la partie sur les hyperparamètres.\n\n\n\n\n\n\n\nFigure 3: Représentation schématique d’un algorithme de boosting\n\n\n\nUn arbre CART (Classification And Regression Tree) est construit en utilisant une approche hiérarchique pour diviser un ensemble de données en sous-groupes de plus en plus homogènes. Intuitivement, voici comment cela se passe :\n\nChoix de la meilleure coupure :\n\nL’arbre commence à la racine, c’est-à-dire l’ensemble complet des données.\n\nÀ chaque étape, on cherche la variable et la valeur de seuil qui divisent le mieux les données en deux groupes selon un critère spécifique (comme l’entropie, l’indice de Gini pour la classification, ou la variance pour la régression).\n\nL’objectif est de minimiser l’hétérogénéité (ou maximiser l’homogénéité) au sein des groupes créés par la division.\n\nDivision récursive :\n\nUne fois la meilleure coupure trouvée, les données sont séparées en deux sous-groupes : un groupe pour les observations qui satisfont la condition de la coupure, et l’autre pour celles qui ne la satisfont pas.\n\nCe processus est répété récursivement sur chaque sous-groupe, formant ainsi de nouveaux “nœuds” dans l’arbre.\n\nArrêt de la croissance de l’arbre :\n\nL’arbre ne continue pas à se développer indéfiniment. La division s’arrête lorsque l’un des critères de fin est atteint, par exemple :\n\nUn nombre minimal d’observations dans un nœud.\n\nUne amélioration trop faible dans le critère de division.\n\nUne profondeur maximale spécifiée.\n\n\nAssignation des prédictions :\n\nUne fois l’arbre construit, chaque feuille (nœud terminal) contient un sous-ensemble de données.\n\nPour la classification, la classe prédominante dans une feuille est assignée comme prédiction pour toutes les observations appartenant à cette feuille.\n\nPour la régression, la moyenne (ou médiane) des valeurs dans une feuille est utilisée comme prédiction.\n\n\nExemple intuitif :\nImaginez que vous essayez de deviner si une personne préfère le café ou le thé. Vous commencez par poser une question générale, comme “Préfères-tu les boissons chaudes ?” Selon la réponse, vous posez d’autres questions plus spécifiques (comme “Ajoutes-tu du lait ?” ou “Aimes-tu les boissons amères ?”), jusqu’à ce que vous puissiez deviner leur préférence avec un haut degré de certitude.\nEn résumé, construire un arbre CART revient à poser des questions successives qui divisent les données de manière optimale pour parvenir à une prédiction claire et précise.",
    "crumbs": [
      "Survol des méthodes ensemblistes",
      "Aperçu des méthodes ensemblistes"
    ]
  },
  {
    "objectID": "chapters/chapter1/2-comparaison_GB_RF.html",
    "href": "chapters/chapter1/2-comparaison_GB_RF.html",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "",
    "text": "Comparaison RF-GBDT\nLes forêts aléatoires et le gradient boosting paraissent très similaires au premier abord: il s’agit de deux approches ensemblistes, qui construisent des modèles très prédictifs performants en combinant un grand nombre d’arbres de décision. Mais en réalité, ces deux approches présentent plusieurs différences fondamentales:\n\nLes deux approches reposent sur des fondements théoriques différents: la loi des grands nombres pour les forêts aléatoires, la théorie de l’apprentissage statistique pour le boosting.\nLes arbres n’ont pas le même statut dans les deux approches. Dans une forêt aléatoire, les arbres sont entraînés indépendamment les uns des autres et constituent chacun un modèle à part entière, qui peut être utilisé, représenté et interprété isolément. Dans un modèle de boosting, les arbres sont entraînés séquentiellement, ce qui implique que chaque arbre n’a pas de sens indépendamment de l’ensemble des arbres qui l’ont précédé dans l’entraînement.\nLes points d’attention dans l’entraînement ne sont pas les mêmes: arbitrage puissance-corrélation dans la RF, arbitrage puissance-overfitting dans le boosting.\n\n\n\noverfitting: borne théorique à l’overfitting dans les RF, contre pas de borne dans le boosting. Deux conséquences: 1/ lutter contre l’overfitting est essentiel dans l’usage du boosting; 2/ le boosting est plus sensible au bruit et aux erreurs sur \\(y\\) que la RF.\nConditions d’utilisation: la RF peut être utilisée en OOB, pas le boosting.\nComplexité d’usage: peu d’hyperparamètres dans les RF, contre un grand nombre dans le boosting.",
    "crumbs": [
      "Survol des méthodes ensemblistes",
      "Comparaison RF-GBDT"
    ]
  },
  {
    "objectID": "chapters/chapter2/2-bagging.html",
    "href": "chapters/chapter2/2-bagging.html",
    "title": "Le bagging",
    "section": "",
    "text": "Le bagging, ou “bootstrap aggregating”, est une méthode ensembliste qui vise à améliorer la stabilité et la précision des algorithmes d’apprentissage automatique en agrégeant plusieurs modèles (Breiman (1996)). Chaque modèle est entraîné sur un échantillon distinct généré par une technique de rééchantillonnage (bootstrap). Ces modèles sont ensuite combinés pour produire une prédiction agrégée, souvent plus robuste et généralisable que celle obtenue par un modèle unique.\n\n\n\nLe bagging comporte trois étapes principales:\n\nL’échantillonnage bootstrap : L’échantillonnage bootstrap consiste à créer des échantillons distincts en tirant aléatoirement avec remise des observations du jeu de données initial. Chaque échantillon bootstrap contient le même nombre d’observations que le jeu de données initial, mais certaines observations sont répétées (car sélectionnées plusieurs fois), tandis que d’autres sont omises.\nL’entraînement de plusieurs modèles : Un modèle (aussi appelé apprenant de base ou weak learner) est entraîné sur chaque échantillon bootstrap. Les modèles peuvent être des arbres de décision, des régressions ou tout autre algorithme d’apprentissage. Le bagging est particulièrement efficace avec des modèles instables, tels que les arbres de décision non élagués.\nL’agrégation des prédictions : Les prédictions de tous les modèles sont ensuite agrégées, en procédant généralement à la moyenne (ou à la médiane) des prédictions dans le cas de la régression, et au vote majoritaire (ou à la moyenne des probabilités prédites pour chaque classe) dans le cas de la classification, afin d’obtenir des prédictions plus précises et généralisables.\n\n\n\n\nCertains modèles sont très sensibles aux données d’entraînement, et leurs prédictions sont très instables d’un échantillon à l’autre. L’objectif du bagging est de construire un prédicteur plus précis en agrégeant les prédictions de plusieurs modèles entraînés sur des échantillons (légèrement) différents les uns des autres.\nBreiman (1996) montre que cette méthode est particulièrement efficace lorsqu’elle est appliquée à des modèles très instables, dont les performances sont particulièrement sensibles aux variations du jeu de données d’entraînement, et peu biaisés.\nCette section vise à mieux comprendre comment (et sous quelles conditions) l’agrégation par bagging permet de construire un prédicteur plus performant.\nDans la suite, nous notons \\(φ(x, L)\\) un prédicteur (d’une valeur numérique dans le cas de la régression ou d’un label dans le cas de la classification), entraîné sur un ensemble d’apprentissage \\(L\\), et prenant en entrée un vecteur de caractéristiques \\(x\\).\n\n\nDans le contexte de la régression, l’objectif est de prédire une valeur numérique \\(Y\\) à partir d’un vecteur de caractéristiques \\(x\\). Un modèle de régression \\(\\phi(x, L)\\) est construit à partir d’un ensemble d’apprentissage \\(L\\), et produit une estimation de \\(Y\\) pour chaque observation \\(x\\).\n\n\nDans le cas de la régression, le prédicteur agrégé est défini comme suit :\n$ _A(x) = E_L[(x, L)] $\noù \\(\\phi_A(x)\\) représente la prédiction agrégée, \\(E_L[.]\\) correspond à l’espérance prise sur tous les échantillons d’apprentissage possibles \\(L\\), chacun étant tiré selon la même distribution que le jeu de données initial, et \\(\\phi(x, L)\\) correspond à la prédiction du modèle construit sur l’échantillon d’apprentissage \\(L\\).\n\n\n\nPour mieux comprendre comment l’agrégation améliore la performance globale d’un modèle individuel \\(\\phi(x, L)\\), revenons à la décomposition biais-variance de l’erreur quadratique moyenne (il s’agit de la mesure de performance classiquement considérée dans un problème de régression):\n\\[E_L[\\left(Y - \\phi(x, L)\\right)^2] = \\underbrace{\\left(E_L\\left[\\phi(x, L) - Y\\right]\\right)^2}_{\\text{Biais}^2} + \\underbrace{E_L[\\left(\\phi(x, L) - E_L[\\phi(x, L)]\\right)^2]}_{\\text{Variance}} \\tag{1}\\]\n\nLe biais est la différence entre la valeur observée \\(Y\\) que l’on souhaite prédire et la prédiction moyenne \\(E_L[\\phi(x, L)]\\). Si le modèle est sous-ajusté, le biais sera élevé.\nLa variance est la variabilité des prédictions (\\(\\phi(x, L)\\)) autour de leur moyenne (\\(E_L[\\phi(x, L)]\\)). Un modèle avec une variance élevée est très sensible aux fluctuations au sein des données d’entraînement: ses prédictions varient beaucoup lorsque les données d’entraînement se modifient.\n\nL’équation Equation 1 illustre l’arbitrage biais-variance qui est omniprésent en machine learning: plus la complexité d’un modèle s’accroît (exemple: la profondeur d’un arbre), plus son biais sera plus faible (car ses prédictions seront de plus en plus proches des données d’entraînement), et plus sa variance sera élevée (car ses prédictions, étant très proches des données d’entraînement, auront tendance à varier fortement d’un jeu d’entraînement à l’autre).\n\n\n\nBreiman (1996) compare l’erreur quadratique moyenne d’un modèle individuel avec celle du modèle agrégé et démontre l’inégalité suivante :\n\n$ (Y - _A(x))^2 E_L[(Y - (x, L))^2] $ {#eq-inegalite-breiman1996}\n\nLe terme \\((Y - \\phi_A(x))^2\\) représente l’erreur quadratique du prédicteur agrégé \\(\\phi_A(x)\\);\nLe terme \\(E_L[(Y - \\phi(x, L))^2]\\) est l’erreur quadratique moyenne d’un prédicteur individuel \\(\\phi(x, L)\\) entraîné sur un échantillon aléatoire \\(L\\). Cette erreur varie en fonction des données d’entraînement.\n\nCette inégalité montre que l’erreur quadratique moyenne du prédicteur agrégé est toujours inférieure ou égale à la moyenne des erreurs des prédicteurs individuels. Puisque le biais du prédicteur agrégé est identique au biais du prédicteur individuel, alors l’inégalité précédente implique que la variance du modèle agrégé \\(\\phi_A(x)\\) est toujours inférieure ou égale à la variance moyenne d’un modèle individuel :\n$ (_A(x)) = (E_L[(x, L)]) E_L[((x, L))] $\nAutrement dit, le processus d’agrégation réduit l’erreur de prédiction globale en réduisant la variance des prédictions, tout en conservant un biais constant.\nCe résultat ouvre la voie à des considérations pratiques immédiates. Lorsque le modèle individuel est instable et présente une variance élevée, l’inégalité \\(Var(\\phi_A(x)) \\leq E_L[Var(\\phi(x,L))]\\) est forte, ce qui signifie que l’agrégation peut améliorer significativement la performance globale du modèle. En revanche, si \\(ϕ(x,L)\\) varie peu d’un ensemble d’entraînement à un autre (modèle stable avec variance faible), alors \\(Var(\\phi_A(x))\\) est proche de \\(E_L[Var(\\phi(x,L))]\\), et la réduction de variance apportée par l’agrégation est faible. Ainsi, le bagging est particulièrement efficace pour les modèles instables, tels que les arbres de décision, mais moins efficace pour les modèles stables tels que les méthodes des k plus proches voisins.\n\n\n\n\nDans le cas de la classification, le mécanisme de réduction de la variance par le bagging permet, sous une certaine condition, d’atteindre un classificateur presque optimal (nearly optimal classifier). Ce concept a été introduit par Breiman (1996) pour décrire un modèle qui tend à classer une observation dans la classe la plus probable, avec une performance approchant celle du classificateur Bayésien optimal (la meilleure performance théorique qu’un modèle de classification puisse atteindre).\nPour comprendre ce résutlat, introduisons \\(Q(j|x) = E_L(1_{φ(x, L) = j}) = P(φ(x, L) = j)\\), la probabilité qu’un modèle \\(φ(x, L)\\) prédise la classe \\(j\\) pour l’observation \\(x\\), et \\(P(j|x)\\), la probabilité réelle (conditionnelle) que \\(x\\) appartienne à la classe \\(j\\).\n\n\nUn classificateur \\(φ(x, L)\\) est dit order-correct pour une observation \\(x\\) si, en espérance, il identifie correctement la classe la plus probable, même s’il ne prédit pas toujours avec exactitude les probabilités associées à chaque classe \\(Q(j∣x)\\).\nCela signifie que si l’on considérait tous les ensemble de données possibles, et que l’on évaluait les prédictions du modèle en \\(x\\), la majorité des prédictions correspondraient à la classe à laquelle il a la plus grande probabilité vraie d’appartenir \\(P(j∣x)\\).\nFormellement, un prédicteur est dit “order-correct” pour une entrée \\(x\\) si :\n$ argmax_j Q(j|x) = argmax_j P(j|x) $\noù \\(P(j|x)\\) est la vraie probabilité que l’observation \\(x\\) appartienne à la classe \\(j\\), et \\(Q(j|x)\\) est la probabilité que \\(x\\) appartienne à la classe \\(j\\) prédite par le modèle \\(φ(x, L)\\).\nUn classificateur est order-correct si, pour chaque observation \\(x\\), la classe qu’il prédit correspond à celle qui a la probabilité maximale \\(P(j|x)\\) dans la distribution vraie.\n\n\n\nDans le cas de la classification, le prédicteur agrégé est défini par le vote majoritaire. Cela signifie que si \\(K\\) classificateurs sont entraînés sur \\(K\\) échantillons distincts, la classe prédite pour \\(x\\) est celle qui reçoit le plus de votes de la part des modèles individuels.\nFormellement, le classificateur agrégé \\(φA(x)\\) est défini par :\n\\(φA(x) =  \\text{argmax}_j \\sum_{L} I(\\phi(x, L) = j) = argmax_j Q(j|x)\\)\n\n\n\nBreiman (1996) montre que si chaque prédicteur individuel \\(φ(x, L)\\) est order-correct pour une observation \\(x\\), alors le prédicteur agrégé \\(φA(x)\\), obtenu par vote majoritaire, atteint la performance optimale pour cette observation, c’est-à-dire qu’il converge vers la classe ayant la probabilité maximale \\(P(j∣x)\\) pour l’observation \\(x\\) lorsque le nombre de prédicteurs individuels augmente. Le vote majoritaire permet ainsi de réduire les erreurs aléatoires des classificateurs individuels.\nLe classificateur agrégé \\(ϕA\\) est optimal s’il prédit systématiquement la classe la plus probable pour l’observation \\(x\\) dans toutes les régions de l’espace.\nCependant, dans les régions de l’espace où les classificateurs individuels ne sont pas order-corrects (c’est-à-dire qu’ils se trompent majoritairement sur la classe d’appartenance), l’agrégation par vote majoritaire n’améliore pas les performances. Elles peuvent même se détériorer par rapport aux modèles individuels si l’agrégation conduit à amplifier des erreurs systématiques (biais).\n\n\n\n\n\nEn pratique, au lieu d’utiliser tous les ensembles d’entraînement possibles \\(L\\), le bagging repose sur un nombre limité d’échantillons bootstrap tirés avec remise à partir d’un même jeu de données initial, ce qui peut introduire des biais par rapport au prédicteur agrégé théorique.\nLes échantillons bootstrap présentent les limites suivantes :\n\nUne taille effective réduite par rapport au jeu de données initial: Bien que chaque échantillon bootstrap présente le même nombre d’observations que le jeu de données initial, environ 1/3 des observations (uniques) du jeu initial sont absentes de chaque échantillon bootstrap (du fait du tirage avec remise). Cela peut limiter la capacité des modèles à capturer des relations complexes au sein des données (et aboutir à des modèles individuels sous-ajustés par rapport à ce qui serait attendu théoriquement), en particulier lorsque l’échantillon initial est de taille modeste.\nUne dépendance entre échantillons : Les échantillons bootstrap sont tirés dans le même jeu de données, ce qui génère une dépendance entre eux, qui réduit la diversité des modèles. Cela peut limiter l’efficacité de la réduction de variance dans le cas de la régression, voire acroître le biais dans le cas de la classification.\nUne couverture incomplète de l’ensemble des échantillons possibles: Les échantillons bootstrap ne couvrent pas l’ensemble des échantillons d’entraînement possibles, ce qui peut introduire un biais supplémentaire par rapport au prédicteur agrégé théorique.\n\n\n\n\n\n\nLe bagging est particulièrement utile lorsque les modèles individuels présentent une variance élevée et sont instables. Dans de tels cas, l’agrégation des prédictions peut réduire significativement la variance globale, améliorant ainsi la performance du modèle agrégé. Les situations où le bagging est recommandé incluent typiquement:\n\nLes modèles instables : Les modèles tels que les arbres de décision non élagués, qui sont sensibles aux variations des données d’entraînement, bénéficient grandement du bagging. L’agrégation atténue les fluctuations des prédictions dues aux différents échantillons.\nLes modèles avec biais faibles: En classification, si les modèles individuels sont order-corrects pour la majorité des observations, le bagging peut améliorer la précision en renforçant les prédictions correctes et en réduisant les erreurs aléatoires.\n\nInversement, le bagging peut être moins efficace ou même néfaste dans certaines situations :\n\nLes modèles stables avec variance faible : Si les modèles individuels sont déjà stables et présentent une faible variance (par exemple, la régression linéaire), le bagging n’apporte que peu d’amélioration, car la réduction de variance supplémentaire est minimale.\nLa présence de biais élevée : Si les modèles individuels sont biaisés, entraînant des erreurs systématiques, le bagging peut amplifier ces erreurs plutôt que de les corriger. Dans de tels cas, il est préférable de s’attaquer d’abord au biais des modèles avant de considérer l’agrégation.\nLes échantillons de petite taille : Avec des ensembles de données limités, les échantillons bootstrap peuvent ne pas être suffisamment diversifiés ou représentatifs, ce qui réduit l’efficacité du bagging et peut augmenter le biais des modèles.\n\nCe qui qu’il faut retenir: le bagging peut améliorer substantiellement la performance des modèles d’apprentissage automatique lorsqu’il est appliqué dans des conditions appropriées. Il est essentiel d’évaluer la variance et le biais des modèles individuels, ainsi que la taille et la représentativité du jeu de données, pour déterminer si le bagging est une stratégie adaptée. Lorsqu’il est utilisé judicieusement, le bagging peut conduire à des modèles plus robustes et précis, exploitant efficacement la puissance de l’agrégation pour améliorer la performance des modèles individuels.\n\n\n\n\n\n“Optimal performance is often found by bagging 50–500 trees. Data sets that have a few strong predictors typically require less trees; whereas data sets with lots of noise or multiple strong predictors may need more. Using too many trees will not lead to overfitting. However, it’s important to realize that since multiple models are being run, the more iterations you perform the more computational and time requirements you will have. As these demands increase, performing k-fold CV can become computationally burdensome.”\n\n\n\n“A benefit to creating ensembles via bagging, which is based on resampling with replacement, is that it can provide its own internal estimate of predictive performance with the out-of-bag (OOB) sample (see Section 2.4.2). The OOB sample can be used to test predictive performance and the results usually compare well compared to k-fold CV assuming your data set is sufficiently large (say n≥1,000). Consequently, as your data sets become larger and your bagging iterations increase, it is common to use the OOB error estimate as a proxy for predictive performance.”\n\n\n\n\n\nOu bien ne commencer les mises en pratique qu’avec les random forest ?",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "Le bagging"
    ]
  },
  {
    "objectID": "chapters/chapter2/2-bagging.html#principe-du-bagging",
    "href": "chapters/chapter2/2-bagging.html#principe-du-bagging",
    "title": "Le bagging",
    "section": "",
    "text": "Le bagging comporte trois étapes principales:\n\nL’échantillonnage bootstrap : L’échantillonnage bootstrap consiste à créer des échantillons distincts en tirant aléatoirement avec remise des observations du jeu de données initial. Chaque échantillon bootstrap contient le même nombre d’observations que le jeu de données initial, mais certaines observations sont répétées (car sélectionnées plusieurs fois), tandis que d’autres sont omises.\nL’entraînement de plusieurs modèles : Un modèle (aussi appelé apprenant de base ou weak learner) est entraîné sur chaque échantillon bootstrap. Les modèles peuvent être des arbres de décision, des régressions ou tout autre algorithme d’apprentissage. Le bagging est particulièrement efficace avec des modèles instables, tels que les arbres de décision non élagués.\nL’agrégation des prédictions : Les prédictions de tous les modèles sont ensuite agrégées, en procédant généralement à la moyenne (ou à la médiane) des prédictions dans le cas de la régression, et au vote majoritaire (ou à la moyenne des probabilités prédites pour chaque classe) dans le cas de la classification, afin d’obtenir des prédictions plus précises et généralisables.",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "Le bagging"
    ]
  },
  {
    "objectID": "chapters/chapter2/2-bagging.html#pourquoi-et-dans-quelles-situations-le-bagging-fonctionne",
    "href": "chapters/chapter2/2-bagging.html#pourquoi-et-dans-quelles-situations-le-bagging-fonctionne",
    "title": "Le bagging",
    "section": "",
    "text": "Certains modèles sont très sensibles aux données d’entraînement, et leurs prédictions sont très instables d’un échantillon à l’autre. L’objectif du bagging est de construire un prédicteur plus précis en agrégeant les prédictions de plusieurs modèles entraînés sur des échantillons (légèrement) différents les uns des autres.\nBreiman (1996) montre que cette méthode est particulièrement efficace lorsqu’elle est appliquée à des modèles très instables, dont les performances sont particulièrement sensibles aux variations du jeu de données d’entraînement, et peu biaisés.\nCette section vise à mieux comprendre comment (et sous quelles conditions) l’agrégation par bagging permet de construire un prédicteur plus performant.\nDans la suite, nous notons \\(φ(x, L)\\) un prédicteur (d’une valeur numérique dans le cas de la régression ou d’un label dans le cas de la classification), entraîné sur un ensemble d’apprentissage \\(L\\), et prenant en entrée un vecteur de caractéristiques \\(x\\).\n\n\nDans le contexte de la régression, l’objectif est de prédire une valeur numérique \\(Y\\) à partir d’un vecteur de caractéristiques \\(x\\). Un modèle de régression \\(\\phi(x, L)\\) est construit à partir d’un ensemble d’apprentissage \\(L\\), et produit une estimation de \\(Y\\) pour chaque observation \\(x\\).\n\n\nDans le cas de la régression, le prédicteur agrégé est défini comme suit :\n$ _A(x) = E_L[(x, L)] $\noù \\(\\phi_A(x)\\) représente la prédiction agrégée, \\(E_L[.]\\) correspond à l’espérance prise sur tous les échantillons d’apprentissage possibles \\(L\\), chacun étant tiré selon la même distribution que le jeu de données initial, et \\(\\phi(x, L)\\) correspond à la prédiction du modèle construit sur l’échantillon d’apprentissage \\(L\\).\n\n\n\nPour mieux comprendre comment l’agrégation améliore la performance globale d’un modèle individuel \\(\\phi(x, L)\\), revenons à la décomposition biais-variance de l’erreur quadratique moyenne (il s’agit de la mesure de performance classiquement considérée dans un problème de régression):\n\\[E_L[\\left(Y - \\phi(x, L)\\right)^2] = \\underbrace{\\left(E_L\\left[\\phi(x, L) - Y\\right]\\right)^2}_{\\text{Biais}^2} + \\underbrace{E_L[\\left(\\phi(x, L) - E_L[\\phi(x, L)]\\right)^2]}_{\\text{Variance}} \\tag{1}\\]\n\nLe biais est la différence entre la valeur observée \\(Y\\) que l’on souhaite prédire et la prédiction moyenne \\(E_L[\\phi(x, L)]\\). Si le modèle est sous-ajusté, le biais sera élevé.\nLa variance est la variabilité des prédictions (\\(\\phi(x, L)\\)) autour de leur moyenne (\\(E_L[\\phi(x, L)]\\)). Un modèle avec une variance élevée est très sensible aux fluctuations au sein des données d’entraînement: ses prédictions varient beaucoup lorsque les données d’entraînement se modifient.\n\nL’équation Equation 1 illustre l’arbitrage biais-variance qui est omniprésent en machine learning: plus la complexité d’un modèle s’accroît (exemple: la profondeur d’un arbre), plus son biais sera plus faible (car ses prédictions seront de plus en plus proches des données d’entraînement), et plus sa variance sera élevée (car ses prédictions, étant très proches des données d’entraînement, auront tendance à varier fortement d’un jeu d’entraînement à l’autre).\n\n\n\nBreiman (1996) compare l’erreur quadratique moyenne d’un modèle individuel avec celle du modèle agrégé et démontre l’inégalité suivante :\n\n$ (Y - _A(x))^2 E_L[(Y - (x, L))^2] $ {#eq-inegalite-breiman1996}\n\nLe terme \\((Y - \\phi_A(x))^2\\) représente l’erreur quadratique du prédicteur agrégé \\(\\phi_A(x)\\);\nLe terme \\(E_L[(Y - \\phi(x, L))^2]\\) est l’erreur quadratique moyenne d’un prédicteur individuel \\(\\phi(x, L)\\) entraîné sur un échantillon aléatoire \\(L\\). Cette erreur varie en fonction des données d’entraînement.\n\nCette inégalité montre que l’erreur quadratique moyenne du prédicteur agrégé est toujours inférieure ou égale à la moyenne des erreurs des prédicteurs individuels. Puisque le biais du prédicteur agrégé est identique au biais du prédicteur individuel, alors l’inégalité précédente implique que la variance du modèle agrégé \\(\\phi_A(x)\\) est toujours inférieure ou égale à la variance moyenne d’un modèle individuel :\n$ (_A(x)) = (E_L[(x, L)]) E_L[((x, L))] $\nAutrement dit, le processus d’agrégation réduit l’erreur de prédiction globale en réduisant la variance des prédictions, tout en conservant un biais constant.\nCe résultat ouvre la voie à des considérations pratiques immédiates. Lorsque le modèle individuel est instable et présente une variance élevée, l’inégalité \\(Var(\\phi_A(x)) \\leq E_L[Var(\\phi(x,L))]\\) est forte, ce qui signifie que l’agrégation peut améliorer significativement la performance globale du modèle. En revanche, si \\(ϕ(x,L)\\) varie peu d’un ensemble d’entraînement à un autre (modèle stable avec variance faible), alors \\(Var(\\phi_A(x))\\) est proche de \\(E_L[Var(\\phi(x,L))]\\), et la réduction de variance apportée par l’agrégation est faible. Ainsi, le bagging est particulièrement efficace pour les modèles instables, tels que les arbres de décision, mais moins efficace pour les modèles stables tels que les méthodes des k plus proches voisins.\n\n\n\n\nDans le cas de la classification, le mécanisme de réduction de la variance par le bagging permet, sous une certaine condition, d’atteindre un classificateur presque optimal (nearly optimal classifier). Ce concept a été introduit par Breiman (1996) pour décrire un modèle qui tend à classer une observation dans la classe la plus probable, avec une performance approchant celle du classificateur Bayésien optimal (la meilleure performance théorique qu’un modèle de classification puisse atteindre).\nPour comprendre ce résutlat, introduisons \\(Q(j|x) = E_L(1_{φ(x, L) = j}) = P(φ(x, L) = j)\\), la probabilité qu’un modèle \\(φ(x, L)\\) prédise la classe \\(j\\) pour l’observation \\(x\\), et \\(P(j|x)\\), la probabilité réelle (conditionnelle) que \\(x\\) appartienne à la classe \\(j\\).\n\n\nUn classificateur \\(φ(x, L)\\) est dit order-correct pour une observation \\(x\\) si, en espérance, il identifie correctement la classe la plus probable, même s’il ne prédit pas toujours avec exactitude les probabilités associées à chaque classe \\(Q(j∣x)\\).\nCela signifie que si l’on considérait tous les ensemble de données possibles, et que l’on évaluait les prédictions du modèle en \\(x\\), la majorité des prédictions correspondraient à la classe à laquelle il a la plus grande probabilité vraie d’appartenir \\(P(j∣x)\\).\nFormellement, un prédicteur est dit “order-correct” pour une entrée \\(x\\) si :\n$ argmax_j Q(j|x) = argmax_j P(j|x) $\noù \\(P(j|x)\\) est la vraie probabilité que l’observation \\(x\\) appartienne à la classe \\(j\\), et \\(Q(j|x)\\) est la probabilité que \\(x\\) appartienne à la classe \\(j\\) prédite par le modèle \\(φ(x, L)\\).\nUn classificateur est order-correct si, pour chaque observation \\(x\\), la classe qu’il prédit correspond à celle qui a la probabilité maximale \\(P(j|x)\\) dans la distribution vraie.\n\n\n\nDans le cas de la classification, le prédicteur agrégé est défini par le vote majoritaire. Cela signifie que si \\(K\\) classificateurs sont entraînés sur \\(K\\) échantillons distincts, la classe prédite pour \\(x\\) est celle qui reçoit le plus de votes de la part des modèles individuels.\nFormellement, le classificateur agrégé \\(φA(x)\\) est défini par :\n\\(φA(x) =  \\text{argmax}_j \\sum_{L} I(\\phi(x, L) = j) = argmax_j Q(j|x)\\)\n\n\n\nBreiman (1996) montre que si chaque prédicteur individuel \\(φ(x, L)\\) est order-correct pour une observation \\(x\\), alors le prédicteur agrégé \\(φA(x)\\), obtenu par vote majoritaire, atteint la performance optimale pour cette observation, c’est-à-dire qu’il converge vers la classe ayant la probabilité maximale \\(P(j∣x)\\) pour l’observation \\(x\\) lorsque le nombre de prédicteurs individuels augmente. Le vote majoritaire permet ainsi de réduire les erreurs aléatoires des classificateurs individuels.\nLe classificateur agrégé \\(ϕA\\) est optimal s’il prédit systématiquement la classe la plus probable pour l’observation \\(x\\) dans toutes les régions de l’espace.\nCependant, dans les régions de l’espace où les classificateurs individuels ne sont pas order-corrects (c’est-à-dire qu’ils se trompent majoritairement sur la classe d’appartenance), l’agrégation par vote majoritaire n’améliore pas les performances. Elles peuvent même se détériorer par rapport aux modèles individuels si l’agrégation conduit à amplifier des erreurs systématiques (biais).",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "Le bagging"
    ]
  },
  {
    "objectID": "chapters/chapter2/2-bagging.html#léchantillage-par-bootstrap-peut-détériorer-les-performances-théoriques-du-modèle-agrégé",
    "href": "chapters/chapter2/2-bagging.html#léchantillage-par-bootstrap-peut-détériorer-les-performances-théoriques-du-modèle-agrégé",
    "title": "Le bagging",
    "section": "",
    "text": "En pratique, au lieu d’utiliser tous les ensembles d’entraînement possibles \\(L\\), le bagging repose sur un nombre limité d’échantillons bootstrap tirés avec remise à partir d’un même jeu de données initial, ce qui peut introduire des biais par rapport au prédicteur agrégé théorique.\nLes échantillons bootstrap présentent les limites suivantes :\n\nUne taille effective réduite par rapport au jeu de données initial: Bien que chaque échantillon bootstrap présente le même nombre d’observations que le jeu de données initial, environ 1/3 des observations (uniques) du jeu initial sont absentes de chaque échantillon bootstrap (du fait du tirage avec remise). Cela peut limiter la capacité des modèles à capturer des relations complexes au sein des données (et aboutir à des modèles individuels sous-ajustés par rapport à ce qui serait attendu théoriquement), en particulier lorsque l’échantillon initial est de taille modeste.\nUne dépendance entre échantillons : Les échantillons bootstrap sont tirés dans le même jeu de données, ce qui génère une dépendance entre eux, qui réduit la diversité des modèles. Cela peut limiter l’efficacité de la réduction de variance dans le cas de la régression, voire acroître le biais dans le cas de la classification.\nUne couverture incomplète de l’ensemble des échantillons possibles: Les échantillons bootstrap ne couvrent pas l’ensemble des échantillons d’entraînement possibles, ce qui peut introduire un biais supplémentaire par rapport au prédicteur agrégé théorique.",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "Le bagging"
    ]
  },
  {
    "objectID": "chapters/chapter2/2-bagging.html#le-bagging-en-pratique",
    "href": "chapters/chapter2/2-bagging.html#le-bagging-en-pratique",
    "title": "Le bagging",
    "section": "",
    "text": "Le bagging est particulièrement utile lorsque les modèles individuels présentent une variance élevée et sont instables. Dans de tels cas, l’agrégation des prédictions peut réduire significativement la variance globale, améliorant ainsi la performance du modèle agrégé. Les situations où le bagging est recommandé incluent typiquement:\n\nLes modèles instables : Les modèles tels que les arbres de décision non élagués, qui sont sensibles aux variations des données d’entraînement, bénéficient grandement du bagging. L’agrégation atténue les fluctuations des prédictions dues aux différents échantillons.\nLes modèles avec biais faibles: En classification, si les modèles individuels sont order-corrects pour la majorité des observations, le bagging peut améliorer la précision en renforçant les prédictions correctes et en réduisant les erreurs aléatoires.\n\nInversement, le bagging peut être moins efficace ou même néfaste dans certaines situations :\n\nLes modèles stables avec variance faible : Si les modèles individuels sont déjà stables et présentent une faible variance (par exemple, la régression linéaire), le bagging n’apporte que peu d’amélioration, car la réduction de variance supplémentaire est minimale.\nLa présence de biais élevée : Si les modèles individuels sont biaisés, entraînant des erreurs systématiques, le bagging peut amplifier ces erreurs plutôt que de les corriger. Dans de tels cas, il est préférable de s’attaquer d’abord au biais des modèles avant de considérer l’agrégation.\nLes échantillons de petite taille : Avec des ensembles de données limités, les échantillons bootstrap peuvent ne pas être suffisamment diversifiés ou représentatifs, ce qui réduit l’efficacité du bagging et peut augmenter le biais des modèles.\n\nCe qui qu’il faut retenir: le bagging peut améliorer substantiellement la performance des modèles d’apprentissage automatique lorsqu’il est appliqué dans des conditions appropriées. Il est essentiel d’évaluer la variance et le biais des modèles individuels, ainsi que la taille et la représentativité du jeu de données, pour déterminer si le bagging est une stratégie adaptée. Lorsqu’il est utilisé judicieusement, le bagging peut conduire à des modèles plus robustes et précis, exploitant efficacement la puissance de l’agrégation pour améliorer la performance des modèles individuels.\n\n\n\n\n\n“Optimal performance is often found by bagging 50–500 trees. Data sets that have a few strong predictors typically require less trees; whereas data sets with lots of noise or multiple strong predictors may need more. Using too many trees will not lead to overfitting. However, it’s important to realize that since multiple models are being run, the more iterations you perform the more computational and time requirements you will have. As these demands increase, performing k-fold CV can become computationally burdensome.”\n\n\n\n“A benefit to creating ensembles via bagging, which is based on resampling with replacement, is that it can provide its own internal estimate of predictive performance with the out-of-bag (OOB) sample (see Section 2.4.2). The OOB sample can be used to test predictive performance and the results usually compare well compared to k-fold CV assuming your data set is sufficiently large (say n≥1,000). Consequently, as your data sets become larger and your bagging iterations increase, it is common to use the OOB error estimate as a proxy for predictive performance.”",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "Le bagging"
    ]
  },
  {
    "objectID": "chapters/chapter2/2-bagging.html#mise-en-pratique-exemple-avec-code",
    "href": "chapters/chapter2/2-bagging.html#mise-en-pratique-exemple-avec-code",
    "title": "Le bagging",
    "section": "",
    "text": "Ou bien ne commencer les mises en pratique qu’avec les random forest ?",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "Le bagging"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "",
    "text": "Introduction\nUne bien belle introduction pour le site et le DT.\nLa version pdf de ce document est disponible ici.",
    "crumbs": [
      "Introduction aux méthodes ensemblistes"
    ]
  },
  {
    "objectID": "chapters/chapter2/4-boosting.html",
    "href": "chapters/chapter2/4-boosting.html",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "",
    "text": "Le fondement théorique du boosting est un article de de 1990 (Shapire (1990)) qui a démontré théoriquement que, sous certaines conditions, il est possible de transformer un modèle prédictif peu performant en un modèle prédictif très performant. Plus précisément, un modèle ayant un pouvoir prédictif arbitrairement élevé (appelé strong learner) peut être construit en combinant des modèles simples dont les prédictions ne sont que légèrement meilleures que le hasard (appelé weak learners). Le boosting est donc une méthode qui combine une approche ensembliste reposant sur un grand nombre de modèles simples avec un entraînement séquentiel: chaque modèle simple (souvent des arbres de décision peu profonds) tâche d’améliorer la prédiction globale en corrigeant les erreurs des prédictions précédentes à chaque étape. Bien qu’une approche de boosting puisse en théorie mobiliser différentes classes de weak learners, en pratique les weak learners utilisés par les algorithmes de boosting sont presque toujours des arbres de décision.\nS’il existe plusieurs variantes, les algorithmes de boosting suivent la même logique :\n\nUn premier modèle simple et peu performant est entraîné sur les données.\nUn deuxième modèle est entraîné de façon à corriger les erreurs du premier modèle (par exemple en pondérant davantage les observations mal prédites);\nCe processus est répété en ajoutant des modèles simples, chaque modèle corrigeant les erreurs commises par l’ensemble des modèles précédents;\nTous ces modèles sont finalement combinés (souvent par une somme pondérée) pour obtenir un modèle complexe et performant.\n\nEn termes plus techniques, les algorithmes de boosting partagent trois caractéristiques communes:\n\nIls visent à trouver une approximation \\(\\hat{F}\\)“)`{=typst} d’une fonction inconnue \\(F*: \\mathbf{x} \\mapsto y\\) à partir d’un ensemble d’entraînement \\((y_i, \\mathbf{x_i})_{i= 1,\\dots,n}\\);\nIls supposent que la fonction \\(F*\\) peut être approchée par une somme pondérée de modèles simples \\(f\\) de paramètres \\(\\theta\\):\n\n\\[ F\\left(\\mathbf{x}\\right) = \\sum_{m=1}^M \\beta_m f\\left(\\mathbf{x}, \\mathbf{\\theta}_m\\right) \\]\n\nils reposent sur une modélisation additive par étapes, qui décompose l’entraînement de ce modèle complexe en une séquence d’entraînements de petits modèles. Chaque étape de l’entraînement cherche le modèle simple \\(f\\) qui améliore la puissance prédictive du modèle complet, sans modifier les modèles précédents, puis l’ajoute de façon incrémentale à ces derniers:\n\n\\[ F_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\hat{\\beta}_m f(\\mathbf{x}_i, \\mathbf{\\hat{\\theta}_m}) \\]\n\nMETTRE ICI UNE FIGURE EN UNE DIMENSION, avec des points et des modèles en escalier qui s’affinent.\n\n\n\n\n\nDans les années 1990, de nombreux travaux ont tâché de proposer des mise en application du boosting (Breiman (1998), Grove and Schuurmans (1998)) et ont comparé les mérites des différentes approches. Deux approches ressortent particulièrement de cette littérature: Adaboost (Adaptive Boosting, Freund and Schapire (1997)) et la Gradient Boosting Machine (Friedman (2001)). Ces deux approches reposent sur des principes très différents.\nLe principe d’Adaboost consiste à pondérer les erreurs commises à chaque itération en donnant plus d’importance aux observations mal prédites, de façon à obliger les modèles simples à se concentrer sur les observations les plus difficiles à prédire. Voici une esquisse du fonctionnement d’AdaBoost:\n\nUn premier modèle simple est entraîné sur un jeu d’entraînement dans lequel toutes les observations ont le même poids.\nA l’issue de cette première itération, les observations mal prédites reçoivent une pondération plus élevé que les observations bien prédites, et un deuxième modèle est entraîné sur ce jeu d’entraînement pondéré.\nCe deuxième modèle est ajouté au premier, puis on repondère à nouveau les observations en fonction de la qualité de prédiction de ce nouveau modèle.\nCette procédure est répétée en ajoutant de nouveaux modèles et en ajustant les pondérations.\n\nL’algorithme Adaboost a été au coeur de la littérature sur le boosting à la fin des années 1990 et dans les années 2000, en raison de ses performances sur les problèmes de classification binaire. Il a toutefois été progressivement remplacé par les algorithmes de gradient boosting inventé quelques années plus tard.\n\n\n\nLa Gradient Boosting Machine (GBM) propose une approche assez différente: elle introduit le gradient boosting en reformulant le boosting sous la forme d’un problème de descente de gradient. Voici une esquisse du fonctionnement de la Gradient Boosting Machine:\n\nUn premier modèle simple est entraîné sur un jeu d’entraînement, de façon à minimiser une fonction de perte qui mesure l’écart entre la variable à prédire et la prédiction du modèle.\nA l’issue de cette première itération, on calcule la dérivée partielle (gradient) de la fonction de perte par rapport à la prédiction en chaque point de l’ensemble d’entraînement. Ce gradient indique dans quelle direction et dans quelle ampleur la prédiction devrait être modifiée afin de réduire la perte.\nA la deuxième itération, on ajoute un deuxième modèle qui va tâcher d’améliorer le modèle complet en prédisant le mieux possible l’opposé de ce gradient.\nCe deuxième modèle est ajouté au premier, puis on recalcule la dérivée partielle de la fonction de perte par rapport à la prédiction de ce nouveau modèle.\nCette procédure est répétée en ajoutant de nouveaux modèles et en recalculant le gradient à chaque étape.\n\n\nL’approche de gradient boosting proposée par Friedman (2001) présente deux grands avantages. D’une part, elle peut être utilisée avec n’importe quelle fonction de perte différentiable, ce qui permet d’appliquer le gradient boosting à de multiples problèmes (régression, classification binaire ou multiclasse, learning-to-rank…). D’autre part, elle offre souvent des performances comparables ou supérieures aux autres approches de boosting. Le gradient boosting d’arbres de décision (Gradient boosted Decision Trees - GBDT) est donc devenue l’approche de référence en matière de boosting. En particulier, les implémentations modernes du gradient boosting comme XGBoost, LightGBM, et CatBoost sont des extensions et améliorations de la Gradient Boosting Machine.\n\n\n\n\nDepuis la publication de Friedman (2001), la méthode de gradient boosting a connu de multiples développements et raffinements, parmi lesquels XGBoost (Chen and Guestrin (2016)), LightGBM (Ke et al. (2017)) et CatBoost (Prokhorenkova et al. (2018)). S’il existe quelques différences entre ces implémentations, elles partagent néanmoins la même mécanique d’ensemble, que la section qui suit va présenter en détail en s’appuyant sur l’implémentation proposée par XBGoost.(^Cette partie reprend la structure et les notations de la partie 2 de Chen and Guestrin 2016.)\nChoses importantes à mettre en avant:\n\nLe boosting est fondamentalement différent des forêts aléatoires. See ESL, chapitre 10.\nToute la mécanique est indépendante de la fonction de perte choisie. En particulier, elle est applicable indifféremment à des problèmes de classification et de régression.\nLes poids sont calculés par une formule explicite, ce qui rend les calculs extrêmement rapides.\nComment on interprète le gradient et la hessienne: cas avec une fonction de perte quadratique.\nLe boosting est fait pour overfitter; contrairement aux RF, il n’y a pas de limite à l’overfitting. Donc lutter contre le surapprentissage est un élément particulièrement important de l’usage des algorithmes de boosting.\n\n\n\nOn veut entraîner un modèle comprenant \\(K\\) arbres de régression ou de classification:\n\\[\\hat{y}_{i} = \\phi\\left(\\mathbf{x}_i\\right) = \\sum_{k=1}^{K} f_k\\left(\\mathbf{x}_i\\right) \\]\nChaque arbre \\(f\\) est défini par trois paramètres:\n\nsa structure qui est une fonction \\(q: \\mathbb{R}^m \\rightarrow \\{1, \\dots, T\\}\\) qui à un vecteur d’inputs \\(\\mathbf{x}\\) de dimension \\(m\\) associe une feuille terminale de l’arbre);\nson nombre de feuilles terminales \\(T\\);\nles valeurs figurant sur ses feuilles terminales \\(\\mathbf{w}\\in \\mathbb{R}^T\\) (appelées poids ou weights).\n\nLe modèle est entraîné avec une fonction-objectif constituée d’une fonction de perte \\(l\\) et d’une fonction de régularisation \\(\\Omega\\). La fonction de perte mesure la distance entre la prédiction \\(hat(y)\\) et la vraie valeur \\(y\\) et présente généralement les propriétés suivantes: elle est convexe et dérivable deux fois, et atteint son minimum lorsque \\(hat(y) = y\\). La fonction de régularisation pénalise la complexité du modèle. Dans le cas présent, elle pénalise les arbres avec un grand nombre de feuilles (\\(T\\) élevé) et les arbres avec des poids élevés (\\(w_t\\) élevés en valeur absolue).\n\\[ \\mathcal{L}(\\phi) = \\underbrace{\\sum_i l(\\hat{y}_{i}, y_{i})}_{\\substack{\\text{Perte sur les} \\\\ \\text{observations}}} + \\underbrace{\\sum_k \\Omega(f_{k})}_{\\substack{\\text{Fonction de} \\\\ \\text{régularisation}}}\\text{avec}\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{t=1}^T w_j^2  \\tag{1}\\]\n\n\n\nLa fonction-objectif introduite précédemment est très complexe et ne peut être utilisée directement pour entraîner le modèle, car il faudrait entraîner tous les arbres en même temps. On va donc reformuler donc cette fonction objectif de façon à isoler le \\(k\\)-ième arbre, qui pourra ensuite être entraîné seul, une fois que les \\(k-1\\) arbres précédents auront été entraînés. Pour cela, on note \\(\\hat{y}_i^{(k)}\\) la prédiction à l’issue de l’étape \\(k\\): \\(\\hat{y}_i^{(k)} = \\sum_{j=1}^t f_j(\\mathbf{x}_i)\\), et on $1 la fonction-objectif \\(\\mathcal{L}^{(k)}\\) au moment de l’entraînement du \\(k\\)-ième arbre:\n\\[\n\\begin{aligned}\n\\mathcal{L}^{(t)}\n&= \\sum_{i=1}^{n} l(y_i, \\hat{y}_{i}^{(t)}) + \\sum_{k=1}^t\\Omega(f_k) \\\\\n&= \\sum_{i=1}^{n} l\\left(y_i, \\hat{y}_{i}^{(t-1)} + f_{t}(\\mathbf{x}_i)\\right) + \\Omega(f_t) + constant\n\\end{aligned}\n\\]\n\n\n\nUne fois isolé le \\(k\\)-ième arbre, on fait un développement limité d’ordre 2 de \\(l(y_i, \\hat{y}_{i}^{(k-1)} + f_{k}(\\mathbf{x}_i))\\) au voisinage de \\(\\hat{y}_{i}^{(k-1)}\\), en considérant que la prédiction du \\(k\\)-ième arbre \\(f_{k}(\\mathbf{x}_i)\\) est\n\\[ \\mathcal{L}^{(t)} \\approx \\sum_{i=1}^{n} [l(y_i, \\hat{y}_{i}^{(t-1)}) + g_i f_t(\\mathbf{x}_i)+ \\frac{1}{2} h_i f^2_t(\\mathbf{x}_i)] + \\Omega(f_t) \\]\navec\n\\[ g_i = \\frac{\\partial l(y_i, \\hat{y}_i^{(t-1)})}{\\partial\\hat{y}_i^{(t-1)}} \\text{et} h_i = \\frac{\\partial^2 l(y_i, \\hat{y}_i^{(t-1)})}{{\\partial \\hat{y}_i^{(t-1)}}^2} \\]\nLes termes \\(g_i\\) et \\(h_i\\) désignent respectivement la dérivée première (le gradient) et la dérivée seconde (la hessienne) de la fonction de perte par rapport à la variable prédite. Il est important de noter que les termes (A) et (B) sont constants car les \\(k-1\\) arbres précédents ont déjà été entraînés et ne sont pas modifiés par l’entraînement du \\(k\\)-ième arbre.  On peut donc retirer ces termes pour obtenir la fonction-objectif simplifiée qui sera utilisée pour l’entraînement du \\(k\\)-ième arbre.\n\\[ \\mathcal{\\tilde{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i)+ \\frac{1}{2} h_i f^2_t(\\mathbf{x}_i)] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_i^2 \\]\nCette expression est importante car elle montre qu’on est passé d’un problème complexe où il fallait entraîner un grand nombre d’arbres simultanément (équation Equation 1) à un problème beaucoup plus simple dans lequel il n’y a qu’un seul arbre à entraîner.\n\n\n\nA partir de l’expression précédente, il est possible de faire apparaître les poids \\(w_j\\) du \\(k\\)-ième arbre. Pour une structure d’arbre donnée (\\(q: \\mathbb{R}^m \\rightarrow \\{1, \\dots, T\\}\\)), on définit \\(I_j = \\{ i | q(\\mathbf{x}_i) = j \\}\\) l’ensemble des observations situées sur la feuille \\(j\\) puis on réorganise \\(\\mathcal{\\tilde{L}}^{(k)}\\):\n\\[\n\\begin{align*}\n\\mathcal{\\tilde{L}}^{(t)} =&   \\sum_{j=1}^{T} \\sum_{i\\in I_{j}} \\bigg[g_i f_t(\\mathbf{x}_i)\\phantom{\\frac{1}{2}} &+ \\frac{1}{2} h_i f^2_t(\\mathbf{x}_i)\\bigg]&+ \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_i^2 \\\\\n     &= \\sum_{j=1}^{T} \\sum_{i\\in I_{j}} \\bigg[g_i w_j &+ \\frac{1}{2} h_i w_j^2\\bigg] &+ \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_i^2 \\\\\n     &= \\sum^T_{j=1} \\bigg[w_j\\sum_{i\\in I_{j}} g_i &+ \\frac{1}{2} w_j^2 \\sum_{i \\in I_{j}} h_i + \\lambda \\bigg] &+ \\gamma T\n\\end{align*}\n\\]\nDans la dernière expression, on voit que la fonction de perte simplifiée se reformule comme une combinaison quadratique des poids \\(w_j\\), dans laquelle les dérivées première et seconde de la fonction de perte interviennent sous forme de pondérations. Tout l’enjeu de l’entraînement devient donc de trouver les poids optimaux \\(w_j\\) qui minimiseront cette fonction de perte, compte tenu de ces opérations.\nIl se trouve que le calculs de ces poids optimaux est très simple: pour une structure d’arbre donnée (\\(q: \\mathbb{R}^m \\rightarrow \\{1, \\dots, T\\}\\)), le poids optimal w_j^{} de la feuille \\(j\\) est donné par l’équation:\nPar conséquent, la valeur optimale de la fonction objectif pour l’arbre \\(q\\) est égale à\n\\[\\mathcal{\\tilde{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^T \\frac{\\left(\\sum_{i\\in I_j} g_i\\right)^2}{\\sum_{i\\in I_j} h_i+\\lambda} + \\gamma T \\tag{2}\\]\nCette équation est utile car elle permet de comparer simplement la qualité de deux arbres, et de déterminer lequel est le meilleur.\n\n\n\nDans la mesure où elle permet de comparer des arbres, on pourrait penser que l’équation Equation 2 est suffisante pour choisir directement le \\(k\\)-ième arbre: il suffirait d’énumérer les arbres possibles, de calculer la qualité de chacun d’entre eux, et de retenir le meilleur. Bien que cette approche soit possible théoriquement, elle est inemployable en pratique car le nombre d’arbres possibles est extrêmement élevé. Par conséquent, le \\(k\\)-ième arbre n’est pas défini en une fois, mais construit de façon gloutonne:\nREFERENCE A LA PARTIE CART/RF?\n\non commence par le noeud racine et on cherche le split qui réduit au maximum la perte en séparant les données d’entraînement entre les deux noeuds-enfants.\npour chaque noeud enfant, on cherche le split qui réduit au maximum la perte en séparant en deux la population de chacun de ces noeuds.\nCette procédure recommence jusqu’à que l’arbre ait atteint sa taille maximale (définie par une combinaison d’hyperparamètres d$1s dans la partie référence à ajouter).\n\n\n\n\nTraduire split par critère de partition?\nReste à comprendre comment le critère de partition optimal est choisi à chaque étape de la construction de l’arbre. Imaginons qu’on envisage de décomposer la feuille \\(I\\) en deux nouvelles feuilles \\(I_L\\) et \\(I_R\\) (avec \\(I = I_L \\cup I_R\\)), selon une condition logique reposant sur une variable et une valeur de cette variable (exemple: \\(x_6 &gt; 11\\)). Par application de l’équation Equation 2, le gain potentiel induit par ce critère de partition est égal à:\n\\[ Gain = \\frac{1}{2} \\left[\\frac{G_L^2}{H_L+\\lambda}+\\frac{G_R^2}{H_R+\\lambda}-\\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda}\\right] - \\gamma  \\tag{3}\\]\n\nCette dernière équation est au coeur de la mécanique du gradient boosting car elle permet de comparer les critères de partition possibles. Plus précisément, l’algorithme de détermination des critère de partition (split finding algorithm) consiste en une double boucle sur les variables et les valeurs prises par ces variables, qui énumère un grand nombre de critères de partition et mesure le gain associé à chacun d’entre eux avec l’équation Equation 3. Le critère de partition retenu est simplement celui dont le gain est le plus élevé.\nL’algorithme qui détermine les critère de partition est un enjeu de performance essentiel dans le gradient boosting. En effet, utiliser l’algorithme le plus simple (énumérer tous les critères de partition possibles, en balayant toutes les valeurs de toutes les variables) s’avère très coûteux dès lors que les données contiennent soit un grand nombre de variables, soit des variables continues prenant un grand nombre de valeurs. C’est pourquoi les algorithmes de détermination des critère de partition ont fait l’objet de multiples améliorations et optimisations visant à réduire leur coût computationnel sans dégrader la qualité des critères de partition.\n\n\n\n\n\n\nle shrinkage;\nle subsampling des lignes et des colonnes;\nles différentes pénalisations.\n\n\n\n\n\nLes principaux hyperparamètres d’XGBoost\n\n\n\n\n\n\n\nHyperparamètre\nDescription\nValeur par défaut\n\n\n\n\nbooster\nLe type de weak learner utilisé\n'gbtree'\n\n\nlearning_rate\nLe taux d’apprentissage\n0.3\n\n\nmax_depth\nLa profondeur maximale des arbres\n6\n\n\nmax_leaves\nLe nombre maximal de feuilles des arbres\n0\n\n\nmin_child_weight\nLe poids minimal qu’une feuille doit contenir\n1\n\n\nn_estimators\nLe nombre d’arbres\n100\n\n\nlambda ou reg_lambda\nLa pénalisation L2\n1\n\n\nalpha ou reg_alpha\nLa pénalisation L1\n0\n\n\ngamma\nLe gain minimal nécessaire pour ajouter un noeud supplémentaire\n0\n\n\ntree_method\nLa méthode utilisée pour rechercher les splits\n'hist'\n\n\nmax_bin\nLe nombre utilisés pour discrétiser les variables continues\n0\n\n\nsubsample\nLe taux d’échantillonnage des données d’entraîenment\n1\n\n\nsampling_method\nLa méthode utilisée pour échantillonner les données d’entraînement\n'uniform'\n\n\ncolsample_bytree  colsample_bylevel  colsample_bynode\nTaux d’échantillonnage des colonnes par arbre, par niveau et par noeud\n1, 1 et 1\n\n\nscale_pos_weight\nLe poids des observations de la classe positive (classification uniquement)\n1\n\n\nsample_weight\nLa pondération des données d’entraînement\n1\n\n\nenable_categorical\nActiver le support des variables catégorielles\nFalse\n\n\nmax_cat_to_onehot\nNombre de modalités en-deça duquel XGBoost utilise le one-hot-encoding\nA COMPLETER\n\n\nmax_cat_threshold\nNombre maximal de catégories considérées dans le partitionnement optimal des variables catégorielles\nA COMPLETER\n\n\n\n\n\n\n\n\nles variables catégorielles:\n\nordonnées: passer en integer;\nnon-ordonnées: OHE ou approche de Fisher.\n\nles variables continues:\n\ninutile de faire des transformations monotones.\nUtile d’ajouter des transformations non monotones.\n\n\n\n\n\n\n\n\n\nSource: Probst, Wright, and Boulesteix (2019)\n\nstructure of each individual tree:\n\ndudu\ndudu\ndudu\n\nstructure and size of the forest:\nThe level of randomness (je dirais plutôt : )",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "Le *boosting*"
    ]
  },
  {
    "objectID": "chapters/chapter2/4-boosting.html#le-boosting",
    "href": "chapters/chapter2/4-boosting.html#le-boosting",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "",
    "text": "Le fondement théorique du boosting est un article de de 1990 (Shapire (1990)) qui a démontré théoriquement que, sous certaines conditions, il est possible de transformer un modèle prédictif peu performant en un modèle prédictif très performant. Plus précisément, un modèle ayant un pouvoir prédictif arbitrairement élevé (appelé strong learner) peut être construit en combinant des modèles simples dont les prédictions ne sont que légèrement meilleures que le hasard (appelé weak learners). Le boosting est donc une méthode qui combine une approche ensembliste reposant sur un grand nombre de modèles simples avec un entraînement séquentiel: chaque modèle simple (souvent des arbres de décision peu profonds) tâche d’améliorer la prédiction globale en corrigeant les erreurs des prédictions précédentes à chaque étape. Bien qu’une approche de boosting puisse en théorie mobiliser différentes classes de weak learners, en pratique les weak learners utilisés par les algorithmes de boosting sont presque toujours des arbres de décision.\nS’il existe plusieurs variantes, les algorithmes de boosting suivent la même logique :\n\nUn premier modèle simple et peu performant est entraîné sur les données.\nUn deuxième modèle est entraîné de façon à corriger les erreurs du premier modèle (par exemple en pondérant davantage les observations mal prédites);\nCe processus est répété en ajoutant des modèles simples, chaque modèle corrigeant les erreurs commises par l’ensemble des modèles précédents;\nTous ces modèles sont finalement combinés (souvent par une somme pondérée) pour obtenir un modèle complexe et performant.\n\nEn termes plus techniques, les algorithmes de boosting partagent trois caractéristiques communes:\n\nIls visent à trouver une approximation \\(\\hat{F}\\)“)`{=typst} d’une fonction inconnue \\(F*: \\mathbf{x} \\mapsto y\\) à partir d’un ensemble d’entraînement \\((y_i, \\mathbf{x_i})_{i= 1,\\dots,n}\\);\nIls supposent que la fonction \\(F*\\) peut être approchée par une somme pondérée de modèles simples \\(f\\) de paramètres \\(\\theta\\):\n\n\\[ F\\left(\\mathbf{x}\\right) = \\sum_{m=1}^M \\beta_m f\\left(\\mathbf{x}, \\mathbf{\\theta}_m\\right) \\]\n\nils reposent sur une modélisation additive par étapes, qui décompose l’entraînement de ce modèle complexe en une séquence d’entraînements de petits modèles. Chaque étape de l’entraînement cherche le modèle simple \\(f\\) qui améliore la puissance prédictive du modèle complet, sans modifier les modèles précédents, puis l’ajoute de façon incrémentale à ces derniers:\n\n\\[ F_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\hat{\\beta}_m f(\\mathbf{x}_i, \\mathbf{\\hat{\\theta}_m}) \\]\n\nMETTRE ICI UNE FIGURE EN UNE DIMENSION, avec des points et des modèles en escalier qui s’affinent.\n\n\n\n\n\nDans les années 1990, de nombreux travaux ont tâché de proposer des mise en application du boosting (Breiman (1998), Grove and Schuurmans (1998)) et ont comparé les mérites des différentes approches. Deux approches ressortent particulièrement de cette littérature: Adaboost (Adaptive Boosting, Freund and Schapire (1997)) et la Gradient Boosting Machine (Friedman (2001)). Ces deux approches reposent sur des principes très différents.\nLe principe d’Adaboost consiste à pondérer les erreurs commises à chaque itération en donnant plus d’importance aux observations mal prédites, de façon à obliger les modèles simples à se concentrer sur les observations les plus difficiles à prédire. Voici une esquisse du fonctionnement d’AdaBoost:\n\nUn premier modèle simple est entraîné sur un jeu d’entraînement dans lequel toutes les observations ont le même poids.\nA l’issue de cette première itération, les observations mal prédites reçoivent une pondération plus élevé que les observations bien prédites, et un deuxième modèle est entraîné sur ce jeu d’entraînement pondéré.\nCe deuxième modèle est ajouté au premier, puis on repondère à nouveau les observations en fonction de la qualité de prédiction de ce nouveau modèle.\nCette procédure est répétée en ajoutant de nouveaux modèles et en ajustant les pondérations.\n\nL’algorithme Adaboost a été au coeur de la littérature sur le boosting à la fin des années 1990 et dans les années 2000, en raison de ses performances sur les problèmes de classification binaire. Il a toutefois été progressivement remplacé par les algorithmes de gradient boosting inventé quelques années plus tard.\n\n\n\nLa Gradient Boosting Machine (GBM) propose une approche assez différente: elle introduit le gradient boosting en reformulant le boosting sous la forme d’un problème de descente de gradient. Voici une esquisse du fonctionnement de la Gradient Boosting Machine:\n\nUn premier modèle simple est entraîné sur un jeu d’entraînement, de façon à minimiser une fonction de perte qui mesure l’écart entre la variable à prédire et la prédiction du modèle.\nA l’issue de cette première itération, on calcule la dérivée partielle (gradient) de la fonction de perte par rapport à la prédiction en chaque point de l’ensemble d’entraînement. Ce gradient indique dans quelle direction et dans quelle ampleur la prédiction devrait être modifiée afin de réduire la perte.\nA la deuxième itération, on ajoute un deuxième modèle qui va tâcher d’améliorer le modèle complet en prédisant le mieux possible l’opposé de ce gradient.\nCe deuxième modèle est ajouté au premier, puis on recalcule la dérivée partielle de la fonction de perte par rapport à la prédiction de ce nouveau modèle.\nCette procédure est répétée en ajoutant de nouveaux modèles et en recalculant le gradient à chaque étape.\n\n\nL’approche de gradient boosting proposée par Friedman (2001) présente deux grands avantages. D’une part, elle peut être utilisée avec n’importe quelle fonction de perte différentiable, ce qui permet d’appliquer le gradient boosting à de multiples problèmes (régression, classification binaire ou multiclasse, learning-to-rank…). D’autre part, elle offre souvent des performances comparables ou supérieures aux autres approches de boosting. Le gradient boosting d’arbres de décision (Gradient boosted Decision Trees - GBDT) est donc devenue l’approche de référence en matière de boosting. En particulier, les implémentations modernes du gradient boosting comme XGBoost, LightGBM, et CatBoost sont des extensions et améliorations de la Gradient Boosting Machine.\n\n\n\n\nDepuis la publication de Friedman (2001), la méthode de gradient boosting a connu de multiples développements et raffinements, parmi lesquels XGBoost (Chen and Guestrin (2016)), LightGBM (Ke et al. (2017)) et CatBoost (Prokhorenkova et al. (2018)). S’il existe quelques différences entre ces implémentations, elles partagent néanmoins la même mécanique d’ensemble, que la section qui suit va présenter en détail en s’appuyant sur l’implémentation proposée par XBGoost.(^Cette partie reprend la structure et les notations de la partie 2 de Chen and Guestrin 2016.)\nChoses importantes à mettre en avant:\n\nLe boosting est fondamentalement différent des forêts aléatoires. See ESL, chapitre 10.\nToute la mécanique est indépendante de la fonction de perte choisie. En particulier, elle est applicable indifféremment à des problèmes de classification et de régression.\nLes poids sont calculés par une formule explicite, ce qui rend les calculs extrêmement rapides.\nComment on interprète le gradient et la hessienne: cas avec une fonction de perte quadratique.\nLe boosting est fait pour overfitter; contrairement aux RF, il n’y a pas de limite à l’overfitting. Donc lutter contre le surapprentissage est un élément particulièrement important de l’usage des algorithmes de boosting.\n\n\n\nOn veut entraîner un modèle comprenant \\(K\\) arbres de régression ou de classification:\n\\[\\hat{y}_{i} = \\phi\\left(\\mathbf{x}_i\\right) = \\sum_{k=1}^{K} f_k\\left(\\mathbf{x}_i\\right) \\]\nChaque arbre \\(f\\) est défini par trois paramètres:\n\nsa structure qui est une fonction \\(q: \\mathbb{R}^m \\rightarrow \\{1, \\dots, T\\}\\) qui à un vecteur d’inputs \\(\\mathbf{x}\\) de dimension \\(m\\) associe une feuille terminale de l’arbre);\nson nombre de feuilles terminales \\(T\\);\nles valeurs figurant sur ses feuilles terminales \\(\\mathbf{w}\\in \\mathbb{R}^T\\) (appelées poids ou weights).\n\nLe modèle est entraîné avec une fonction-objectif constituée d’une fonction de perte \\(l\\) et d’une fonction de régularisation \\(\\Omega\\). La fonction de perte mesure la distance entre la prédiction \\(hat(y)\\) et la vraie valeur \\(y\\) et présente généralement les propriétés suivantes: elle est convexe et dérivable deux fois, et atteint son minimum lorsque \\(hat(y) = y\\). La fonction de régularisation pénalise la complexité du modèle. Dans le cas présent, elle pénalise les arbres avec un grand nombre de feuilles (\\(T\\) élevé) et les arbres avec des poids élevés (\\(w_t\\) élevés en valeur absolue).\n\\[ \\mathcal{L}(\\phi) = \\underbrace{\\sum_i l(\\hat{y}_{i}, y_{i})}_{\\substack{\\text{Perte sur les} \\\\ \\text{observations}}} + \\underbrace{\\sum_k \\Omega(f_{k})}_{\\substack{\\text{Fonction de} \\\\ \\text{régularisation}}}\\text{avec}\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{t=1}^T w_j^2  \\tag{1}\\]\n\n\n\nLa fonction-objectif introduite précédemment est très complexe et ne peut être utilisée directement pour entraîner le modèle, car il faudrait entraîner tous les arbres en même temps. On va donc reformuler donc cette fonction objectif de façon à isoler le \\(k\\)-ième arbre, qui pourra ensuite être entraîné seul, une fois que les \\(k-1\\) arbres précédents auront été entraînés. Pour cela, on note \\(\\hat{y}_i^{(k)}\\) la prédiction à l’issue de l’étape \\(k\\): \\(\\hat{y}_i^{(k)} = \\sum_{j=1}^t f_j(\\mathbf{x}_i)\\), et on $1 la fonction-objectif \\(\\mathcal{L}^{(k)}\\) au moment de l’entraînement du \\(k\\)-ième arbre:\n\\[\n\\begin{aligned}\n\\mathcal{L}^{(t)}\n&= \\sum_{i=1}^{n} l(y_i, \\hat{y}_{i}^{(t)}) + \\sum_{k=1}^t\\Omega(f_k) \\\\\n&= \\sum_{i=1}^{n} l\\left(y_i, \\hat{y}_{i}^{(t-1)} + f_{t}(\\mathbf{x}_i)\\right) + \\Omega(f_t) + constant\n\\end{aligned}\n\\]\n\n\n\nUne fois isolé le \\(k\\)-ième arbre, on fait un développement limité d’ordre 2 de \\(l(y_i, \\hat{y}_{i}^{(k-1)} + f_{k}(\\mathbf{x}_i))\\) au voisinage de \\(\\hat{y}_{i}^{(k-1)}\\), en considérant que la prédiction du \\(k\\)-ième arbre \\(f_{k}(\\mathbf{x}_i)\\) est\n\\[ \\mathcal{L}^{(t)} \\approx \\sum_{i=1}^{n} [l(y_i, \\hat{y}_{i}^{(t-1)}) + g_i f_t(\\mathbf{x}_i)+ \\frac{1}{2} h_i f^2_t(\\mathbf{x}_i)] + \\Omega(f_t) \\]\navec\n\\[ g_i = \\frac{\\partial l(y_i, \\hat{y}_i^{(t-1)})}{\\partial\\hat{y}_i^{(t-1)}} \\text{et} h_i = \\frac{\\partial^2 l(y_i, \\hat{y}_i^{(t-1)})}{{\\partial \\hat{y}_i^{(t-1)}}^2} \\]\nLes termes \\(g_i\\) et \\(h_i\\) désignent respectivement la dérivée première (le gradient) et la dérivée seconde (la hessienne) de la fonction de perte par rapport à la variable prédite. Il est important de noter que les termes (A) et (B) sont constants car les \\(k-1\\) arbres précédents ont déjà été entraînés et ne sont pas modifiés par l’entraînement du \\(k\\)-ième arbre.  On peut donc retirer ces termes pour obtenir la fonction-objectif simplifiée qui sera utilisée pour l’entraînement du \\(k\\)-ième arbre.\n\\[ \\mathcal{\\tilde{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i)+ \\frac{1}{2} h_i f^2_t(\\mathbf{x}_i)] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_i^2 \\]\nCette expression est importante car elle montre qu’on est passé d’un problème complexe où il fallait entraîner un grand nombre d’arbres simultanément (équation Equation 1) à un problème beaucoup plus simple dans lequel il n’y a qu’un seul arbre à entraîner.\n\n\n\nA partir de l’expression précédente, il est possible de faire apparaître les poids \\(w_j\\) du \\(k\\)-ième arbre. Pour une structure d’arbre donnée (\\(q: \\mathbb{R}^m \\rightarrow \\{1, \\dots, T\\}\\)), on définit \\(I_j = \\{ i | q(\\mathbf{x}_i) = j \\}\\) l’ensemble des observations situées sur la feuille \\(j\\) puis on réorganise \\(\\mathcal{\\tilde{L}}^{(k)}\\):\n\\[\n\\begin{align*}\n\\mathcal{\\tilde{L}}^{(t)} =&   \\sum_{j=1}^{T} \\sum_{i\\in I_{j}} \\bigg[g_i f_t(\\mathbf{x}_i)\\phantom{\\frac{1}{2}} &+ \\frac{1}{2} h_i f^2_t(\\mathbf{x}_i)\\bigg]&+ \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_i^2 \\\\\n     &= \\sum_{j=1}^{T} \\sum_{i\\in I_{j}} \\bigg[g_i w_j &+ \\frac{1}{2} h_i w_j^2\\bigg] &+ \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_i^2 \\\\\n     &= \\sum^T_{j=1} \\bigg[w_j\\sum_{i\\in I_{j}} g_i &+ \\frac{1}{2} w_j^2 \\sum_{i \\in I_{j}} h_i + \\lambda \\bigg] &+ \\gamma T\n\\end{align*}\n\\]\nDans la dernière expression, on voit que la fonction de perte simplifiée se reformule comme une combinaison quadratique des poids \\(w_j\\), dans laquelle les dérivées première et seconde de la fonction de perte interviennent sous forme de pondérations. Tout l’enjeu de l’entraînement devient donc de trouver les poids optimaux \\(w_j\\) qui minimiseront cette fonction de perte, compte tenu de ces opérations.\nIl se trouve que le calculs de ces poids optimaux est très simple: pour une structure d’arbre donnée (\\(q: \\mathbb{R}^m \\rightarrow \\{1, \\dots, T\\}\\)), le poids optimal w_j^{} de la feuille \\(j\\) est donné par l’équation:\nPar conséquent, la valeur optimale de la fonction objectif pour l’arbre \\(q\\) est égale à\n\\[\\mathcal{\\tilde{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^T \\frac{\\left(\\sum_{i\\in I_j} g_i\\right)^2}{\\sum_{i\\in I_j} h_i+\\lambda} + \\gamma T \\tag{2}\\]\nCette équation est utile car elle permet de comparer simplement la qualité de deux arbres, et de déterminer lequel est le meilleur.\n\n\n\nDans la mesure où elle permet de comparer des arbres, on pourrait penser que l’équation Equation 2 est suffisante pour choisir directement le \\(k\\)-ième arbre: il suffirait d’énumérer les arbres possibles, de calculer la qualité de chacun d’entre eux, et de retenir le meilleur. Bien que cette approche soit possible théoriquement, elle est inemployable en pratique car le nombre d’arbres possibles est extrêmement élevé. Par conséquent, le \\(k\\)-ième arbre n’est pas défini en une fois, mais construit de façon gloutonne:\nREFERENCE A LA PARTIE CART/RF?\n\non commence par le noeud racine et on cherche le split qui réduit au maximum la perte en séparant les données d’entraînement entre les deux noeuds-enfants.\npour chaque noeud enfant, on cherche le split qui réduit au maximum la perte en séparant en deux la population de chacun de ces noeuds.\nCette procédure recommence jusqu’à que l’arbre ait atteint sa taille maximale (définie par une combinaison d’hyperparamètres d$1s dans la partie référence à ajouter).\n\n\n\n\nTraduire split par critère de partition?\nReste à comprendre comment le critère de partition optimal est choisi à chaque étape de la construction de l’arbre. Imaginons qu’on envisage de décomposer la feuille \\(I\\) en deux nouvelles feuilles \\(I_L\\) et \\(I_R\\) (avec \\(I = I_L \\cup I_R\\)), selon une condition logique reposant sur une variable et une valeur de cette variable (exemple: \\(x_6 &gt; 11\\)). Par application de l’équation Equation 2, le gain potentiel induit par ce critère de partition est égal à:\n\\[ Gain = \\frac{1}{2} \\left[\\frac{G_L^2}{H_L+\\lambda}+\\frac{G_R^2}{H_R+\\lambda}-\\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda}\\right] - \\gamma  \\tag{3}\\]\n\nCette dernière équation est au coeur de la mécanique du gradient boosting car elle permet de comparer les critères de partition possibles. Plus précisément, l’algorithme de détermination des critère de partition (split finding algorithm) consiste en une double boucle sur les variables et les valeurs prises par ces variables, qui énumère un grand nombre de critères de partition et mesure le gain associé à chacun d’entre eux avec l’équation Equation 3. Le critère de partition retenu est simplement celui dont le gain est le plus élevé.\nL’algorithme qui détermine les critère de partition est un enjeu de performance essentiel dans le gradient boosting. En effet, utiliser l’algorithme le plus simple (énumérer tous les critères de partition possibles, en balayant toutes les valeurs de toutes les variables) s’avère très coûteux dès lors que les données contiennent soit un grand nombre de variables, soit des variables continues prenant un grand nombre de valeurs. C’est pourquoi les algorithmes de détermination des critère de partition ont fait l’objet de multiples améliorations et optimisations visant à réduire leur coût computationnel sans dégrader la qualité des critères de partition.\n\n\n\n\n\n\nle shrinkage;\nle subsampling des lignes et des colonnes;\nles différentes pénalisations.\n\n\n\n\n\nLes principaux hyperparamètres d’XGBoost\n\n\n\n\n\n\n\nHyperparamètre\nDescription\nValeur par défaut\n\n\n\n\nbooster\nLe type de weak learner utilisé\n'gbtree'\n\n\nlearning_rate\nLe taux d’apprentissage\n0.3\n\n\nmax_depth\nLa profondeur maximale des arbres\n6\n\n\nmax_leaves\nLe nombre maximal de feuilles des arbres\n0\n\n\nmin_child_weight\nLe poids minimal qu’une feuille doit contenir\n1\n\n\nn_estimators\nLe nombre d’arbres\n100\n\n\nlambda ou reg_lambda\nLa pénalisation L2\n1\n\n\nalpha ou reg_alpha\nLa pénalisation L1\n0\n\n\ngamma\nLe gain minimal nécessaire pour ajouter un noeud supplémentaire\n0\n\n\ntree_method\nLa méthode utilisée pour rechercher les splits\n'hist'\n\n\nmax_bin\nLe nombre utilisés pour discrétiser les variables continues\n0\n\n\nsubsample\nLe taux d’échantillonnage des données d’entraîenment\n1\n\n\nsampling_method\nLa méthode utilisée pour échantillonner les données d’entraînement\n'uniform'\n\n\ncolsample_bytree  colsample_bylevel  colsample_bynode\nTaux d’échantillonnage des colonnes par arbre, par niveau et par noeud\n1, 1 et 1\n\n\nscale_pos_weight\nLe poids des observations de la classe positive (classification uniquement)\n1\n\n\nsample_weight\nLa pondération des données d’entraînement\n1\n\n\nenable_categorical\nActiver le support des variables catégorielles\nFalse\n\n\nmax_cat_to_onehot\nNombre de modalités en-deça duquel XGBoost utilise le one-hot-encoding\nA COMPLETER\n\n\nmax_cat_threshold\nNombre maximal de catégories considérées dans le partitionnement optimal des variables catégorielles\nA COMPLETER\n\n\n\n\n\n\n\n\nles variables catégorielles:\n\nordonnées: passer en integer;\nnon-ordonnées: OHE ou approche de Fisher.\n\nles variables continues:\n\ninutile de faire des transformations monotones.\nUtile d’ajouter des transformations non monotones.\n\n\n\n\n\n\n\n\n\nSource: Probst, Wright, and Boulesteix (2019)\n\nstructure of each individual tree:\n\ndudu\ndudu\ndudu\n\nstructure and size of the forest:\nThe level of randomness (je dirais plutôt : )",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "Le *boosting*"
    ]
  },
  {
    "objectID": "chapters/chapter3/1-guide_usage_RF.html",
    "href": "chapters/chapter3/1-guide_usage_RF.html",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "",
    "text": "Cette section rassemble et synthétise des recommandations sur l’entraînement des forêts aléatoires disponibles dans la littérature, en particulier dans Probst, Wright, and Boulesteix (2019).\n\n\nLe processus pour construire une Random Forest se résume comme suit:\n\nSélectionnez le nombre d’arbres à construire (n_trees).\nPour chaque arbre, effectuez les étapes suivantes :\n\nGénérer un échantillon bootstrap à partir du jeu de données.\nConstruire un arbre de décision à partir de cet échantillon:\nÀ chaque nœud de l’arbre, sélectionner un sous-ensemble aléatoire de caractéristiques (mtry).\nTrouver la meilleure division parmi ce sous-ensemble et créer des nœuds enfants.\nArrêter la croissance de l’arbre selon des critères de fin spécifiques (comme une taille minimale de nœud), mais sans élaguer l’arbre.\n\nAgréger les arbres pour effectuer les prédictions finales :\n\nRégression : la prédiction finale est la moyenne des prédictions de tous les arbres.\nClassification : chaque arbre vote pour une classe, et la classe majoritaire est retenue.\n\n\n\n\n\nIl existe de multiples implémentations des forêts aléatoires. Le présent document présente et recommande l’usage de deux implémentations de référence: le package R ranger et le package Python scikit-learn. Il est à noter qu’il est possible d’entraîner des forêts aléatoires avec les algorithmes XGBoost et LightGBM, mais il s’agit d’un usage avancé qui n’est recommandé en première approche. Cette approche est présentée dans la partie REFERENCE A LA PARTIE USAGE AVANCE.\n\n\n\nCette section décrit en détail les principaux hyperparamètres des forêts aléatoires listés dans le tableau . Les noms des hyperparamètres utilisés sont ceux figurant dans le package R ranger, et dans le package Python scikit-learn. Il arrive qu’ils portent un nom différent dans d’autres implémentations des random forests, mais il est généralement facile de s’y retrouver en lisant attentivement la documentation.\n\nLes principaux hyperparamètres des forêts aléatoires\n\n\n\n\n\n\nHyperparamètre (ranger / scikit-learn)\nDescription\n\n\n\n\nmtry / max_features\nLe nombre de variables candidates à chaque noeud\n\n\nreplacement / absent\nL’échantillonnage des données se fait-il avec ou sans remise?\n\n\nsample.fraction / max_samples\nLe taux d’échantillonnage des données\n\n\nmin.node.size / min_samples_leaf\nNombre minimal d’observations nécessaire pour qu’un noeud puisse être partagé\n\n\nnum.trees / n_estimators\nLe nombre d’arbres\n\n\nsplitrule / criterion\nLe critère de choix de la règle de division des noeuds intermédiaires\n\n\nmin.bucket / min_samples_split\nNombre minimal d’observations dans les noeuds terminaux\n\n\nmax.depth / max_depth\nProfondeur maximale des arbres\n\n\n\n\nLe nombre de variables candidates à chaque noeud contrôle l’échantillonnage des variables lors de l’entraînement. La valeur par défaut est fréquemment \\(\\sqrt p\\) pour la classification et \\(p/3\\) pour la régression. C’est l’hyperparamètre qui a le plus fort effet sur la performance de la forêt aléatoire. Une valeur plus basse aboutit à des arbres plus différents, donc moins corrélés (car ils reposent sur des variables différentes), mais ces arbres peuvent être moins performants car ils reposent parfois sur des variables peu pertinentes. Inversement, une valeur plus élevée du nombre de variables candidates aboutit à des arbres plus performants, mais plus corrélés. C’est en particulier le cas si seulement certaines variables sont très prédictives, car ce sont ces variables qui apparaitront dans la plupart des arbres.\nLe taux d’échantillonnage et le mode de tirage contrôlent le plan d’échantillonnage des données d’entraînement. Les valeurs par défaut varient d’une implémentation à l’autre; dans le cas de ranger, le taux d’échantillonnage est de 63,2% sans remise, et de 100% avec remise. L’implémentation scikit-learn ne propose pas le tirage sans remise. Ces hyperparamètres ont des effets sur la performance similaires à ceux du nombre de variables candidates, mais dans une moindre ampleur. Un taux d’échantillonnage plus faible aboutit à des arbres plus différents et donc moins corrélés (car ils sont entraînés sur des échantillons très différents), mais ces arbres peuvent être peu performants car ils sont entraînés sur des échantillons de petite taille. Inversement, un taux d’échantillonnage élevé aboutit à des arbres plus performants mais plus corrélés. Les effets de l’échantillonnage avec ou sans remise sur la performance de la forêt aléatoire sont moins clairs et ne font pas consensus. Les travaux les plus récents suggèrent toutefois qu’il est préférable d’échantillonner sans remise (Probst, Wright, and Boulesteix (2019)).\nLe nombre minimal d’observations dans les noeuds terminaux contrôle la taille des noeuds terminaux. La valeur par défaut est faible dans la plupart des implémentations (entre 1 et 5). Il n’y a pas vraiment de consensus sur l’effet de cet hyperparamètre sur les performances. En revanche, il est certain que le temps d’entraînement décroît fortement avec cet hyperparamètre: une valeur faible implique des arbres très profonds, avec un grand nombre de noeuds. Il peut donc être utile de fixer ce nombre à une valeur plus élevée pour accélérer l’entraînement, en particulier si les données sont volumineuses.\nLe nombre d’arbres par défaut varie selon les implémentations (500 dans ranger, 100 dans scikit-learn). Il s’agit d’un hyperparamètre particulier car il n’est associé à aucun arbitrage en matière de performance: la performance de la forêt aléatoire croît avec le nombre d’arbres, puis se stabilise à un niveau approximativement constant. Le nombre optimal d’arbres est donc intuitivement celui à partir duquel la performance de la forêt ne croît plus (ce point est détaillé plus bas). Il est important de noter que ce nombre optimal dépend des autres hyperparamètres. Par exemple, un taux d’échantillonnage faible et un nombre faible de variables candidates à chaque noeud aboutissent à des arbres peu corrélés, mais peu performants, ce qui requiert probablement un plus grand nombre d’arbres.\nLe critère de choix de la règle de division des noeuds intermédiaires: la plupart des implémentations des forêts aléatoires retiennent par défaut l’impureté de Gini pour la classification et la variance pour la régression, même si d’autres critères de choix ont été proposés dans la littérature. A ce stade, aucun critère de choix ne paraît systématiquement supérieur aux autres en matière de performance. Le lecteur intéressé pourra se référer à la discussion détaillée dans Probst, Wright, and Boulesteix (2019).\n\n\n\n\nComme indiqué dans la partie REFERENCE A AJOUTER, la performance prédictive d’une forêt aléatoire varie en fonction de deux critères essentiels: elle croît avec le pouvoir prédictif moyen des arbres, et décroît avec la corrélation entre les arbres. La recherche d’arbres très prédictifs pouvant aboutir à augmenter la corrélation entre eux, l’objectif de l’entraînement d’une forêt aléatoire revient à trouver le meilleur arbitrage possible entre pouvoir prédictif et corrélation.",
    "crumbs": [
      "Comment bien utiliser les algorithmes?",
      "Guide d'entraînement des forêts aléatoires"
    ]
  },
  {
    "objectID": "chapters/chapter3/1-guide_usage_RF.html#guide-dentraînement-des-forêts-aléatoires",
    "href": "chapters/chapter3/1-guide_usage_RF.html#guide-dentraînement-des-forêts-aléatoires",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "",
    "text": "Cette section rassemble et synthétise des recommandations sur l’entraînement des forêts aléatoires disponibles dans la littérature, en particulier dans Probst, Wright, and Boulesteix (2019).\n\n\nLe processus pour construire une Random Forest se résume comme suit:\n\nSélectionnez le nombre d’arbres à construire (n_trees).\nPour chaque arbre, effectuez les étapes suivantes :\n\nGénérer un échantillon bootstrap à partir du jeu de données.\nConstruire un arbre de décision à partir de cet échantillon:\nÀ chaque nœud de l’arbre, sélectionner un sous-ensemble aléatoire de caractéristiques (mtry).\nTrouver la meilleure division parmi ce sous-ensemble et créer des nœuds enfants.\nArrêter la croissance de l’arbre selon des critères de fin spécifiques (comme une taille minimale de nœud), mais sans élaguer l’arbre.\n\nAgréger les arbres pour effectuer les prédictions finales :\n\nRégression : la prédiction finale est la moyenne des prédictions de tous les arbres.\nClassification : chaque arbre vote pour une classe, et la classe majoritaire est retenue.\n\n\n\n\n\nIl existe de multiples implémentations des forêts aléatoires. Le présent document présente et recommande l’usage de deux implémentations de référence: le package R ranger et le package Python scikit-learn. Il est à noter qu’il est possible d’entraîner des forêts aléatoires avec les algorithmes XGBoost et LightGBM, mais il s’agit d’un usage avancé qui n’est recommandé en première approche. Cette approche est présentée dans la partie REFERENCE A LA PARTIE USAGE AVANCE.\n\n\n\nCette section décrit en détail les principaux hyperparamètres des forêts aléatoires listés dans le tableau . Les noms des hyperparamètres utilisés sont ceux figurant dans le package R ranger, et dans le package Python scikit-learn. Il arrive qu’ils portent un nom différent dans d’autres implémentations des random forests, mais il est généralement facile de s’y retrouver en lisant attentivement la documentation.\n\nLes principaux hyperparamètres des forêts aléatoires\n\n\n\n\n\n\nHyperparamètre (ranger / scikit-learn)\nDescription\n\n\n\n\nmtry / max_features\nLe nombre de variables candidates à chaque noeud\n\n\nreplacement / absent\nL’échantillonnage des données se fait-il avec ou sans remise?\n\n\nsample.fraction / max_samples\nLe taux d’échantillonnage des données\n\n\nmin.node.size / min_samples_leaf\nNombre minimal d’observations nécessaire pour qu’un noeud puisse être partagé\n\n\nnum.trees / n_estimators\nLe nombre d’arbres\n\n\nsplitrule / criterion\nLe critère de choix de la règle de division des noeuds intermédiaires\n\n\nmin.bucket / min_samples_split\nNombre minimal d’observations dans les noeuds terminaux\n\n\nmax.depth / max_depth\nProfondeur maximale des arbres\n\n\n\n\nLe nombre de variables candidates à chaque noeud contrôle l’échantillonnage des variables lors de l’entraînement. La valeur par défaut est fréquemment \\(\\sqrt p\\) pour la classification et \\(p/3\\) pour la régression. C’est l’hyperparamètre qui a le plus fort effet sur la performance de la forêt aléatoire. Une valeur plus basse aboutit à des arbres plus différents, donc moins corrélés (car ils reposent sur des variables différentes), mais ces arbres peuvent être moins performants car ils reposent parfois sur des variables peu pertinentes. Inversement, une valeur plus élevée du nombre de variables candidates aboutit à des arbres plus performants, mais plus corrélés. C’est en particulier le cas si seulement certaines variables sont très prédictives, car ce sont ces variables qui apparaitront dans la plupart des arbres.\nLe taux d’échantillonnage et le mode de tirage contrôlent le plan d’échantillonnage des données d’entraînement. Les valeurs par défaut varient d’une implémentation à l’autre; dans le cas de ranger, le taux d’échantillonnage est de 63,2% sans remise, et de 100% avec remise. L’implémentation scikit-learn ne propose pas le tirage sans remise. Ces hyperparamètres ont des effets sur la performance similaires à ceux du nombre de variables candidates, mais dans une moindre ampleur. Un taux d’échantillonnage plus faible aboutit à des arbres plus différents et donc moins corrélés (car ils sont entraînés sur des échantillons très différents), mais ces arbres peuvent être peu performants car ils sont entraînés sur des échantillons de petite taille. Inversement, un taux d’échantillonnage élevé aboutit à des arbres plus performants mais plus corrélés. Les effets de l’échantillonnage avec ou sans remise sur la performance de la forêt aléatoire sont moins clairs et ne font pas consensus. Les travaux les plus récents suggèrent toutefois qu’il est préférable d’échantillonner sans remise (Probst, Wright, and Boulesteix (2019)).\nLe nombre minimal d’observations dans les noeuds terminaux contrôle la taille des noeuds terminaux. La valeur par défaut est faible dans la plupart des implémentations (entre 1 et 5). Il n’y a pas vraiment de consensus sur l’effet de cet hyperparamètre sur les performances. En revanche, il est certain que le temps d’entraînement décroît fortement avec cet hyperparamètre: une valeur faible implique des arbres très profonds, avec un grand nombre de noeuds. Il peut donc être utile de fixer ce nombre à une valeur plus élevée pour accélérer l’entraînement, en particulier si les données sont volumineuses.\nLe nombre d’arbres par défaut varie selon les implémentations (500 dans ranger, 100 dans scikit-learn). Il s’agit d’un hyperparamètre particulier car il n’est associé à aucun arbitrage en matière de performance: la performance de la forêt aléatoire croît avec le nombre d’arbres, puis se stabilise à un niveau approximativement constant. Le nombre optimal d’arbres est donc intuitivement celui à partir duquel la performance de la forêt ne croît plus (ce point est détaillé plus bas). Il est important de noter que ce nombre optimal dépend des autres hyperparamètres. Par exemple, un taux d’échantillonnage faible et un nombre faible de variables candidates à chaque noeud aboutissent à des arbres peu corrélés, mais peu performants, ce qui requiert probablement un plus grand nombre d’arbres.\nLe critère de choix de la règle de division des noeuds intermédiaires: la plupart des implémentations des forêts aléatoires retiennent par défaut l’impureté de Gini pour la classification et la variance pour la régression, même si d’autres critères de choix ont été proposés dans la littérature. A ce stade, aucun critère de choix ne paraît systématiquement supérieur aux autres en matière de performance. Le lecteur intéressé pourra se référer à la discussion détaillée dans Probst, Wright, and Boulesteix (2019).\n\n\n\n\nComme indiqué dans la partie REFERENCE A AJOUTER, la performance prédictive d’une forêt aléatoire varie en fonction de deux critères essentiels: elle croît avec le pouvoir prédictif moyen des arbres, et décroît avec la corrélation entre les arbres. La recherche d’arbres très prédictifs pouvant aboutir à augmenter la corrélation entre eux, l’objectif de l’entraînement d’une forêt aléatoire revient à trouver le meilleur arbitrage possible entre pouvoir prédictif et corrélation.",
    "crumbs": [
      "Comment bien utiliser les algorithmes?",
      "Guide d'entraînement des forêts aléatoires"
    ]
  }
]