[
  {
    "objectID": "chapters/chapter3/2-guide_usage_RF.html",
    "href": "chapters/chapter3/2-guide_usage_RF.html",
    "title": "1 Guide d’usage des forêts aléatoires",
    "section": "",
    "text": "Ce guide d’entraînement des forêts aléatoires rassemble et synthétise des recommandations sur l’entraînement des forêts aléatoires disponibles dans la littérature, en particulier dans Probst, Wright, and Boulesteix (2019) et Biau and Scornet (2016). Ce guide comporte un certain nombre de choix méthodologiques forts, comme les implémentations recommandées ou la procédure proposée pour l’optimisation des hyperparamètres, et d’autres choix pertinents sont évidemment possibles. C’est pourquoi les recommandations de ce guide doivent être considérées comme un point de départ raisonnable, pas comme un ensemble de règles devant être respectées à tout prix.\n\n\nIl existe de multiples implémentations des forêts aléatoires. Le présent document présente et recommande l’usage de deux implémentations de référence: le package R ranger et le package Python scikit-learn pour leur rigueur, leur efficacité et leur simplicité d’utilisation. Il est à noter qu’il est possible d’entraîner des forêts aléatoires avec les algorithmes XGBoost et LightGBM, mais il s’agit d’un usage avancé qui n’est pas recommandé en première approche. Cette approche est présentée dans la partie REFERENCE A LA PARTIE USAGE AVANCE.\n\n\n\nCette section décrit en détail les principaux hyperparamètres des forêts aléatoires listés dans le tableau 1. Les noms des hyperparamètres utilisés sont ceux figurant dans le package R ranger, et dans le package Python scikit-learn. Il arrive qu’ils portent un nom différent dans d’autres implémentations des forêts aléatoires, mais il est généralement facile de s’y retrouver en lisant attentivement la documentation.\n\n\n\nTable 1: Les principaux hyperparamètres des forêts aléatoires\n\n\n\n\n\n\n\n\n\nHyperparamètre (ranger / scikit-learn)\nDescription\n\n\n\n\nnum.trees / n_estimators\nLe nombre d’arbres\n\n\nmtry / max_features\nLe nombre de variables candidates à chaque noeud\n\n\nsample.fraction / max_samples\nLe taux d’échantillonnage des données (ou la taille de l’échantillon)\n\n\nreplacement / absent\nL’échantillonnage des données se fait-il avec ou sans remise?\n\n\nmin.bucket / min_samples_leaf\nNombre minimal d’observations dans les noeuds terminaux\n\n\nmax.depth / max_depth\nProfondeur maximale des arbres\n\n\nmin.node.size / min_samples_split\nNombre minimal d’observations nécessaire pour qu’un noeud puisse être partagé\n\n\nsplitrule / criterion\nLe critère de choix de la règle de division des noeuds intermédiaires\n\n\noob.error / oob_score\nCalculer la performance de la forêt par l’erreur OOB (et choix de la métrique pour scikit)\n\n\n\n\n\n\nVoici une présentation des principaux hyperparamètres et de leurs effets sur les performances de la forêt aléatoire:\n\nLe nombre d’arbres par défaut varie selon les implémentations (500 dans ranger, 100 dans scikit-learn). Il s’agit d’un hyperparamètre particulier car il n’est associé à aucun arbitrage en matière de performance: la performance de la forêt aléatoire croît avec le nombre d’arbres, puis se stabilise. Le nombre optimal d’arbres est celui à partir duquel la performance de la forêt ne croît plus (ce point est détaillé plus bas) où à partir duquel l’ajout d’arbres supplémentaires génère des gains marginaux. Il est important de noter que ce nombre optimal dépend des autres hyperparamètres. Par exemple, un taux d’échantillonnage faible et un nombre faible de variables candidates à chaque noeud aboutissent à des arbres peu corrélés, mais peu performants, ce qui requiert probablement un plus grand nombre d’arbres. Dans le cas d’une classification, l’utilisation de mesures comme le score de Brier ou la fonction de perte logarithmique est recommandée pour évaluer la convergence plutôt que la précision (métrique par défaut de ranger et scikit-learn).\nLe nombre (ou la part) de variables candidates à chaque noeud (souvent appelé mtry) est un hyperparamètre essentiel qui détermine le nombre de variables prédictives sélectionnées aléatoirement à chaque nœud lors de la construction des arbres. Ce paramètre exerce la plus forte influence sur les performances du modèle, et un compromis doit être trouvé entre puissance prédictive des arbres et corrélation entre arbres. Une faible valeur de mtry conduit à des arbres moins performants mais plus diversifiés et donc moins corrélés entre eux. Inversement, une valeur plus élevée améliore la précision des arbres individuels mais accroît leur corrélation (les mêmes variables ayant tendance à être sélectionnées dans tous les arbres). La valeur optimale de mtry dépend du nombre de variables réellement pertinentes dans les données: elle est plus faible lorsque la plupart des variables sont pertinentes, et plus élevée lorsqu’il y a peu de variables pertinentes. Par ailleurs, une valeur élevée de mtry est préférable si les données comprennent un grand nombre de variables binaires issues du one-hot-encoding des variables catégorielles (LIEN AVEC LA PARTIE PREPROCESSING). Par défaut, cette valeur est fréquemment fixée à \\(\\sqrt{p}\\) pour les problèmes de classification et à \\(p/3\\) pour les problèmes de régression, où \\(p\\) représente le nombre total de variables prédictives disponibles.\nLe taux d’échantillonnage et le mode de tirage contrôlent le plan d’échantillonnage des données d’entraînement. Les valeurs par défaut varient d’une implémentation à l’autre; dans le cas de ranger, le taux d’échantillonnage est de 63,2% sans remise, et de 100% avec remise. L’implémentation scikit-learn ne propose pas le tirage sans remise. Ces hyperparamètres ont des effets sur la performance similaires à ceux du nombre de variables candidates, mais d’une moindre ampleur. Un taux d’échantillonnage plus faible aboutit à des arbres plus diversifiés et donc moins corrélés (car ils sont entraînés sur des échantillons très différents), mais ces arbres peuvent être peu performants car ils sont entraînés sur des échantillons de petite taille. Inversement, un taux d’échantillonnage élevé aboutit à des arbres plus performants mais plus corrélés. Les effets de l’échantillonnage avec ou sans remise sur la performance de la forêt aléatoire sont moins clairs et ne font pas consensus. Les travaux les plus récents semblent toutefois suggérer qu’il est préférable d’échantillonner sans remise (Probst, Wright, and Boulesteix (2019)).\nLe nombre minimal d’observations dans les noeuds terminaux contrôle la taille des noeuds terminaux. La valeur par défaut est faible dans la plupart des implémentations (entre 1 et 5). Il n’y a pas vraiment de consensus sur l’effet de cet hyperparamètre sur les performances, bien qu’une valeur plus faible augmente le risque de sur-apprentissage. En revanche, il est certain que le temps d’entraînement décroît fortement avec cet hyperparamètre: une valeur faible implique des arbres très profonds, avec un grand nombre de noeuds. Il peut donc être utile de fixer ce nombre à une valeur plus élevée pour accélérer l’entraînement, en particulier si les données sont volumineuses et si on utilise une méthode de validation croisée pour le choix des autres hyperparamètres. Cela se fait généralement sans perte significative de performance.\nLe critère de choix de la règle de division des noeuds intermédiaires: la plupart des implémentations des forêts aléatoires retiennent par défaut l’impureté de Gini pour la classification et la variance pour la régression, même si d’autres critères de choix ont été proposés dans la littérature (p-value dans les forêts d’inférence conditionnelle, arbres extrêmement randomisés, etc.). Chaque règle présente des avantages et des inconvénients, notamment en termes de biais de sélection des variables et de vitesse de calcul. A ce stade, aucun critère de choix ne paraît systématiquement supérieur aux autres en matière de performance. Modifier cet hyperparamètre relève d’un usage avancé des forêts aléatoires. Le lecteur intéressé pourra se référer à la discussion détaillée dans Probst, Wright, and Boulesteix (2019).\n\n\n\n\nLes forêts aléatoires nécessitent généralement moins d’optimisation que d’autres modèles de machine learning, car leurs performances varient relativement peu en fonction des hyperparamètres. Les valeurs par défaut fournissent souvent des résultats satisfaisants, ce qui réduit le besoin d’optimisation intensive. Cependant, un ajustement précis des hyperparamètres peut apporter des gains de performance, notamment sur des jeux de données complexes.\nComme indiqué dans la partie ?@sec-facteur-perf-rf, la performance prédictive d’une forêt aléatoire varie en fonction de deux critères essentiels: elle croît avec le pouvoir prédictif des arbres, et décroît avec la corrélation des arbres entre eux. L’optimisation des hyperparamètres d’une forêt aléatoire vise donc à trouver un équilibre optimal où les arbres sont suffisamment puissants pour être prédictifs, tout en étant suffisamment diversifiés pour que leurs erreurs ne soient pas trop corrélées.\nLa littérature propose de multiples approches pour optimiser simultanément plusieurs hyperparamètres: la recherche par grille (grid search), la recherche aléatoire (random search) et l’optimisation basée sur modèle séquentiel (SMBO), et il peut être difficile de savoir quelle approche adopter. Ce guide propose donc une première approche délibérément simple, avant de présenter les approches plus avancées.\n\n\nVoici une procédure simple pour entraîner une forêt aléatoire. Elle ne garantit pas l’obtention d’un modèle optimal, mais elle est lisible et permet d’obtenir rapidement un modèle raisonnablement performant.\n\nEntraîner une forêt aléatoire avec les valeurs des hyperparamètres par défaut. Ce premier modèle servira de point de comparaison pour la suite.\nAjuster le nombre d’arbres: entraîner une forêt aléatoire avec les hyperparamètres par défaut en augmentant progressivement le nombre d’arbres, puis déterminer à partir de quel nombre d’arbres la performance se stabilise (en mesurant la performance avec l’erreur OOB avec pour métrique le score de Brier). Fixer le nombre d’arbres à cette valeur par la suite.\nAjuster le nombre de variables candidates et le taux d’échantillonnage: optimiser ces deux hyperparamètres grâce à une méthode de grid search évaluée par une approche de validation-croisée, ou par une approche reposant sur l’erreur OOB.\nAjuster le nombre minimal d’observations dans les noeuds terminaux: optimiser cet hyperparamètre grâce à une méthode de grid search évaluée par une approche de validation-croisée, ou par une approche reposant sur l’erreur OOB. Ce n’est pas l’hyperparamètre le plus important, mais s’il est possible de le fixer à une valeur plus élevée que la valeur par défaut sans perte de performance, cela permet d’accélérer le reste de la procédure.\nEntraîner le modèle final: entraîner une forêt aléatoire avec les hyperparamètres optimisés déduits des étapes précédentes.\nÉvaluer le modèle final: mesurer la performance du modèle final soit avec l’approche out-of-bag (OOB), soit avec un ensemble de test. Il est souvent instructif de comparer les performances du modèle final et du modèle entraîné avec les valeurs des hyperparamètres par défaut (parfois pour se rendre compte que ce dernier était déjà suffisamment performant…).\n\n\n\n\nLorsque l’espace des hyperparamètres est large ou que les performances initiales sont insuffisantes, adopter des méthodes avancées comme l’ptimisation basée sur un modèle séquentiel (SMBO). En R, il existe plusieurs implémentations d’appuyant sur cette méthode: tuneRF (limité à l’optimisation de mtry), tuneRanger (optimise simultanément mtry, node size, et sample size). La méthode SMBO est généralement la plus performante, mais demande un temps de calcul plus important.\nIl est également possible de remplacer les critères classiques (le taux d’erreur pour une classification par exemple) par d’autres critères de performance, comme le score de Brier ou la fonction de perte logarithmique (Probst and Boulesteix (2018)).\nPour gérer la contrainte computationnelle, il est possible de commencer par utiliser des échantillons réduits pour les étapes exploratoires, puis d’augmenter la taille de l’échantillon pour les tests finaux.\n\n\n\n\n\nLes méthodes classiques d’évaluation de l’importance des variables, telles que l’indice de Gini (Mean Decrease in Impurity - MDI) et l’importance par permutation (Mean Decrease Accuracy - MDA), peuvent produire des résultats biaisés dans certaines situations (Strobl et al. (2007), Bénard, Da Veiga, and Scornet (2022), Bénard et al. (2022)). Notamment, lorsque les variables prédictives sont fortement corrélées, présentent des échelles de mesure différentes ou possèdent un nombre variable de catégories, ces méthodes peuvent surestimer l’importance de certaines variables. Par exemple, les variables avec un grand nombre de catégories ou des échelles continues étendues peuvent être artificiellement privilégiées, même si leur contribution réelle à la prédiction est limitée.\nEn pratique, il est recommandé d’utiliser des méthodes d’importance des variables moins sensibles aux biais, comme les CIF ou la Sobol-MDA. Les valeurs de Shapley, issues de la théorie des jeux, sont également une alternative intéressante. Elles attribuent à chaque variable une contribution proportionnelle à son impact sur la prédiction. Cependant, leur calcul est souvent complexe et coûteux en ressources computationnelles, surtout en présence de nombreuses variables. Des méthodes comme SHAFF (SHApley eFfects via random Forests) ont été développées pour estimer efficacement ces valeurs, même en présence de dépendances entre variables.\nOn conseille l’utilisation de trois implémentations pour comparer l’importances des variables d’une forêt aléatoire:\n\nPour la MDI: l’algorithme CIF proposé par Strobl et al. (2007) et implémenté en R\nPour la MDA: l’algorithme Sobol-MDA proposé par Bénard, Da Veiga, and Scornet (2022) et implémenté en R\nPour les valeurs de Shapley : l’alogrithme SHAFF proposé par Bénard et al. (2022) et implémenté en R\n\nEnfin, nous recommandons de combiner plusieurs méthodes pour une analyse plus robuste et de tenir compte des prétraitements des données afin de minimiser les biais potentiels.",
    "crumbs": [
      "Comment bien utiliser les algorithmes?",
      "Guide d'usage des forêts aléatoires"
    ]
  },
  {
    "objectID": "chapters/chapter3/2-guide_usage_RF.html#sec-implementation-rf",
    "href": "chapters/chapter3/2-guide_usage_RF.html#sec-implementation-rf",
    "title": "1 Guide d’usage des forêts aléatoires",
    "section": "",
    "text": "Il existe de multiples implémentations des forêts aléatoires. Le présent document présente et recommande l’usage de deux implémentations de référence: le package R ranger et le package Python scikit-learn pour leur rigueur, leur efficacité et leur simplicité d’utilisation. Il est à noter qu’il est possible d’entraîner des forêts aléatoires avec les algorithmes XGBoost et LightGBM, mais il s’agit d’un usage avancé qui n’est pas recommandé en première approche. Cette approche est présentée dans la partie REFERENCE A LA PARTIE USAGE AVANCE.",
    "crumbs": [
      "Comment bien utiliser les algorithmes?",
      "Guide d'usage des forêts aléatoires"
    ]
  },
  {
    "objectID": "chapters/chapter3/2-guide_usage_RF.html#sec-hyperparam-rf",
    "href": "chapters/chapter3/2-guide_usage_RF.html#sec-hyperparam-rf",
    "title": "1 Guide d’usage des forêts aléatoires",
    "section": "",
    "text": "Cette section décrit en détail les principaux hyperparamètres des forêts aléatoires listés dans le tableau 1. Les noms des hyperparamètres utilisés sont ceux figurant dans le package R ranger, et dans le package Python scikit-learn. Il arrive qu’ils portent un nom différent dans d’autres implémentations des forêts aléatoires, mais il est généralement facile de s’y retrouver en lisant attentivement la documentation.\n\n\n\nTable 1: Les principaux hyperparamètres des forêts aléatoires\n\n\n\n\n\n\n\n\n\nHyperparamètre (ranger / scikit-learn)\nDescription\n\n\n\n\nnum.trees / n_estimators\nLe nombre d’arbres\n\n\nmtry / max_features\nLe nombre de variables candidates à chaque noeud\n\n\nsample.fraction / max_samples\nLe taux d’échantillonnage des données (ou la taille de l’échantillon)\n\n\nreplacement / absent\nL’échantillonnage des données se fait-il avec ou sans remise?\n\n\nmin.bucket / min_samples_leaf\nNombre minimal d’observations dans les noeuds terminaux\n\n\nmax.depth / max_depth\nProfondeur maximale des arbres\n\n\nmin.node.size / min_samples_split\nNombre minimal d’observations nécessaire pour qu’un noeud puisse être partagé\n\n\nsplitrule / criterion\nLe critère de choix de la règle de division des noeuds intermédiaires\n\n\noob.error / oob_score\nCalculer la performance de la forêt par l’erreur OOB (et choix de la métrique pour scikit)\n\n\n\n\n\n\nVoici une présentation des principaux hyperparamètres et de leurs effets sur les performances de la forêt aléatoire:\n\nLe nombre d’arbres par défaut varie selon les implémentations (500 dans ranger, 100 dans scikit-learn). Il s’agit d’un hyperparamètre particulier car il n’est associé à aucun arbitrage en matière de performance: la performance de la forêt aléatoire croît avec le nombre d’arbres, puis se stabilise. Le nombre optimal d’arbres est celui à partir duquel la performance de la forêt ne croît plus (ce point est détaillé plus bas) où à partir duquel l’ajout d’arbres supplémentaires génère des gains marginaux. Il est important de noter que ce nombre optimal dépend des autres hyperparamètres. Par exemple, un taux d’échantillonnage faible et un nombre faible de variables candidates à chaque noeud aboutissent à des arbres peu corrélés, mais peu performants, ce qui requiert probablement un plus grand nombre d’arbres. Dans le cas d’une classification, l’utilisation de mesures comme le score de Brier ou la fonction de perte logarithmique est recommandée pour évaluer la convergence plutôt que la précision (métrique par défaut de ranger et scikit-learn).\nLe nombre (ou la part) de variables candidates à chaque noeud (souvent appelé mtry) est un hyperparamètre essentiel qui détermine le nombre de variables prédictives sélectionnées aléatoirement à chaque nœud lors de la construction des arbres. Ce paramètre exerce la plus forte influence sur les performances du modèle, et un compromis doit être trouvé entre puissance prédictive des arbres et corrélation entre arbres. Une faible valeur de mtry conduit à des arbres moins performants mais plus diversifiés et donc moins corrélés entre eux. Inversement, une valeur plus élevée améliore la précision des arbres individuels mais accroît leur corrélation (les mêmes variables ayant tendance à être sélectionnées dans tous les arbres). La valeur optimale de mtry dépend du nombre de variables réellement pertinentes dans les données: elle est plus faible lorsque la plupart des variables sont pertinentes, et plus élevée lorsqu’il y a peu de variables pertinentes. Par ailleurs, une valeur élevée de mtry est préférable si les données comprennent un grand nombre de variables binaires issues du one-hot-encoding des variables catégorielles (LIEN AVEC LA PARTIE PREPROCESSING). Par défaut, cette valeur est fréquemment fixée à \\(\\sqrt{p}\\) pour les problèmes de classification et à \\(p/3\\) pour les problèmes de régression, où \\(p\\) représente le nombre total de variables prédictives disponibles.\nLe taux d’échantillonnage et le mode de tirage contrôlent le plan d’échantillonnage des données d’entraînement. Les valeurs par défaut varient d’une implémentation à l’autre; dans le cas de ranger, le taux d’échantillonnage est de 63,2% sans remise, et de 100% avec remise. L’implémentation scikit-learn ne propose pas le tirage sans remise. Ces hyperparamètres ont des effets sur la performance similaires à ceux du nombre de variables candidates, mais d’une moindre ampleur. Un taux d’échantillonnage plus faible aboutit à des arbres plus diversifiés et donc moins corrélés (car ils sont entraînés sur des échantillons très différents), mais ces arbres peuvent être peu performants car ils sont entraînés sur des échantillons de petite taille. Inversement, un taux d’échantillonnage élevé aboutit à des arbres plus performants mais plus corrélés. Les effets de l’échantillonnage avec ou sans remise sur la performance de la forêt aléatoire sont moins clairs et ne font pas consensus. Les travaux les plus récents semblent toutefois suggérer qu’il est préférable d’échantillonner sans remise (Probst, Wright, and Boulesteix (2019)).\nLe nombre minimal d’observations dans les noeuds terminaux contrôle la taille des noeuds terminaux. La valeur par défaut est faible dans la plupart des implémentations (entre 1 et 5). Il n’y a pas vraiment de consensus sur l’effet de cet hyperparamètre sur les performances, bien qu’une valeur plus faible augmente le risque de sur-apprentissage. En revanche, il est certain que le temps d’entraînement décroît fortement avec cet hyperparamètre: une valeur faible implique des arbres très profonds, avec un grand nombre de noeuds. Il peut donc être utile de fixer ce nombre à une valeur plus élevée pour accélérer l’entraînement, en particulier si les données sont volumineuses et si on utilise une méthode de validation croisée pour le choix des autres hyperparamètres. Cela se fait généralement sans perte significative de performance.\nLe critère de choix de la règle de division des noeuds intermédiaires: la plupart des implémentations des forêts aléatoires retiennent par défaut l’impureté de Gini pour la classification et la variance pour la régression, même si d’autres critères de choix ont été proposés dans la littérature (p-value dans les forêts d’inférence conditionnelle, arbres extrêmement randomisés, etc.). Chaque règle présente des avantages et des inconvénients, notamment en termes de biais de sélection des variables et de vitesse de calcul. A ce stade, aucun critère de choix ne paraît systématiquement supérieur aux autres en matière de performance. Modifier cet hyperparamètre relève d’un usage avancé des forêts aléatoires. Le lecteur intéressé pourra se référer à la discussion détaillée dans Probst, Wright, and Boulesteix (2019).",
    "crumbs": [
      "Comment bien utiliser les algorithmes?",
      "Guide d'usage des forêts aléatoires"
    ]
  },
  {
    "objectID": "chapters/chapter3/2-guide_usage_RF.html#sec-procedure-training-rf",
    "href": "chapters/chapter3/2-guide_usage_RF.html#sec-procedure-training-rf",
    "title": "1 Guide d’usage des forêts aléatoires",
    "section": "",
    "text": "Les forêts aléatoires nécessitent généralement moins d’optimisation que d’autres modèles de machine learning, car leurs performances varient relativement peu en fonction des hyperparamètres. Les valeurs par défaut fournissent souvent des résultats satisfaisants, ce qui réduit le besoin d’optimisation intensive. Cependant, un ajustement précis des hyperparamètres peut apporter des gains de performance, notamment sur des jeux de données complexes.\nComme indiqué dans la partie ?@sec-facteur-perf-rf, la performance prédictive d’une forêt aléatoire varie en fonction de deux critères essentiels: elle croît avec le pouvoir prédictif des arbres, et décroît avec la corrélation des arbres entre eux. L’optimisation des hyperparamètres d’une forêt aléatoire vise donc à trouver un équilibre optimal où les arbres sont suffisamment puissants pour être prédictifs, tout en étant suffisamment diversifiés pour que leurs erreurs ne soient pas trop corrélées.\nLa littérature propose de multiples approches pour optimiser simultanément plusieurs hyperparamètres: la recherche par grille (grid search), la recherche aléatoire (random search) et l’optimisation basée sur modèle séquentiel (SMBO), et il peut être difficile de savoir quelle approche adopter. Ce guide propose donc une première approche délibérément simple, avant de présenter les approches plus avancées.\n\n\nVoici une procédure simple pour entraîner une forêt aléatoire. Elle ne garantit pas l’obtention d’un modèle optimal, mais elle est lisible et permet d’obtenir rapidement un modèle raisonnablement performant.\n\nEntraîner une forêt aléatoire avec les valeurs des hyperparamètres par défaut. Ce premier modèle servira de point de comparaison pour la suite.\nAjuster le nombre d’arbres: entraîner une forêt aléatoire avec les hyperparamètres par défaut en augmentant progressivement le nombre d’arbres, puis déterminer à partir de quel nombre d’arbres la performance se stabilise (en mesurant la performance avec l’erreur OOB avec pour métrique le score de Brier). Fixer le nombre d’arbres à cette valeur par la suite.\nAjuster le nombre de variables candidates et le taux d’échantillonnage: optimiser ces deux hyperparamètres grâce à une méthode de grid search évaluée par une approche de validation-croisée, ou par une approche reposant sur l’erreur OOB.\nAjuster le nombre minimal d’observations dans les noeuds terminaux: optimiser cet hyperparamètre grâce à une méthode de grid search évaluée par une approche de validation-croisée, ou par une approche reposant sur l’erreur OOB. Ce n’est pas l’hyperparamètre le plus important, mais s’il est possible de le fixer à une valeur plus élevée que la valeur par défaut sans perte de performance, cela permet d’accélérer le reste de la procédure.\nEntraîner le modèle final: entraîner une forêt aléatoire avec les hyperparamètres optimisés déduits des étapes précédentes.\nÉvaluer le modèle final: mesurer la performance du modèle final soit avec l’approche out-of-bag (OOB), soit avec un ensemble de test. Il est souvent instructif de comparer les performances du modèle final et du modèle entraîné avec les valeurs des hyperparamètres par défaut (parfois pour se rendre compte que ce dernier était déjà suffisamment performant…).\n\n\n\n\nLorsque l’espace des hyperparamètres est large ou que les performances initiales sont insuffisantes, adopter des méthodes avancées comme l’ptimisation basée sur un modèle séquentiel (SMBO). En R, il existe plusieurs implémentations d’appuyant sur cette méthode: tuneRF (limité à l’optimisation de mtry), tuneRanger (optimise simultanément mtry, node size, et sample size). La méthode SMBO est généralement la plus performante, mais demande un temps de calcul plus important.\nIl est également possible de remplacer les critères classiques (le taux d’erreur pour une classification par exemple) par d’autres critères de performance, comme le score de Brier ou la fonction de perte logarithmique (Probst and Boulesteix (2018)).\nPour gérer la contrainte computationnelle, il est possible de commencer par utiliser des échantillons réduits pour les étapes exploratoires, puis d’augmenter la taille de l’échantillon pour les tests finaux.",
    "crumbs": [
      "Comment bien utiliser les algorithmes?",
      "Guide d'usage des forêts aléatoires"
    ]
  },
  {
    "objectID": "chapters/chapter3/2-guide_usage_RF.html#mesurer-limportance-des-variables",
    "href": "chapters/chapter3/2-guide_usage_RF.html#mesurer-limportance-des-variables",
    "title": "1 Guide d’usage des forêts aléatoires",
    "section": "",
    "text": "Les méthodes classiques d’évaluation de l’importance des variables, telles que l’indice de Gini (Mean Decrease in Impurity - MDI) et l’importance par permutation (Mean Decrease Accuracy - MDA), peuvent produire des résultats biaisés dans certaines situations (Strobl et al. (2007), Bénard, Da Veiga, and Scornet (2022), Bénard et al. (2022)). Notamment, lorsque les variables prédictives sont fortement corrélées, présentent des échelles de mesure différentes ou possèdent un nombre variable de catégories, ces méthodes peuvent surestimer l’importance de certaines variables. Par exemple, les variables avec un grand nombre de catégories ou des échelles continues étendues peuvent être artificiellement privilégiées, même si leur contribution réelle à la prédiction est limitée.\nEn pratique, il est recommandé d’utiliser des méthodes d’importance des variables moins sensibles aux biais, comme les CIF ou la Sobol-MDA. Les valeurs de Shapley, issues de la théorie des jeux, sont également une alternative intéressante. Elles attribuent à chaque variable une contribution proportionnelle à son impact sur la prédiction. Cependant, leur calcul est souvent complexe et coûteux en ressources computationnelles, surtout en présence de nombreuses variables. Des méthodes comme SHAFF (SHApley eFfects via random Forests) ont été développées pour estimer efficacement ces valeurs, même en présence de dépendances entre variables.\nOn conseille l’utilisation de trois implémentations pour comparer l’importances des variables d’une forêt aléatoire:\n\nPour la MDI: l’algorithme CIF proposé par Strobl et al. (2007) et implémenté en R\nPour la MDA: l’algorithme Sobol-MDA proposé par Bénard, Da Veiga, and Scornet (2022) et implémenté en R\nPour les valeurs de Shapley : l’alogrithme SHAFF proposé par Bénard et al. (2022) et implémenté en R\n\nEnfin, nous recommandons de combiner plusieurs méthodes pour une analyse plus robuste et de tenir compte des prétraitements des données afin de minimiser les biais potentiels.",
    "crumbs": [
      "Comment bien utiliser les algorithmes?",
      "Guide d'usage des forêts aléatoires"
    ]
  },
  {
    "objectID": "chapters/chapter3/0-intro.html",
    "href": "chapters/chapter3/0-intro.html",
    "title": "1 Comment bien utiliser les méthodes ensemblistes?",
    "section": "",
    "text": "1 Comment bien utiliser les méthodes ensemblistes?\nPrincipe: Conseils + mise en oeuvre pratique.",
    "crumbs": [
      "Comment bien utiliser les algorithmes?"
    ]
  },
  {
    "objectID": "chapters/chapter2/4-boosting.html",
    "href": "chapters/chapter2/4-boosting.html",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "",
    "text": "Le fondement théorique du boosting est un article de de 1990 (Shapire (1990)) qui a démontré théoriquement que, sous certaines conditions, il est possible de transformer un modèle prédictif peu performant en un modèle prédictif très performant. Plus précisément, cet article prouve que s’il est possible de construire un modèle simple dont les prédictions ne sont que légèrement meilleures que le hasard (appelé weak learner), alors il est possible de construire un modèle ayant un pouvoir prédictif arbitrairement élevé (appelé strong learner) en améliorant progressivement ce modèle simple. Le boosting est donc une méthode qui combine une approche ensembliste reposant sur un grand nombre de modèles simples avec un entraînement séquentiel: chaque modèle simple (souvent des arbres de décision peu profonds) tâche d’améliorer la prédiction globale en corrigeant les erreurs des prédictions précédentes à chaque étape. Bien qu’une approche de boosting puisse en théorie mobiliser différentes classes de weak learners, en pratique les weak learners utilisés par les algorithmes de boosting sont presque toujours des arbres de décision.\n\nEn termes plus techniques, les différentes variantes du boosting partagent toutes trois caractéristiques communes:\n\nIls visent à trouver une approximation \\(\\hat{F}\\) d’une fonction inconnue \\(F^{\\ast}: \\mathbf{x} \\mapsto y\\) à partir d’un ensemble d’entraînement \\((y_i, \\mathbf{x_i})_{i= 1,\\dots,n}\\);\nIls supposent que la fonction \\(F^{\\ast}\\) peut être approchée par une somme pondérée de modèles simples \\(f\\) de paramètres \\(\\theta\\):\n\n$ F() = _{m=1}^M _m f(, _m) $\n\nIls reposent sur une modélisation additive par étapes (forward stagewise additive modeling), qui décompose l’entraînement de ce modèle complexe en une séquence d’entraînements de petits modèles. Chaque étape de l’entraînement cherche le modèle simple \\(f\\) qui améliore la puissance prédictive du modèle complet, sans modifier les modèles précédents, puis l’ajoute de façon incrémentale à ces derniers:\n\n$ F_m() = F_{m-1}() + _m f(_i, ) $\n\nMETTRE ICI UNE FIGURE EN UNE DIMENSION, avec des points et des modèles en escalier qui s’affinent.\n\n\n\n\n\nDans les années 1990, de nombreux travaux ont tâché de proposer des mise en application du boosting (Breiman (1998), Grove and Schuurmans (1998)) et ont comparé les mérites des différentes approches. Deux approches ressortent particulièrement de cette littérature: Adaboost (Adaptive Boosting, Freund and Schapire (1997)) et la Gradient Boosting Machine (Friedman (2001)). Ces deux approches reposent sur des principes très différents.\nLe principe d’Adaboost consiste à pondérer les erreurs commises à chaque itération en donnant plus d’importance aux observations mal prédites, de façon à obliger les modèles simples à se concentrer sur les observations les plus difficiles à prédire. Voici une esquisse du fonctionnement d’AdaBoost:\n\nUn premier modèle simple est entraîné sur un jeu d’entraînement dans lequel toutes les observations ont le même poids.\nA l’issue de cette première itération, les observations mal prédites reçoivent une pondération plus élevée que les observations bien prédites, et un deuxième modèle est entraîné sur ce jeu d’entraînement pondéré.\nCe deuxième modèle est ajouté au premier, puis on repondère à nouveau les observations en fonction de la qualité de prédiction de ce nouveau modèle.\nCette procédure est répétée en ajoutant de nouveaux modèles et en ajustant les pondérations.\n\nL’algorithme Adaboost a été au coeur de la littérature sur le boosting à la fin des années 1990 et dans les années 2000, en raison de ses performances sur les problèmes de classification binaire. Il a toutefois été progressivement remplacé par les algorithmes de gradient boosting mis au point quelques années plus tard.\n\n\n\nLa Gradient Boosting Machine (GBM) propose une approche assez différente: elle introduit le gradient boosting en reformulant le boosting sous la forme d’un problème de descente de gradient. Voici une esquisse du fonctionnement de la Gradient Boosting Machine:\n\nUn premier modèle simple est entraîné sur les données d’entraînement, de façon à minimiser une fonction de perte qui mesure l’écart entre la variable à prédire et la prédiction du modèle.\nA l’issue de cette première itération, on calcule la dérivée partielle (gradient) de la fonction de perte par rapport à la prédiction en chaque point de l’ensemble d’entraînement. Ce gradient indique à la fois dans quelle direction et dans quelle ampleur la prédiction devrait être modifiée afin de réduire la perte.\nA la deuxième itération, on ajoute un deuxième modèle qui va tâcher d’améliorer le modèle complet en prédisant le mieux possible l’opposé de ce gradient.\nCe deuxième modèle est ajouté au premier, puis on recalcule la dérivée partielle de la fonction de perte par rapport à la prédiction de ce nouveau modèle.\nCette procédure est répétée en ajoutant de nouveaux modèles et en recalculant le gradient à chaque étape.\nLa qualité du modèle final est évaluée sur un ensemble de test.\n\nLa mécanique de la Gradient Boosting Machine est résumée de façon plus formelle dans l’algorithem REFERENCE.\nCommentaire: On pourrait peut-être donner la version formelle de la GBM, mais c’est peut-être inutile. 1. Initialiser le modèle avec \\(f_0\\left(\\mathbf{x}\\right) = y_0\\). 2. Pour \\(m = 1, \\dots, M:\\) (a) Entraîner le \\(m\\)-ième modèle: $ (m, m) = {, } {i=1}^N L(y_i, f_{m-1}(_i) + b(_i, )) $ (b) Définir \\(f_m\\left(\\mathbf{x}\\right) = f_{m-1}\\left(\\mathbf{x}\\right) + \\hat{\\beta}_m b\\left(\\mathbf{x}_i, \\mathbf{\\hat{\\theta}_m}\\right)\\)\nL’approche de gradient boosting proposée par Friedman (2001) présente deux grands avantages. D’une part, elle peut être utilisée avec n’importe quelle fonction de perte différentiable, ce qui permet d’appliquer le gradient boosting à de multiples problèmes (régression, classification binaire ou multiclasse, learning-to-rank…). D’autre part, elle offre souvent des performances comparables ou supérieures aux autres approches de boosting. Le gradient boosting d’arbres de décision (Gradient boosted Decision Trees - GBDT) est donc devenue l’approche de référence en matière de boosting: toutes les implémentations modernes du gradient boosting comme scikit-learn, XGBoost, LightGBM, et CatBoost sont des extensions et améliorations de la Gradient Boosting Machine.\nAJOUTER ICI LA GBM en pseudo-code\n\n\n\n\nLa méthode de gradient boosting proposée Friedman (2001) a fait l’objet de multiples implémentations intégrant de nombreuses optimisations et raffinements, parmi lesquelles XGBoost (Chen and Guestrin (2016)), LightGBM (Ke et al. (2017)) et CatBoost (Prokhorenkova et al. (2018)). S’il existe quelques différences entre ces implémentations, elles partagent néanmoins la même mécanique d’ensemble, que la section qui suit va présenter en détail en s’appuyant sur l’implémentation proposée par XBGoost.1\nChoses importantes à mettre en avant:\n\nLe boosting est fondamentalement différent des forêts aléatoires. See ESL, chapitre 10.\nToute la mécanique est indépendante de la fonction de perte choisie. En particulier, elle est applicable indifféremment à des problèmes de classification et de régression.\nLe boosting est fait pour overfitter; contrairement aux RF, il n’y a pas de limite à l’overfitting. Donc lutter contre le surapprentissage est un élément particulièrement important de l’usage des algorithmes de gradient boosting.\nLes termes de régularisation sont directement intégrées à la mécanique du gradient boosting.\nComment on interprète le gradient et la hessienne: cas avec une fonction de perte quadratique.\n\n\n\nOn veut entraîner un modèle comprenant \\(K\\) arbres de régression ou de classification:\n\\[ \\hat{y}_{i} =F\\left(\\mathbf{x}_i\\right) = \\sum_{k=1}^{K} f_k\\left(\\mathbf{x}_i\\right) \\]\nChaque arbre \\(f\\) est défini par trois paramètres:\n\nsa structure qui est une fonction \\(q: \\mathbb{R}^m \\rightarrow \\{1, \\dots, T\\}\\) qui à un vecteur \\(\\mathbf{x}\\) de dimension \\(m\\) associe une feuille terminale de l’arbre;\nson nombre de feuilles terminales \\(T\\);\nles valeurs figurant sur ses feuilles terminales \\(\\mathbf{w}\\in \\mathbb{R}^T\\) (appelées poids ou weights).\n\nLe modèle est entraîné avec une fonction-objectif constituée d’une fonction de perte \\(l\\) et d’une fonction de régularisation \\(\\Omega\\):\n\nLa fonction de perte mesure la distance entre la prédiction \\(\\hat{y}\\) et la vraie valeur \\(y\\) et présente généralement les propriétés suivantes: elle est convexe et dérivable deux fois, et atteint son minimum lorsque \\(\\hat{y} = y\\).\nLa fonction de régularisation pénalise la complexité du modèle. Dans le cas présent, elle pénalise les arbres avec un grand nombre de feuilles (\\(T\\) élevé) et les arbres avec des poids élevés (\\(w_j\\) élevés en valeur absolue).\n\n\\[ \\mathcal{L}(\\phi) = \\underbrace{\\sum_i l(\\hat{y}_{i}, y_{i})}_{\\substack{\\text{Perte sur les} \\\\ \\text{observations}}} + \\underbrace{\\sum_k \\Omega(f_{k})}_{\\substack{\\text{Fonction de} \\\\ \\text{régularisation}}}\\,\\,\\text{avec}\\,\\,\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{k=1}^K \\sum_{j=1}^{T_k} w_j^2\n\\tag{1}\\]\nUn point essentiel est que toute la mécanique du gradient boosting est indépendante de la fonction de perte choisie et de la nature du problème modélisé. En particulier, toutes les formules restent inchangées, que l’on traite un un problème de classification, de régression ou de classement (ranking). C’est là l’une des grandes forces du gradient boosting: un seul cadre conceptuel permet de traiter de multiples situations.\n\n\n\nLa fonction-objectif introduite précédemment est très complexe et ne peut être utilisée directement pour entraîner le modèle, car il faudrait entraîner tous les arbres en même temps. On reformule donc cette fonction objectif de façon à isoler le \\(t\\)-ième arbre, qui pourra ensuite être entraîné seul, une fois que les \\(t-1\\) arbres précédents auront été entraînés. Pour cela, on note \\(F_t(x) = \\hat{y}_i^{(t)}\\) la prédiction à l’issue de l’étape \\(t\\): \\(\\hat{y}_i^{(t)} = \\sum_{k=1}^t f_k(\\mathbf{x}_i)\\), et on note \\(\\mathcal{L}^{(t)}\\) la fonction-objectif au moment de l’entraînement du \\(t\\)-ième arbre:\n\\[\n\\begin{aligned}\n\\mathcal{L}^{(t)}\n&= \\sum_{i=1}^{n} l(y_i, \\hat{y}_{i}^{(t)}) + \\sum_{k=1}^t\\Omega(f_k) \\\\\n&= \\sum_{i=1}^{n} l\\left(y_i, \\hat{y}_{i}^{(t-1)} + f_{t}(\\mathbf{x}_i)\\right) + \\Omega(f_t) + constant\n\\end{aligned}\n\\]\n\n\n\nUne fois isolé le \\(t\\)-ième arbre, on fait un développement limité d’ordre 2 de \\(l(y_i, \\hat{y}_{i}^{(t-1)} + f_{t}(\\mathbf{x}_i))\\) au voisinage de \\(\\hat{y}_{i}^{(t-1)}\\), en considérant que la prédiction du \\(t\\)-ième arbre \\(f_{t}(\\mathbf{x}_i)\\) est un incrément de petite taille:\n\\[ \\mathcal{L}^{(t)} \\approx \\sum_{i=1}^{n} [\\underbrace{l(y_i, \\hat{y}_{i}^{(t-1)})}_{\\text{(A)}} + g_i f_t(\\mathbf{x}_i)+ \\frac{1}{2} h_i f^2_t(\\mathbf{x}_i)] + \\underbrace{\\sum_{j=1}^{t-1}\\Omega(f_j)}_{\\text{(B)}} + \\Omega(f_t) \\]\navec\n\\[ g_i = \\frac{\\partial l(y_i, \\hat{y}_i^{(t-1)})}{\\partial\\hat{y}_i^{(t-1)}} \\text{et} h_i = \\frac{\\partial^2 l(y_i, \\hat{y}_i^{(t-1)})}{{\\partial \\hat{y}_i^{(t-1)}}^2} \\]\nLes termes \\(g_i\\) et \\(h_i\\) désignent respectivement la dérivée première (le gradient) et la dérivée seconde (la hessienne) de la fonction de perte par rapport à la variable prédite. Dans cette équation, les termes (A) et (B) sont constants car les \\(t-1\\) arbres précédents ont déjà été entraînés et ne sont pas modifiés par l’entraînement du \\(t\\)-ième arbre.  On peut donc retirer ces termes pour obtenir la fonction-objectif simplifiée \\(\\tilde{L}^{(t)}\\) qui sera utilisée pour l’entraînement du \\(t\\)-ième arbre.\n\\[ \\mathcal{\\tilde{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i)+ \\frac{1}{2} h_i f^2_t(\\mathbf{x}_i)] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_i^2\n\\tag{2}\\]\n\n\n\n\nA partir de l’équation 2, il est possible de faire apparaître les poids \\(w_j\\) du \\(t\\)-ième arbre. Pour un arbre donné comprenant \\(T\\) feuilles (\\(q: \\mathbb{R}^T \\rightarrow \\{1, \\dots, T\\}\\)), on définit \\(I_j = \\{ i | q(\\mathbf{x}_i) = j \\}\\) l’ensemble des observations situées sur la feuille terminale \\(j\\), et \\(w_j\\) la valeur prédite par l’arbre pour la feuille \\(j\\). Avec cette notation, on réorganise \\(\\mathcal{\\tilde{L}}^{(t)}\\) en regroupant les observations selon la feuille terminale à laquelle elles appartiennent:\n\\[\n\\begin{align*}\n\\mathcal{\\tilde{L}}^{(t)} =&   \\sum_{j=1}^{T} \\sum_{i\\in I_{j}} \\bigg[g_i f_t(\\mathbf{x}_i)\\phantom{\\frac{1}{2}} &+ \\frac{1}{2} h_i f^2_t(\\mathbf{x}_i)\\bigg]&+ \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_i^2 \\\\\n     &= \\sum_{j=1}^{T} \\sum_{i\\in I_{j}} \\bigg[g_i w_j &+ \\frac{1}{2} h_i w_j^2\\bigg] &+ \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_i^2 \\\\\n     &= \\sum^T_{j=1} \\bigg[w_j\\sum_{i\\in I_{j}} g_i &+ \\frac{1}{2} w_j^2 \\left( \\sum_{i \\in I_{j}} h_i + \\lambda \\right) \\bigg] &+ \\gamma T\n\\end{align*}\n\\]\nDans la dernière expression, la fonction de perte simplifiée se reformule comme une combinaison quadratique des poids \\(w_j\\), dans laquelle les dérivées première et seconde de la fonction de perte interviennent sous forme de pondérations (\\(\\sum_{i \\in I_j} g_i\\) et \\(\\sum_{i \\in I_j} h_i\\)). Pour un arbre donné, les poids optimaux \\(w_j\\) sont ceux minimisent cette fonction de perte, compte tenu de ces pondérations. Il se trouve que le calcul de ces poids optimaux est très simple: le poids optimal \\(w_j^{\\ast}\\) de la feuille \\(j\\) est donné par l’équation:\n\\[ w_j^{\\ast} = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}  \\tag{3}\\]\n\n\n\nEn combinant les équations 2 et 3, on déduit que la valeur optimale de la fonction objectif pour l’arbre \\(q\\) est égale à\n\\[ \\mathcal{\\tilde{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^T \\frac{\\left(\\sum_{i\\in I_j} g_i\\right)^2}{\\sum_{i\\in I_j} h_i+\\lambda} + \\gamma T  \\tag{4}\\]\nCette équation est utile car elle permet de comparer la qualité de deux arbres, et de déterminer lequel est le meilleur. On pourrait penser que l’équation 4 est à elle seule suffisante pour choisir le \\(t\\)-ième arbre: il suffirait d’énumérer les arbres possibles, de calculer la qualité de chacun d’entre eux, et de retenir le meilleur. Bien que cette approche soit possible en théorie, elle est inemployable en pratique car le nombre d’arbres possibles est extrêmement élevé. Par conséquent, le \\(t\\)-ième arbre n’est pas défini en une fois, mais construit de façon gloutonne (greedy), en utilisant l’équation 4 à chaque étape.\nImaginons qu’on envisage de décomposer la feuille \\(I\\) en deux nouvelles feuilles \\(I_L\\) et \\(I_R\\) (avec \\(I = I_L \\cup I_R\\)), selon une condition logique reposant sur une variable et une valeur de cette variable (exemple: \\(x_6 &gt; 11\\)). Par application de l’équation 4, le gain potentiel induit par ce critère de partition est égal à:\n\\[ Gain = \\frac{1}{2} \\left[\\frac{G_L^2}{H_L+\\lambda}+\\frac{G_R^2}{H_R+\\lambda}-\\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda}\\right] - \\gamma \\,\\,\\text{avec}\\,\\, G = \\sum_i g_i \\,\\,\\text{et}\\,\\, H = \\sum_i H_i \\tag{5}\\]\n\nL’équation 5 est au coeur de la mécanique du gradient boosting car elle permet de comparer les critères de partition possibles.\n\n\n\nLa méthode de construction des arbres dans les algorithmes de gradient boosting est identique à celle décrite dans la partie REFERENCE A LA PARTIE CART/RF, à une différence près: ces algorithmes utilisent l’équation 4 pour choisir la condition de partition (split) à chaque étape de la construction de l’arbre. Sur tous les autres points la mécanique est identique à celle des arbres CART:\n\nOn commence par le noeud-racine comprenant l’ensemble des données d’entraînement, et on recherche le critère de partition des données qui maximise le gain mesuré par l’équation 4. Puis on recherche le critère de partition optimal pour chacun des deux noeuds-enfants, et ainsi de suite jusqu’à ce qu’un critère d’arrêt soit atteint.\nPour chaque noeud, l’algorithme de détermination des critère de partition (split finding algorithm) consiste en une double boucle sur les variables et les valeurs prises par ces variables, qui énumère un grand nombre de critères de partition possibles et mesure le gain associé à chacun d’entre eux avec l’équation 5. Le critère de partition retenu sera simplement celui dont le gain est le plus élevé.\n\nL’algorithme de détermination des critère de partition est le composant le plus intense en calcul des algorithmes de gradient boosting. Les différentes implémentations du gradient boosting proposent donc de multiples améliorations et optimisations visant à le rendre le plus efficace possible. Certaines de ces optimisations sont présentées dans la partie LIEN A LA PARTIE HISTOGRAMME/CATVAR.\n\n\n\nUne fois qu’un arbre a été entraîné, on met à jour le modèle par la formule suivante:\n\\[ F_{m}(x)=F_{m-1}(x)+ \\eta f_{m}(x)  \\tag{6}\\]\nIl est alors possible de commencer l’entraînement de l’arbre suivant, selon la même logique que précédemment. La raison d’être du paramètre \\(\\eta\\) est expliquée dans le paragraphe suivant.\n\n\n\n\n\n\nle shrinkage;\nle subsampling des lignes et des colonnes;\nles différentes pénalisations:\n\\(\\lambda\\): privilégier les poids faibles;\n\\(\\gamma\\): privilégier les arbres courts et les splits porteurs de sens.\n\n\n\n\n\n\nles variables catégorielles:\n\nordonnées: passer en integer;\nnon-ordonnées: OHE ou approche de Fisher.\n\nles variables continues:\n\ninutile de faire des transformations monotones.\nUtile d’ajouter des transformations non monotones.",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "Le *boosting*"
    ]
  },
  {
    "objectID": "chapters/chapter2/4-boosting.html#le-boosting",
    "href": "chapters/chapter2/4-boosting.html#le-boosting",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "",
    "text": "Le fondement théorique du boosting est un article de de 1990 (Shapire (1990)) qui a démontré théoriquement que, sous certaines conditions, il est possible de transformer un modèle prédictif peu performant en un modèle prédictif très performant. Plus précisément, cet article prouve que s’il est possible de construire un modèle simple dont les prédictions ne sont que légèrement meilleures que le hasard (appelé weak learner), alors il est possible de construire un modèle ayant un pouvoir prédictif arbitrairement élevé (appelé strong learner) en améliorant progressivement ce modèle simple. Le boosting est donc une méthode qui combine une approche ensembliste reposant sur un grand nombre de modèles simples avec un entraînement séquentiel: chaque modèle simple (souvent des arbres de décision peu profonds) tâche d’améliorer la prédiction globale en corrigeant les erreurs des prédictions précédentes à chaque étape. Bien qu’une approche de boosting puisse en théorie mobiliser différentes classes de weak learners, en pratique les weak learners utilisés par les algorithmes de boosting sont presque toujours des arbres de décision.\n\nEn termes plus techniques, les différentes variantes du boosting partagent toutes trois caractéristiques communes:\n\nIls visent à trouver une approximation \\(\\hat{F}\\) d’une fonction inconnue \\(F^{\\ast}: \\mathbf{x} \\mapsto y\\) à partir d’un ensemble d’entraînement \\((y_i, \\mathbf{x_i})_{i= 1,\\dots,n}\\);\nIls supposent que la fonction \\(F^{\\ast}\\) peut être approchée par une somme pondérée de modèles simples \\(f\\) de paramètres \\(\\theta\\):\n\n$ F() = _{m=1}^M _m f(, _m) $\n\nIls reposent sur une modélisation additive par étapes (forward stagewise additive modeling), qui décompose l’entraînement de ce modèle complexe en une séquence d’entraînements de petits modèles. Chaque étape de l’entraînement cherche le modèle simple \\(f\\) qui améliore la puissance prédictive du modèle complet, sans modifier les modèles précédents, puis l’ajoute de façon incrémentale à ces derniers:\n\n$ F_m() = F_{m-1}() + _m f(_i, ) $\n\nMETTRE ICI UNE FIGURE EN UNE DIMENSION, avec des points et des modèles en escalier qui s’affinent.\n\n\n\n\n\nDans les années 1990, de nombreux travaux ont tâché de proposer des mise en application du boosting (Breiman (1998), Grove and Schuurmans (1998)) et ont comparé les mérites des différentes approches. Deux approches ressortent particulièrement de cette littérature: Adaboost (Adaptive Boosting, Freund and Schapire (1997)) et la Gradient Boosting Machine (Friedman (2001)). Ces deux approches reposent sur des principes très différents.\nLe principe d’Adaboost consiste à pondérer les erreurs commises à chaque itération en donnant plus d’importance aux observations mal prédites, de façon à obliger les modèles simples à se concentrer sur les observations les plus difficiles à prédire. Voici une esquisse du fonctionnement d’AdaBoost:\n\nUn premier modèle simple est entraîné sur un jeu d’entraînement dans lequel toutes les observations ont le même poids.\nA l’issue de cette première itération, les observations mal prédites reçoivent une pondération plus élevée que les observations bien prédites, et un deuxième modèle est entraîné sur ce jeu d’entraînement pondéré.\nCe deuxième modèle est ajouté au premier, puis on repondère à nouveau les observations en fonction de la qualité de prédiction de ce nouveau modèle.\nCette procédure est répétée en ajoutant de nouveaux modèles et en ajustant les pondérations.\n\nL’algorithme Adaboost a été au coeur de la littérature sur le boosting à la fin des années 1990 et dans les années 2000, en raison de ses performances sur les problèmes de classification binaire. Il a toutefois été progressivement remplacé par les algorithmes de gradient boosting mis au point quelques années plus tard.\n\n\n\nLa Gradient Boosting Machine (GBM) propose une approche assez différente: elle introduit le gradient boosting en reformulant le boosting sous la forme d’un problème de descente de gradient. Voici une esquisse du fonctionnement de la Gradient Boosting Machine:\n\nUn premier modèle simple est entraîné sur les données d’entraînement, de façon à minimiser une fonction de perte qui mesure l’écart entre la variable à prédire et la prédiction du modèle.\nA l’issue de cette première itération, on calcule la dérivée partielle (gradient) de la fonction de perte par rapport à la prédiction en chaque point de l’ensemble d’entraînement. Ce gradient indique à la fois dans quelle direction et dans quelle ampleur la prédiction devrait être modifiée afin de réduire la perte.\nA la deuxième itération, on ajoute un deuxième modèle qui va tâcher d’améliorer le modèle complet en prédisant le mieux possible l’opposé de ce gradient.\nCe deuxième modèle est ajouté au premier, puis on recalcule la dérivée partielle de la fonction de perte par rapport à la prédiction de ce nouveau modèle.\nCette procédure est répétée en ajoutant de nouveaux modèles et en recalculant le gradient à chaque étape.\nLa qualité du modèle final est évaluée sur un ensemble de test.\n\nLa mécanique de la Gradient Boosting Machine est résumée de façon plus formelle dans l’algorithem REFERENCE.\nCommentaire: On pourrait peut-être donner la version formelle de la GBM, mais c’est peut-être inutile. 1. Initialiser le modèle avec \\(f_0\\left(\\mathbf{x}\\right) = y_0\\). 2. Pour \\(m = 1, \\dots, M:\\) (a) Entraîner le \\(m\\)-ième modèle: $ (m, m) = {, } {i=1}^N L(y_i, f_{m-1}(_i) + b(_i, )) $ (b) Définir \\(f_m\\left(\\mathbf{x}\\right) = f_{m-1}\\left(\\mathbf{x}\\right) + \\hat{\\beta}_m b\\left(\\mathbf{x}_i, \\mathbf{\\hat{\\theta}_m}\\right)\\)\nL’approche de gradient boosting proposée par Friedman (2001) présente deux grands avantages. D’une part, elle peut être utilisée avec n’importe quelle fonction de perte différentiable, ce qui permet d’appliquer le gradient boosting à de multiples problèmes (régression, classification binaire ou multiclasse, learning-to-rank…). D’autre part, elle offre souvent des performances comparables ou supérieures aux autres approches de boosting. Le gradient boosting d’arbres de décision (Gradient boosted Decision Trees - GBDT) est donc devenue l’approche de référence en matière de boosting: toutes les implémentations modernes du gradient boosting comme scikit-learn, XGBoost, LightGBM, et CatBoost sont des extensions et améliorations de la Gradient Boosting Machine.\nAJOUTER ICI LA GBM en pseudo-code\n\n\n\n\nLa méthode de gradient boosting proposée Friedman (2001) a fait l’objet de multiples implémentations intégrant de nombreuses optimisations et raffinements, parmi lesquelles XGBoost (Chen and Guestrin (2016)), LightGBM (Ke et al. (2017)) et CatBoost (Prokhorenkova et al. (2018)). S’il existe quelques différences entre ces implémentations, elles partagent néanmoins la même mécanique d’ensemble, que la section qui suit va présenter en détail en s’appuyant sur l’implémentation proposée par XBGoost.1\nChoses importantes à mettre en avant:\n\nLe boosting est fondamentalement différent des forêts aléatoires. See ESL, chapitre 10.\nToute la mécanique est indépendante de la fonction de perte choisie. En particulier, elle est applicable indifféremment à des problèmes de classification et de régression.\nLe boosting est fait pour overfitter; contrairement aux RF, il n’y a pas de limite à l’overfitting. Donc lutter contre le surapprentissage est un élément particulièrement important de l’usage des algorithmes de gradient boosting.\nLes termes de régularisation sont directement intégrées à la mécanique du gradient boosting.\nComment on interprète le gradient et la hessienne: cas avec une fonction de perte quadratique.\n\n\n\nOn veut entraîner un modèle comprenant \\(K\\) arbres de régression ou de classification:\n\\[ \\hat{y}_{i} =F\\left(\\mathbf{x}_i\\right) = \\sum_{k=1}^{K} f_k\\left(\\mathbf{x}_i\\right) \\]\nChaque arbre \\(f\\) est défini par trois paramètres:\n\nsa structure qui est une fonction \\(q: \\mathbb{R}^m \\rightarrow \\{1, \\dots, T\\}\\) qui à un vecteur \\(\\mathbf{x}\\) de dimension \\(m\\) associe une feuille terminale de l’arbre;\nson nombre de feuilles terminales \\(T\\);\nles valeurs figurant sur ses feuilles terminales \\(\\mathbf{w}\\in \\mathbb{R}^T\\) (appelées poids ou weights).\n\nLe modèle est entraîné avec une fonction-objectif constituée d’une fonction de perte \\(l\\) et d’une fonction de régularisation \\(\\Omega\\):\n\nLa fonction de perte mesure la distance entre la prédiction \\(\\hat{y}\\) et la vraie valeur \\(y\\) et présente généralement les propriétés suivantes: elle est convexe et dérivable deux fois, et atteint son minimum lorsque \\(\\hat{y} = y\\).\nLa fonction de régularisation pénalise la complexité du modèle. Dans le cas présent, elle pénalise les arbres avec un grand nombre de feuilles (\\(T\\) élevé) et les arbres avec des poids élevés (\\(w_j\\) élevés en valeur absolue).\n\n\\[ \\mathcal{L}(\\phi) = \\underbrace{\\sum_i l(\\hat{y}_{i}, y_{i})}_{\\substack{\\text{Perte sur les} \\\\ \\text{observations}}} + \\underbrace{\\sum_k \\Omega(f_{k})}_{\\substack{\\text{Fonction de} \\\\ \\text{régularisation}}}\\,\\,\\text{avec}\\,\\,\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{k=1}^K \\sum_{j=1}^{T_k} w_j^2\n\\tag{1}\\]\nUn point essentiel est que toute la mécanique du gradient boosting est indépendante de la fonction de perte choisie et de la nature du problème modélisé. En particulier, toutes les formules restent inchangées, que l’on traite un un problème de classification, de régression ou de classement (ranking). C’est là l’une des grandes forces du gradient boosting: un seul cadre conceptuel permet de traiter de multiples situations.\n\n\n\nLa fonction-objectif introduite précédemment est très complexe et ne peut être utilisée directement pour entraîner le modèle, car il faudrait entraîner tous les arbres en même temps. On reformule donc cette fonction objectif de façon à isoler le \\(t\\)-ième arbre, qui pourra ensuite être entraîné seul, une fois que les \\(t-1\\) arbres précédents auront été entraînés. Pour cela, on note \\(F_t(x) = \\hat{y}_i^{(t)}\\) la prédiction à l’issue de l’étape \\(t\\): \\(\\hat{y}_i^{(t)} = \\sum_{k=1}^t f_k(\\mathbf{x}_i)\\), et on note \\(\\mathcal{L}^{(t)}\\) la fonction-objectif au moment de l’entraînement du \\(t\\)-ième arbre:\n\\[\n\\begin{aligned}\n\\mathcal{L}^{(t)}\n&= \\sum_{i=1}^{n} l(y_i, \\hat{y}_{i}^{(t)}) + \\sum_{k=1}^t\\Omega(f_k) \\\\\n&= \\sum_{i=1}^{n} l\\left(y_i, \\hat{y}_{i}^{(t-1)} + f_{t}(\\mathbf{x}_i)\\right) + \\Omega(f_t) + constant\n\\end{aligned}\n\\]\n\n\n\nUne fois isolé le \\(t\\)-ième arbre, on fait un développement limité d’ordre 2 de \\(l(y_i, \\hat{y}_{i}^{(t-1)} + f_{t}(\\mathbf{x}_i))\\) au voisinage de \\(\\hat{y}_{i}^{(t-1)}\\), en considérant que la prédiction du \\(t\\)-ième arbre \\(f_{t}(\\mathbf{x}_i)\\) est un incrément de petite taille:\n\\[ \\mathcal{L}^{(t)} \\approx \\sum_{i=1}^{n} [\\underbrace{l(y_i, \\hat{y}_{i}^{(t-1)})}_{\\text{(A)}} + g_i f_t(\\mathbf{x}_i)+ \\frac{1}{2} h_i f^2_t(\\mathbf{x}_i)] + \\underbrace{\\sum_{j=1}^{t-1}\\Omega(f_j)}_{\\text{(B)}} + \\Omega(f_t) \\]\navec\n\\[ g_i = \\frac{\\partial l(y_i, \\hat{y}_i^{(t-1)})}{\\partial\\hat{y}_i^{(t-1)}} \\text{et} h_i = \\frac{\\partial^2 l(y_i, \\hat{y}_i^{(t-1)})}{{\\partial \\hat{y}_i^{(t-1)}}^2} \\]\nLes termes \\(g_i\\) et \\(h_i\\) désignent respectivement la dérivée première (le gradient) et la dérivée seconde (la hessienne) de la fonction de perte par rapport à la variable prédite. Dans cette équation, les termes (A) et (B) sont constants car les \\(t-1\\) arbres précédents ont déjà été entraînés et ne sont pas modifiés par l’entraînement du \\(t\\)-ième arbre.  On peut donc retirer ces termes pour obtenir la fonction-objectif simplifiée \\(\\tilde{L}^{(t)}\\) qui sera utilisée pour l’entraînement du \\(t\\)-ième arbre.\n\\[ \\mathcal{\\tilde{L}}^{(t)} = \\sum_{i=1}^{n} [g_i f_t(\\mathbf{x}_i)+ \\frac{1}{2} h_i f^2_t(\\mathbf{x}_i)] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_i^2\n\\tag{2}\\]\n\n\n\n\nA partir de l’équation 2, il est possible de faire apparaître les poids \\(w_j\\) du \\(t\\)-ième arbre. Pour un arbre donné comprenant \\(T\\) feuilles (\\(q: \\mathbb{R}^T \\rightarrow \\{1, \\dots, T\\}\\)), on définit \\(I_j = \\{ i | q(\\mathbf{x}_i) = j \\}\\) l’ensemble des observations situées sur la feuille terminale \\(j\\), et \\(w_j\\) la valeur prédite par l’arbre pour la feuille \\(j\\). Avec cette notation, on réorganise \\(\\mathcal{\\tilde{L}}^{(t)}\\) en regroupant les observations selon la feuille terminale à laquelle elles appartiennent:\n\\[\n\\begin{align*}\n\\mathcal{\\tilde{L}}^{(t)} =&   \\sum_{j=1}^{T} \\sum_{i\\in I_{j}} \\bigg[g_i f_t(\\mathbf{x}_i)\\phantom{\\frac{1}{2}} &+ \\frac{1}{2} h_i f^2_t(\\mathbf{x}_i)\\bigg]&+ \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_i^2 \\\\\n     &= \\sum_{j=1}^{T} \\sum_{i\\in I_{j}} \\bigg[g_i w_j &+ \\frac{1}{2} h_i w_j^2\\bigg] &+ \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_i^2 \\\\\n     &= \\sum^T_{j=1} \\bigg[w_j\\sum_{i\\in I_{j}} g_i &+ \\frac{1}{2} w_j^2 \\left( \\sum_{i \\in I_{j}} h_i + \\lambda \\right) \\bigg] &+ \\gamma T\n\\end{align*}\n\\]\nDans la dernière expression, la fonction de perte simplifiée se reformule comme une combinaison quadratique des poids \\(w_j\\), dans laquelle les dérivées première et seconde de la fonction de perte interviennent sous forme de pondérations (\\(\\sum_{i \\in I_j} g_i\\) et \\(\\sum_{i \\in I_j} h_i\\)). Pour un arbre donné, les poids optimaux \\(w_j\\) sont ceux minimisent cette fonction de perte, compte tenu de ces pondérations. Il se trouve que le calcul de ces poids optimaux est très simple: le poids optimal \\(w_j^{\\ast}\\) de la feuille \\(j\\) est donné par l’équation:\n\\[ w_j^{\\ast} = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}  \\tag{3}\\]\n\n\n\nEn combinant les équations 2 et 3, on déduit que la valeur optimale de la fonction objectif pour l’arbre \\(q\\) est égale à\n\\[ \\mathcal{\\tilde{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^T \\frac{\\left(\\sum_{i\\in I_j} g_i\\right)^2}{\\sum_{i\\in I_j} h_i+\\lambda} + \\gamma T  \\tag{4}\\]\nCette équation est utile car elle permet de comparer la qualité de deux arbres, et de déterminer lequel est le meilleur. On pourrait penser que l’équation 4 est à elle seule suffisante pour choisir le \\(t\\)-ième arbre: il suffirait d’énumérer les arbres possibles, de calculer la qualité de chacun d’entre eux, et de retenir le meilleur. Bien que cette approche soit possible en théorie, elle est inemployable en pratique car le nombre d’arbres possibles est extrêmement élevé. Par conséquent, le \\(t\\)-ième arbre n’est pas défini en une fois, mais construit de façon gloutonne (greedy), en utilisant l’équation 4 à chaque étape.\nImaginons qu’on envisage de décomposer la feuille \\(I\\) en deux nouvelles feuilles \\(I_L\\) et \\(I_R\\) (avec \\(I = I_L \\cup I_R\\)), selon une condition logique reposant sur une variable et une valeur de cette variable (exemple: \\(x_6 &gt; 11\\)). Par application de l’équation 4, le gain potentiel induit par ce critère de partition est égal à:\n\\[ Gain = \\frac{1}{2} \\left[\\frac{G_L^2}{H_L+\\lambda}+\\frac{G_R^2}{H_R+\\lambda}-\\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda}\\right] - \\gamma \\,\\,\\text{avec}\\,\\, G = \\sum_i g_i \\,\\,\\text{et}\\,\\, H = \\sum_i H_i \\tag{5}\\]\n\nL’équation 5 est au coeur de la mécanique du gradient boosting car elle permet de comparer les critères de partition possibles.\n\n\n\nLa méthode de construction des arbres dans les algorithmes de gradient boosting est identique à celle décrite dans la partie REFERENCE A LA PARTIE CART/RF, à une différence près: ces algorithmes utilisent l’équation 4 pour choisir la condition de partition (split) à chaque étape de la construction de l’arbre. Sur tous les autres points la mécanique est identique à celle des arbres CART:\n\nOn commence par le noeud-racine comprenant l’ensemble des données d’entraînement, et on recherche le critère de partition des données qui maximise le gain mesuré par l’équation 4. Puis on recherche le critère de partition optimal pour chacun des deux noeuds-enfants, et ainsi de suite jusqu’à ce qu’un critère d’arrêt soit atteint.\nPour chaque noeud, l’algorithme de détermination des critère de partition (split finding algorithm) consiste en une double boucle sur les variables et les valeurs prises par ces variables, qui énumère un grand nombre de critères de partition possibles et mesure le gain associé à chacun d’entre eux avec l’équation 5. Le critère de partition retenu sera simplement celui dont le gain est le plus élevé.\n\nL’algorithme de détermination des critère de partition est le composant le plus intense en calcul des algorithmes de gradient boosting. Les différentes implémentations du gradient boosting proposent donc de multiples améliorations et optimisations visant à le rendre le plus efficace possible. Certaines de ces optimisations sont présentées dans la partie LIEN A LA PARTIE HISTOGRAMME/CATVAR.\n\n\n\nUne fois qu’un arbre a été entraîné, on met à jour le modèle par la formule suivante:\n\\[ F_{m}(x)=F_{m-1}(x)+ \\eta f_{m}(x)  \\tag{6}\\]\nIl est alors possible de commencer l’entraînement de l’arbre suivant, selon la même logique que précédemment. La raison d’être du paramètre \\(\\eta\\) est expliquée dans le paragraphe suivant.\n\n\n\n\n\n\nle shrinkage;\nle subsampling des lignes et des colonnes;\nles différentes pénalisations:\n\\(\\lambda\\): privilégier les poids faibles;\n\\(\\gamma\\): privilégier les arbres courts et les splits porteurs de sens.\n\n\n\n\n\n\nles variables catégorielles:\n\nordonnées: passer en integer;\nnon-ordonnées: OHE ou approche de Fisher.\n\nles variables continues:\n\ninutile de faire des transformations monotones.\nUtile d’ajouter des transformations non monotones.",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "Le *boosting*"
    ]
  },
  {
    "objectID": "chapters/chapter2/4-boosting.html#footnotes",
    "href": "chapters/chapter2/4-boosting.html#footnotes",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCette partie reprend la structure et les notations de la partie 2 de Chen and Guestrin (2016).↩︎",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "Le *boosting*"
    ]
  },
  {
    "objectID": "chapters/chapter2/2-bagging.html",
    "href": "chapters/chapter2/2-bagging.html",
    "title": "1 Le bagging",
    "section": "",
    "text": "Le bagging, ou “bootstrap aggregating”, est une méthode ensembliste qui vise à améliorer la stabilité et la précision des algorithmes d’apprentissage automatique en agrégeant plusieurs modèles (Breiman (1996)). Chaque modèle est entraîné sur un échantillon distinct généré par une technique de rééchantillonnage (bootstrap). Ces modèles sont ensuite combinés pour produire une prédiction agrégée, souvent plus robuste et généralisable que celle obtenue par un modèle unique.\n\n\n\nLe bagging comporte trois étapes principales:\n\nL’échantillonnage bootstrap : L’échantillonnage bootstrap consiste à créer des échantillons distincts en tirant aléatoirement avec remise des observations du jeu de données initial. Chaque échantillon bootstrap contient le même nombre d’observations que le jeu de données initial, mais certaines observations sont répétées (car sélectionnées plusieurs fois), tandis que d’autres sont omises.\nL’entraînement de plusieurs modèles : Un modèle (aussi appelé apprenant de base ou weak learner) est entraîné sur chaque échantillon bootstrap. Les modèles peuvent être des arbres de décision, des régressions ou tout autre algorithme d’apprentissage. Le bagging est particulièrement efficace avec des modèles instables, tels que les arbres de décision non élagués.\nL’agrégation des prédictions : Les prédictions de tous les modèles sont ensuite agrégées, en procédant généralement à la moyenne (ou à la médiane) des prédictions dans le cas de la régression, et au vote majoritaire (ou à la moyenne des probabilités prédites pour chaque classe) dans le cas de la classification, afin d’obtenir des prédictions plus précises et généralisables.\n\n\n\n\nCertains modèles sont très sensibles aux données d’entraînement, et leurs prédictions sont très instables d’un échantillon à l’autre. L’objectif du bagging est de construire un prédicteur plus précis en agrégeant les prédictions de plusieurs modèles entraînés sur des échantillons (légèrement) différents les uns des autres.\nBreiman (1996) montre que cette méthode est particulièrement efficace lorsqu’elle est appliquée à des modèles très instables, dont les performances sont particulièrement sensibles aux variations du jeu de données d’entraînement, et peu biaisés.\nCette section vise à mieux comprendre comment (et sous quelles conditions) l’agrégation par bagging permet de construire un prédicteur plus performant.\nDans la suite, nous notons \\(φ(x, L)\\) un prédicteur (d’une valeur numérique dans le cas de la régression ou d’un label dans le cas de la classification), entraîné sur un ensemble d’apprentissage \\(L\\), et prenant en entrée un vecteur de caractéristiques \\(x\\).\n\n\nDans le contexte de la régression, l’objectif est de prédire une valeur numérique \\(Y\\) à partir d’un vecteur de caractéristiques \\(x\\). Un modèle de régression \\(\\phi(x, L)\\) est construit à partir d’un ensemble d’apprentissage \\(L\\), et produit une estimation de \\(Y\\) pour chaque observation \\(x\\).\n\n\nDans le cas de la régression, le prédicteur agrégé est défini comme suit :\n$ _A(x) = E_L[(x, L)] $\noù \\(\\phi_A(x)\\) représente la prédiction agrégée, \\(E_L[.]\\) correspond à l’espérance prise sur tous les échantillons d’apprentissage possibles \\(L\\), chacun étant tiré selon la même distribution que le jeu de données initial, et \\(\\phi(x, L)\\) correspond à la prédiction du modèle construit sur l’échantillon d’apprentissage \\(L\\).\n\n\n\nPour mieux comprendre comment l’agrégation améliore la performance globale d’un modèle individuel \\(\\phi(x, L)\\), revenons à la décomposition biais-variance de l’erreur quadratique moyenne (il s’agit de la mesure de performance classiquement considérée dans un problème de régression):\n\\[E_L[\\left(Y - \\phi(x, L)\\right)^2] = \\underbrace{\\left(E_L\\left[\\phi(x, L) - Y\\right]\\right)^2}_{\\text{Biais}^2} + \\underbrace{E_L[\\left(\\phi(x, L) - E_L[\\phi(x, L)]\\right)^2]}_{\\text{Variance}} \\tag{1}\\]\n\nLe biais est la différence entre la valeur observée \\(Y\\) que l’on souhaite prédire et la prédiction moyenne \\(E_L[\\phi(x, L)]\\). Si le modèle est sous-ajusté, le biais sera élevé.\nLa variance est la variabilité des prédictions (\\(\\phi(x, L)\\)) autour de leur moyenne (\\(E_L[\\phi(x, L)]\\)). Un modèle avec une variance élevée est très sensible aux fluctuations au sein des données d’entraînement: ses prédictions varient beaucoup lorsque les données d’entraînement se modifient.\n\nL’équation 1 illustre l’arbitrage biais-variance qui est omniprésent en machine learning: plus la complexité d’un modèle s’accroît (exemple: la profondeur d’un arbre), plus son biais sera plus faible (car ses prédictions seront de plus en plus proches des données d’entraînement), et plus sa variance sera élevée (car ses prédictions, étant très proches des données d’entraînement, auront tendance à varier fortement d’un jeu d’entraînement à l’autre).\n\n\n\nBreiman (1996) compare l’erreur quadratique moyenne d’un modèle individuel avec celle du modèle agrégé et démontre l’inégalité suivante :\n\n$ (Y - _A(x))^2 E_L[(Y - (x, L))^2] $ {#eq-inegalite-breiman1996}\n\nLe terme \\((Y - \\phi_A(x))^2\\) représente l’erreur quadratique du prédicteur agrégé \\(\\phi_A(x)\\);\nLe terme \\(E_L[(Y - \\phi(x, L))^2]\\) est l’erreur quadratique moyenne d’un prédicteur individuel \\(\\phi(x, L)\\) entraîné sur un échantillon aléatoire \\(L\\). Cette erreur varie en fonction des données d’entraînement.\n\nCette inégalité montre que l’erreur quadratique moyenne du prédicteur agrégé est toujours inférieure ou égale à la moyenne des erreurs des prédicteurs individuels. Puisque le biais du prédicteur agrégé est identique au biais du prédicteur individuel, alors l’inégalité précédente implique que la variance du modèle agrégé \\(\\phi_A(x)\\) est toujours inférieure ou égale à la variance moyenne d’un modèle individuel :\n$ (_A(x)) = (E_L[(x, L)]) E_L[((x, L))] $\nAutrement dit, le processus d’agrégation réduit l’erreur de prédiction globale en réduisant la variance des prédictions, tout en conservant un biais constant.\nCe résultat ouvre la voie à des considérations pratiques immédiates. Lorsque le modèle individuel est instable et présente une variance élevée, l’inégalité \\(Var(\\phi_A(x)) \\leq E_L[Var(\\phi(x,L))]\\) est forte, ce qui signifie que l’agrégation peut améliorer significativement la performance globale du modèle. En revanche, si \\(ϕ(x,L)\\) varie peu d’un ensemble d’entraînement à un autre (modèle stable avec variance faible), alors \\(Var(\\phi_A(x))\\) est proche de \\(E_L[Var(\\phi(x,L))]\\), et la réduction de variance apportée par l’agrégation est faible. Ainsi, le bagging est particulièrement efficace pour les modèles instables, tels que les arbres de décision, mais moins efficace pour les modèles stables tels que les méthodes des k plus proches voisins.\n\n\n\n\nDans le cas de la classification, le mécanisme de réduction de la variance par le bagging permet, sous une certaine condition, d’atteindre un classificateur presque optimal (nearly optimal classifier). Ce concept a été introduit par Breiman (1996) pour décrire un modèle qui tend à classer une observation dans la classe la plus probable, avec une performance approchant celle du classificateur Bayésien optimal (la meilleure performance théorique qu’un modèle de classification puisse atteindre).\nPour comprendre ce résutlat, introduisons \\(Q(j|x) = E_L(1_{φ(x, L) = j}) = P(φ(x, L) = j)\\), la probabilité qu’un modèle \\(φ(x, L)\\) prédise la classe \\(j\\) pour l’observation \\(x\\), et \\(P(j|x)\\), la probabilité réelle (conditionnelle) que \\(x\\) appartienne à la classe \\(j\\).\n\n\nUn classificateur \\(φ(x, L)\\) est dit order-correct pour une observation \\(x\\) si, en espérance, il identifie correctement la classe la plus probable, même s’il ne prédit pas toujours avec exactitude les probabilités associées à chaque classe \\(Q(j∣x)\\).\nCela signifie que si l’on considérait tous les ensemble de données possibles, et que l’on évaluait les prédictions du modèle en \\(x\\), la majorité des prédictions correspondraient à la classe à laquelle il a la plus grande probabilité vraie d’appartenir \\(P(j∣x)\\).\nFormellement, un prédicteur est dit “order-correct” pour une entrée \\(x\\) si :\n$ argmax_j Q(j|x) = argmax_j P(j|x) $\noù \\(P(j|x)\\) est la vraie probabilité que l’observation \\(x\\) appartienne à la classe \\(j\\), et \\(Q(j|x)\\) est la probabilité que \\(x\\) appartienne à la classe \\(j\\) prédite par le modèle \\(φ(x, L)\\).\nUn classificateur est order-correct si, pour chaque observation \\(x\\), la classe qu’il prédit correspond à celle qui a la probabilité maximale \\(P(j|x)\\) dans la distribution vraie.\n\n\n\nDans le cas de la classification, le prédicteur agrégé est défini par le vote majoritaire. Cela signifie que si \\(K\\) classificateurs sont entraînés sur \\(K\\) échantillons distincts, la classe prédite pour \\(x\\) est celle qui reçoit le plus de votes de la part des modèles individuels.\nFormellement, le classificateur agrégé \\(φA(x)\\) est défini par :\n\\(φA(x) =  \\text{argmax}_j \\sum_{L} I(\\phi(x, L) = j) = argmax_j Q(j|x)\\)\n\n\n\nBreiman (1996) montre que si chaque prédicteur individuel \\(φ(x, L)\\) est order-correct pour une observation \\(x\\), alors le prédicteur agrégé \\(φA(x)\\), obtenu par vote majoritaire, atteint la performance optimale pour cette observation, c’est-à-dire qu’il converge vers la classe ayant la probabilité maximale \\(P(j∣x)\\) pour l’observation \\(x\\) lorsque le nombre de prédicteurs individuels augmente. Le vote majoritaire permet ainsi de réduire les erreurs aléatoires des classificateurs individuels.\nLe classificateur agrégé \\(ϕA\\) est optimal s’il prédit systématiquement la classe la plus probable pour l’observation \\(x\\) dans toutes les régions de l’espace.\nCependant, dans les régions de l’espace où les classificateurs individuels ne sont pas order-corrects (c’est-à-dire qu’ils se trompent majoritairement sur la classe d’appartenance), l’agrégation par vote majoritaire n’améliore pas les performances. Elles peuvent même se détériorer par rapport aux modèles individuels si l’agrégation conduit à amplifier des erreurs systématiques (biais).\n\n\n\n\n\nEn pratique, au lieu d’utiliser tous les ensembles d’entraînement possibles \\(L\\), le bagging repose sur un nombre limité d’échantillons bootstrap tirés avec remise à partir d’un même jeu de données initial, ce qui peut introduire des biais par rapport au prédicteur agrégé théorique.\nLes échantillons bootstrap présentent les limites suivantes :\n\nUne taille effective réduite par rapport au jeu de données initial: Bien que chaque échantillon bootstrap présente le même nombre d’observations que le jeu de données initial, environ 1/3 des observations (uniques) du jeu initial sont absentes de chaque échantillon bootstrap (du fait du tirage avec remise). Cela peut limiter la capacité des modèles à capturer des relations complexes au sein des données (et aboutir à des modèles individuels sous-ajustés par rapport à ce qui serait attendu théoriquement), en particulier lorsque l’échantillon initial est de taille modeste.\nUne dépendance entre échantillons : Les échantillons bootstrap sont tirés dans le même jeu de données, ce qui génère une dépendance entre eux, qui réduit la diversité des modèles. Cela peut limiter l’efficacité de la réduction de variance dans le cas de la régression, voire acroître le biais dans le cas de la classification.\nUne couverture incomplète de l’ensemble des échantillons possibles: Les échantillons bootstrap ne couvrent pas l’ensemble des échantillons d’entraînement possibles, ce qui peut introduire un biais supplémentaire par rapport au prédicteur agrégé théorique.\n\n\n\n\n\n\nLe bagging est particulièrement utile lorsque les modèles individuels présentent une variance élevée et sont instables. Dans de tels cas, l’agrégation des prédictions peut réduire significativement la variance globale, améliorant ainsi la performance du modèle agrégé. Les situations où le bagging est recommandé incluent typiquement:\n\nLes modèles instables : Les modèles tels que les arbres de décision non élagués, qui sont sensibles aux variations des données d’entraînement, bénéficient grandement du bagging. L’agrégation atténue les fluctuations des prédictions dues aux différents échantillons.\nLes modèles avec biais faibles: En classification, si les modèles individuels sont order-corrects pour la majorité des observations, le bagging peut améliorer la précision en renforçant les prédictions correctes et en réduisant les erreurs aléatoires.\n\nInversement, le bagging peut être moins efficace ou même néfaste dans certaines situations :\n\nLes modèles stables avec variance faible : Si les modèles individuels sont déjà stables et présentent une faible variance (par exemple, la régression linéaire), le bagging n’apporte que peu d’amélioration, car la réduction de variance supplémentaire est minimale.\nLa présence de biais élevée : Si les modèles individuels sont biaisés, entraînant des erreurs systématiques, le bagging peut amplifier ces erreurs plutôt que de les corriger. Dans de tels cas, il est préférable de s’attaquer d’abord au biais des modèles avant de considérer l’agrégation.\nLes échantillons de petite taille : Avec des ensembles de données limités, les échantillons bootstrap peuvent ne pas être suffisamment diversifiés ou représentatifs, ce qui réduit l’efficacité du bagging et peut augmenter le biais des modèles.\n\nCe qui qu’il faut retenir: le bagging peut améliorer substantiellement la performance des modèles d’apprentissage automatique lorsqu’il est appliqué dans des conditions appropriées. Il est essentiel d’évaluer la variance et le biais des modèles individuels, ainsi que la taille et la représentativité du jeu de données, pour déterminer si le bagging est une stratégie adaptée. Lorsqu’il est utilisé judicieusement, le bagging peut conduire à des modèles plus robustes et précis, exploitant efficacement la puissance de l’agrégation pour améliorer la performance des modèles individuels.\n\n\n\n\n\n“Optimal performance is often found by bagging 50–500 trees. Data sets that have a few strong predictors typically require less trees; whereas data sets with lots of noise or multiple strong predictors may need more. Using too many trees will not lead to overfitting. However, it’s important to realize that since multiple models are being run, the more iterations you perform the more computational and time requirements you will have. As these demands increase, performing k-fold CV can become computationally burdensome.”\n\n\n\n“A benefit to creating ensembles via bagging, which is based on resampling with replacement, is that it can provide its own internal estimate of predictive performance with the out-of-bag (OOB) sample (see Section 2.4.2). The OOB sample can be used to test predictive performance and the results usually compare well compared to k-fold CV assuming your data set is sufficiently large (say n≥1,000). Consequently, as your data sets become larger and your bagging iterations increase, it is common to use the OOB error estimate as a proxy for predictive performance.”\n\n\n\n\n\nOu bien ne commencer les mises en pratique qu’avec les random forest ?",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "Le _bagging_"
    ]
  },
  {
    "objectID": "chapters/chapter2/2-bagging.html#principe-du-bagging",
    "href": "chapters/chapter2/2-bagging.html#principe-du-bagging",
    "title": "1 Le bagging",
    "section": "",
    "text": "Le bagging comporte trois étapes principales:\n\nL’échantillonnage bootstrap : L’échantillonnage bootstrap consiste à créer des échantillons distincts en tirant aléatoirement avec remise des observations du jeu de données initial. Chaque échantillon bootstrap contient le même nombre d’observations que le jeu de données initial, mais certaines observations sont répétées (car sélectionnées plusieurs fois), tandis que d’autres sont omises.\nL’entraînement de plusieurs modèles : Un modèle (aussi appelé apprenant de base ou weak learner) est entraîné sur chaque échantillon bootstrap. Les modèles peuvent être des arbres de décision, des régressions ou tout autre algorithme d’apprentissage. Le bagging est particulièrement efficace avec des modèles instables, tels que les arbres de décision non élagués.\nL’agrégation des prédictions : Les prédictions de tous les modèles sont ensuite agrégées, en procédant généralement à la moyenne (ou à la médiane) des prédictions dans le cas de la régression, et au vote majoritaire (ou à la moyenne des probabilités prédites pour chaque classe) dans le cas de la classification, afin d’obtenir des prédictions plus précises et généralisables.",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "Le _bagging_"
    ]
  },
  {
    "objectID": "chapters/chapter2/2-bagging.html#pourquoi-et-dans-quelles-situations-le-bagging-fonctionne",
    "href": "chapters/chapter2/2-bagging.html#pourquoi-et-dans-quelles-situations-le-bagging-fonctionne",
    "title": "1 Le bagging",
    "section": "",
    "text": "Certains modèles sont très sensibles aux données d’entraînement, et leurs prédictions sont très instables d’un échantillon à l’autre. L’objectif du bagging est de construire un prédicteur plus précis en agrégeant les prédictions de plusieurs modèles entraînés sur des échantillons (légèrement) différents les uns des autres.\nBreiman (1996) montre que cette méthode est particulièrement efficace lorsqu’elle est appliquée à des modèles très instables, dont les performances sont particulièrement sensibles aux variations du jeu de données d’entraînement, et peu biaisés.\nCette section vise à mieux comprendre comment (et sous quelles conditions) l’agrégation par bagging permet de construire un prédicteur plus performant.\nDans la suite, nous notons \\(φ(x, L)\\) un prédicteur (d’une valeur numérique dans le cas de la régression ou d’un label dans le cas de la classification), entraîné sur un ensemble d’apprentissage \\(L\\), et prenant en entrée un vecteur de caractéristiques \\(x\\).\n\n\nDans le contexte de la régression, l’objectif est de prédire une valeur numérique \\(Y\\) à partir d’un vecteur de caractéristiques \\(x\\). Un modèle de régression \\(\\phi(x, L)\\) est construit à partir d’un ensemble d’apprentissage \\(L\\), et produit une estimation de \\(Y\\) pour chaque observation \\(x\\).\n\n\nDans le cas de la régression, le prédicteur agrégé est défini comme suit :\n$ _A(x) = E_L[(x, L)] $\noù \\(\\phi_A(x)\\) représente la prédiction agrégée, \\(E_L[.]\\) correspond à l’espérance prise sur tous les échantillons d’apprentissage possibles \\(L\\), chacun étant tiré selon la même distribution que le jeu de données initial, et \\(\\phi(x, L)\\) correspond à la prédiction du modèle construit sur l’échantillon d’apprentissage \\(L\\).\n\n\n\nPour mieux comprendre comment l’agrégation améliore la performance globale d’un modèle individuel \\(\\phi(x, L)\\), revenons à la décomposition biais-variance de l’erreur quadratique moyenne (il s’agit de la mesure de performance classiquement considérée dans un problème de régression):\n\\[E_L[\\left(Y - \\phi(x, L)\\right)^2] = \\underbrace{\\left(E_L\\left[\\phi(x, L) - Y\\right]\\right)^2}_{\\text{Biais}^2} + \\underbrace{E_L[\\left(\\phi(x, L) - E_L[\\phi(x, L)]\\right)^2]}_{\\text{Variance}} \\tag{1}\\]\n\nLe biais est la différence entre la valeur observée \\(Y\\) que l’on souhaite prédire et la prédiction moyenne \\(E_L[\\phi(x, L)]\\). Si le modèle est sous-ajusté, le biais sera élevé.\nLa variance est la variabilité des prédictions (\\(\\phi(x, L)\\)) autour de leur moyenne (\\(E_L[\\phi(x, L)]\\)). Un modèle avec une variance élevée est très sensible aux fluctuations au sein des données d’entraînement: ses prédictions varient beaucoup lorsque les données d’entraînement se modifient.\n\nL’équation 1 illustre l’arbitrage biais-variance qui est omniprésent en machine learning: plus la complexité d’un modèle s’accroît (exemple: la profondeur d’un arbre), plus son biais sera plus faible (car ses prédictions seront de plus en plus proches des données d’entraînement), et plus sa variance sera élevée (car ses prédictions, étant très proches des données d’entraînement, auront tendance à varier fortement d’un jeu d’entraînement à l’autre).\n\n\n\nBreiman (1996) compare l’erreur quadratique moyenne d’un modèle individuel avec celle du modèle agrégé et démontre l’inégalité suivante :\n\n$ (Y - _A(x))^2 E_L[(Y - (x, L))^2] $ {#eq-inegalite-breiman1996}\n\nLe terme \\((Y - \\phi_A(x))^2\\) représente l’erreur quadratique du prédicteur agrégé \\(\\phi_A(x)\\);\nLe terme \\(E_L[(Y - \\phi(x, L))^2]\\) est l’erreur quadratique moyenne d’un prédicteur individuel \\(\\phi(x, L)\\) entraîné sur un échantillon aléatoire \\(L\\). Cette erreur varie en fonction des données d’entraînement.\n\nCette inégalité montre que l’erreur quadratique moyenne du prédicteur agrégé est toujours inférieure ou égale à la moyenne des erreurs des prédicteurs individuels. Puisque le biais du prédicteur agrégé est identique au biais du prédicteur individuel, alors l’inégalité précédente implique que la variance du modèle agrégé \\(\\phi_A(x)\\) est toujours inférieure ou égale à la variance moyenne d’un modèle individuel :\n$ (_A(x)) = (E_L[(x, L)]) E_L[((x, L))] $\nAutrement dit, le processus d’agrégation réduit l’erreur de prédiction globale en réduisant la variance des prédictions, tout en conservant un biais constant.\nCe résultat ouvre la voie à des considérations pratiques immédiates. Lorsque le modèle individuel est instable et présente une variance élevée, l’inégalité \\(Var(\\phi_A(x)) \\leq E_L[Var(\\phi(x,L))]\\) est forte, ce qui signifie que l’agrégation peut améliorer significativement la performance globale du modèle. En revanche, si \\(ϕ(x,L)\\) varie peu d’un ensemble d’entraînement à un autre (modèle stable avec variance faible), alors \\(Var(\\phi_A(x))\\) est proche de \\(E_L[Var(\\phi(x,L))]\\), et la réduction de variance apportée par l’agrégation est faible. Ainsi, le bagging est particulièrement efficace pour les modèles instables, tels que les arbres de décision, mais moins efficace pour les modèles stables tels que les méthodes des k plus proches voisins.\n\n\n\n\nDans le cas de la classification, le mécanisme de réduction de la variance par le bagging permet, sous une certaine condition, d’atteindre un classificateur presque optimal (nearly optimal classifier). Ce concept a été introduit par Breiman (1996) pour décrire un modèle qui tend à classer une observation dans la classe la plus probable, avec une performance approchant celle du classificateur Bayésien optimal (la meilleure performance théorique qu’un modèle de classification puisse atteindre).\nPour comprendre ce résutlat, introduisons \\(Q(j|x) = E_L(1_{φ(x, L) = j}) = P(φ(x, L) = j)\\), la probabilité qu’un modèle \\(φ(x, L)\\) prédise la classe \\(j\\) pour l’observation \\(x\\), et \\(P(j|x)\\), la probabilité réelle (conditionnelle) que \\(x\\) appartienne à la classe \\(j\\).\n\n\nUn classificateur \\(φ(x, L)\\) est dit order-correct pour une observation \\(x\\) si, en espérance, il identifie correctement la classe la plus probable, même s’il ne prédit pas toujours avec exactitude les probabilités associées à chaque classe \\(Q(j∣x)\\).\nCela signifie que si l’on considérait tous les ensemble de données possibles, et que l’on évaluait les prédictions du modèle en \\(x\\), la majorité des prédictions correspondraient à la classe à laquelle il a la plus grande probabilité vraie d’appartenir \\(P(j∣x)\\).\nFormellement, un prédicteur est dit “order-correct” pour une entrée \\(x\\) si :\n$ argmax_j Q(j|x) = argmax_j P(j|x) $\noù \\(P(j|x)\\) est la vraie probabilité que l’observation \\(x\\) appartienne à la classe \\(j\\), et \\(Q(j|x)\\) est la probabilité que \\(x\\) appartienne à la classe \\(j\\) prédite par le modèle \\(φ(x, L)\\).\nUn classificateur est order-correct si, pour chaque observation \\(x\\), la classe qu’il prédit correspond à celle qui a la probabilité maximale \\(P(j|x)\\) dans la distribution vraie.\n\n\n\nDans le cas de la classification, le prédicteur agrégé est défini par le vote majoritaire. Cela signifie que si \\(K\\) classificateurs sont entraînés sur \\(K\\) échantillons distincts, la classe prédite pour \\(x\\) est celle qui reçoit le plus de votes de la part des modèles individuels.\nFormellement, le classificateur agrégé \\(φA(x)\\) est défini par :\n\\(φA(x) =  \\text{argmax}_j \\sum_{L} I(\\phi(x, L) = j) = argmax_j Q(j|x)\\)\n\n\n\nBreiman (1996) montre que si chaque prédicteur individuel \\(φ(x, L)\\) est order-correct pour une observation \\(x\\), alors le prédicteur agrégé \\(φA(x)\\), obtenu par vote majoritaire, atteint la performance optimale pour cette observation, c’est-à-dire qu’il converge vers la classe ayant la probabilité maximale \\(P(j∣x)\\) pour l’observation \\(x\\) lorsque le nombre de prédicteurs individuels augmente. Le vote majoritaire permet ainsi de réduire les erreurs aléatoires des classificateurs individuels.\nLe classificateur agrégé \\(ϕA\\) est optimal s’il prédit systématiquement la classe la plus probable pour l’observation \\(x\\) dans toutes les régions de l’espace.\nCependant, dans les régions de l’espace où les classificateurs individuels ne sont pas order-corrects (c’est-à-dire qu’ils se trompent majoritairement sur la classe d’appartenance), l’agrégation par vote majoritaire n’améliore pas les performances. Elles peuvent même se détériorer par rapport aux modèles individuels si l’agrégation conduit à amplifier des erreurs systématiques (biais).",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "Le _bagging_"
    ]
  },
  {
    "objectID": "chapters/chapter2/2-bagging.html#léchantillage-par-bootstrap-peut-détériorer-les-performances-théoriques-du-modèle-agrégé",
    "href": "chapters/chapter2/2-bagging.html#léchantillage-par-bootstrap-peut-détériorer-les-performances-théoriques-du-modèle-agrégé",
    "title": "1 Le bagging",
    "section": "",
    "text": "En pratique, au lieu d’utiliser tous les ensembles d’entraînement possibles \\(L\\), le bagging repose sur un nombre limité d’échantillons bootstrap tirés avec remise à partir d’un même jeu de données initial, ce qui peut introduire des biais par rapport au prédicteur agrégé théorique.\nLes échantillons bootstrap présentent les limites suivantes :\n\nUne taille effective réduite par rapport au jeu de données initial: Bien que chaque échantillon bootstrap présente le même nombre d’observations que le jeu de données initial, environ 1/3 des observations (uniques) du jeu initial sont absentes de chaque échantillon bootstrap (du fait du tirage avec remise). Cela peut limiter la capacité des modèles à capturer des relations complexes au sein des données (et aboutir à des modèles individuels sous-ajustés par rapport à ce qui serait attendu théoriquement), en particulier lorsque l’échantillon initial est de taille modeste.\nUne dépendance entre échantillons : Les échantillons bootstrap sont tirés dans le même jeu de données, ce qui génère une dépendance entre eux, qui réduit la diversité des modèles. Cela peut limiter l’efficacité de la réduction de variance dans le cas de la régression, voire acroître le biais dans le cas de la classification.\nUne couverture incomplète de l’ensemble des échantillons possibles: Les échantillons bootstrap ne couvrent pas l’ensemble des échantillons d’entraînement possibles, ce qui peut introduire un biais supplémentaire par rapport au prédicteur agrégé théorique.",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "Le _bagging_"
    ]
  },
  {
    "objectID": "chapters/chapter2/2-bagging.html#le-bagging-en-pratique",
    "href": "chapters/chapter2/2-bagging.html#le-bagging-en-pratique",
    "title": "1 Le bagging",
    "section": "",
    "text": "Le bagging est particulièrement utile lorsque les modèles individuels présentent une variance élevée et sont instables. Dans de tels cas, l’agrégation des prédictions peut réduire significativement la variance globale, améliorant ainsi la performance du modèle agrégé. Les situations où le bagging est recommandé incluent typiquement:\n\nLes modèles instables : Les modèles tels que les arbres de décision non élagués, qui sont sensibles aux variations des données d’entraînement, bénéficient grandement du bagging. L’agrégation atténue les fluctuations des prédictions dues aux différents échantillons.\nLes modèles avec biais faibles: En classification, si les modèles individuels sont order-corrects pour la majorité des observations, le bagging peut améliorer la précision en renforçant les prédictions correctes et en réduisant les erreurs aléatoires.\n\nInversement, le bagging peut être moins efficace ou même néfaste dans certaines situations :\n\nLes modèles stables avec variance faible : Si les modèles individuels sont déjà stables et présentent une faible variance (par exemple, la régression linéaire), le bagging n’apporte que peu d’amélioration, car la réduction de variance supplémentaire est minimale.\nLa présence de biais élevée : Si les modèles individuels sont biaisés, entraînant des erreurs systématiques, le bagging peut amplifier ces erreurs plutôt que de les corriger. Dans de tels cas, il est préférable de s’attaquer d’abord au biais des modèles avant de considérer l’agrégation.\nLes échantillons de petite taille : Avec des ensembles de données limités, les échantillons bootstrap peuvent ne pas être suffisamment diversifiés ou représentatifs, ce qui réduit l’efficacité du bagging et peut augmenter le biais des modèles.\n\nCe qui qu’il faut retenir: le bagging peut améliorer substantiellement la performance des modèles d’apprentissage automatique lorsqu’il est appliqué dans des conditions appropriées. Il est essentiel d’évaluer la variance et le biais des modèles individuels, ainsi que la taille et la représentativité du jeu de données, pour déterminer si le bagging est une stratégie adaptée. Lorsqu’il est utilisé judicieusement, le bagging peut conduire à des modèles plus robustes et précis, exploitant efficacement la puissance de l’agrégation pour améliorer la performance des modèles individuels.\n\n\n\n\n\n“Optimal performance is often found by bagging 50–500 trees. Data sets that have a few strong predictors typically require less trees; whereas data sets with lots of noise or multiple strong predictors may need more. Using too many trees will not lead to overfitting. However, it’s important to realize that since multiple models are being run, the more iterations you perform the more computational and time requirements you will have. As these demands increase, performing k-fold CV can become computationally burdensome.”\n\n\n\n“A benefit to creating ensembles via bagging, which is based on resampling with replacement, is that it can provide its own internal estimate of predictive performance with the out-of-bag (OOB) sample (see Section 2.4.2). The OOB sample can be used to test predictive performance and the results usually compare well compared to k-fold CV assuming your data set is sufficiently large (say n≥1,000). Consequently, as your data sets become larger and your bagging iterations increase, it is common to use the OOB error estimate as a proxy for predictive performance.”",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "Le _bagging_"
    ]
  },
  {
    "objectID": "chapters/chapter2/2-bagging.html#mise-en-pratique-exemple-avec-code",
    "href": "chapters/chapter2/2-bagging.html#mise-en-pratique-exemple-avec-code",
    "title": "1 Le bagging",
    "section": "",
    "text": "Ou bien ne commencer les mises en pratique qu’avec les random forest ?",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "Le _bagging_"
    ]
  },
  {
    "objectID": "chapters/chapter2/0-intro.html",
    "href": "chapters/chapter2/0-intro.html",
    "title": "1 Présentation formelle des méthodes ensemblistes",
    "section": "",
    "text": "1 Présentation formelle des méthodes ensemblistes\nPrincipe: cette partie propose une présentation formalisée des méthodes ensemblistes, à destination des personnes souhaitant comprendre en détail le fonctionnement des algorithmes.",
    "crumbs": [
      "Présentation formelle des algorithmes"
    ]
  },
  {
    "objectID": "chapters/chapter1/1-survol.html",
    "href": "chapters/chapter1/1-survol.html",
    "title": "1 Aperçu des méthodes ensemblistes",
    "section": "",
    "text": "Principe: Cette section propose une introduction intuitive aux méthodes ensemblistes. Elle s’adresse aux lecteurs qui souhaitent acquérir une compréhension générale du fonctionnement de ces techniques et identifier rapidement les situations concrètes dans lesquelles elles peuvent être utiles. L’objectif est d’en expliciter les principes-clés sans recourir au formalisme mathématique, afin de rendre le contenu accessible sans prérequis.\n\n\nLes méthodes ensemblistes sont des techniques d’apprentissage supervisé en machine learning développées depuis le début des années 1990. Leur objectif est de prédire une variable-cible \\(y\\) (appelée target) à partir d’un ensemble de variables prédictives \\(\\mathbf{X}\\) (appelées features), que ce soit pour des tâches de classification (prédire une catégorie) ou de régression (prédire une valeur numérique). Elles peuvent par exemple être utilisées pour prédire le salaire d’un salarié, la probabilité de réponse dans une enquête, le niveau de diplôme…\nPlutôt que de s’appuyer sur un seul modèle complexe, les méthodes ensemblistes se caractérisent par la combinaison des prédictions de plusieurs modèles plus simples, appelés “apprenants faibles” (weak learner ou base learner), pour créer un modèle performant, dit “apprenant fort” (strong learner).\nLe choix de ces modèles de base, ainsi que la manière dont leurs prédictions sont combinées, sont des facteurs déterminants de la performance finale. Le présent document se concentre sur les méthodes à base d’arbres de décisions, qui sont parmi les plus utilisées en pratique. Nous allons examiner les fondements de ces méthodes, leurs avantages et inconvénients, ainsi que les algorithmes les plus populaires.\n\n\n\nLes méthodes ensemblistes sont particulièrement bien adaptées à de nombreux cas d’usage de la statistique publique, pour deux raisons. D’une part, elles sont conçues pour s’appliquer à des données tabulaires (enregistrements en lignes, variables en colonnes), structure de données omniprésente dans la statistique publique. D’autre part, elles peuvent être mobilisées dans toutes les situations où le statisticien mobilise une régression linéaire ou une régression logistisque (imputation, repondération…).\nLes méthodes ensemblistes présentent trois avantages par rapport aux méthodes économétriques traditionnelles (régression linéaire et régression logistique):\n\nElles ont une puissance prédictive supérieure: alors que les méthodes traditionnelles supposent fréquemment l’existence d’une relation linéaire ou log-linéaire entre \\(y\\) et \\(\\mathbf{X}\\), les méthodes ensemblistes ne font quasiment aucune hypothèse sur la relation entre \\(y\\) et \\(\\mathbf{X}\\), et se contentent d’approximer le mieux possible cette relation à partir des données disponibles. En particulier, les modèles ensemblistes peuvent facilement modéliser des non-linéarités de la relation entre \\(y\\) et \\(\\mathbf{X}\\) et des interactions entre variables explicatives sans avoir à les spécifier explicitement au préalable, alors que les méthodes traditionnelles supposent fréquemment l’existence d’une relation linéaire ou log-linéaire entre \\(y\\) et \\(\\mathbf{X}\\).\nElles nécessitent moins de préparation des données: elles ne requièrent pas de normalisation des variables explicatives et peuvent s’accommoder des valeurs manquantes (selon des techniques variables selon les algorithmes).\nElles sont généralement moins sensibles aux valeurs extrêmes et à l’hétéroscédasticité des variables explicatives que les approches traditionnelles.\n\nElles présentent par ailleurs deux inconvénients rapport aux méthodes économétriques traditionnelles. Premièrement, bien qu’il existe désormais de multiples approches permettent d’interpétrer partiellement les modèles ensemblistes, leur interprétabilité reste moindre que celle d’une régression linéaire ou logistique. Deuxièmement, les modèles ensemblistes sont plus complexes que les approches traditionnelles, et leurs hyperparamètres doivent faire l’objet d’une optimisation, par exemple au travers d’une validation croisée. Ce processus d’optimisation est généralement plus complexe et plus long que l’estimation d’une régression linéaire ou logistique. En revanche, les méthodes ensemblistes sont relativement simples à prendre en main, et ne requièrent pas nécessairement une puissance de calcul importante.\n\n\n\n\n\n\nEt par rapport au deep learning?\n\n\n\nSi les approches de deep learning sont sans conteste très performantes pour le traitement du langage naturel, des images et du son, leur supériorité n’est pas établie pour les applications reposant sur des données tabulaires. Les comparaisons disponibles dans la littérature concluent en effet que les méthodes ensemblistes à base d’arbres sont soit plus performantes que les approches de deep learning (Grinsztajn, Oyallon, and Varoquaux (2022), Shwartz-Ziv and Armon (2022)), soit font jeu égal avec elles (McElfresh et al. (2024)). Ces études ont identifié trois avantages des méthodes ensemblistes: elles sont peu sensibles aux variables explicatives non pertinentes, robustes aux valeurs extrêmes des variables explicatives, et capables d’approximer des fonctions très irrégulières. De plus, dans la pratique les méthodes ensemblistes sont souvent plus rapides à entraîner et moins gourmandes en ressources informatiques, et l’optimisation des hyperparamètres s’avère souvent moins complexe (Shwartz-Ziv and Armon (2022)).\n\n\n\n\n\nCe paragraphe présente d’abord le modèle de base sur lesquelles sont construites les méthodes ensemblistes à base d’arbres: l’arbre de classification et de régression (CART) (Section 1.3.1). Bien que simples et intuitifs, les arbres CART sont souvent insuffisants en termes de performance lorsqu’ils sont utilisés isolément.\nElle introduit ensuite les deux grandes familles de méthodes ensemblistes décrites dans ce document: le bagging et les forêts aléatoires (Section 1.3.2), et le gradient boosting (Section 1.3.3).\n\n\n\n\nLe modèle de base des méthodes ensemblistes est souvent un arbre de classification et de régression (CART, Breiman et al. (1984)). Un arbre CART est un algorithme prédictif qui traite un problème de prédiction complexe en le décomposant en une série de décisions simples, organisées de manière hiérarchique. Ces décisions permettent de segmenter progressivement les données en régions homogènes au sein desquelles il est plus simple de faire des prédictions. Il s’agit d’un outil puissant pour explorer les relations entre les variables explicatives et la variable cible, sans recourir à des hypothèses a priori sur la forme de cette relation.\nTrois caractéristiques essentielles définissent un arbre CART :\n\nL’arbre partitionne l’espace des variables explicatives \\(X\\) en régions (appelées feuilles ou leaves) les plus homogènes possible, au sens d’une mesure de l’hétérogénéité (par exemple, l’entropie ou l’erreur quadratique moyenne). Ces divisions vont permettre de regrouper des observations similaires pour faciliter la prédiction;\nChaque région est définie par un ensemble de conditions, appelées régles de décision (splitting rules), appliquées successivement sur les variables explicatives. Par exemple, une première règle pourrait poser la question : “L’individu est-il en emploi ?”, et subdiviser les données en deux groupes (oui/non). Une deuxième règle pourrait alors affiner la segmentation en posant la question : “L’individu est-il diplômé du supérieur ?”. Une région spécifique serait ainsi définie par la condition combinée : “l’individu est en emploi et est diplômé du supérieur”.\nUne fois l’arbre construit, chaque feuille produit une prédiction en se basant sur les données de la région correspondante. En classification, la prédiction est généralement la classe la plus fréquente parmi les observations de la région. En régression, la prédiction est souvent la moyenne des valeurs observées dans la région.\n\nDeux conséquences importantes découlent de cette construction : - L’algorithme CART ne fait aucune hypothèse a priori sur la relation entre les variables explicatives \\(\\mathbf{X}\\) et la variable cible \\(y\\). C’est une différence majeure avec les modèles économétriques standard, tels que la régression linéaire qui suppose une relation linéaire de la forme \\(E(y) = \\mathbf{X \\beta}\\). - L’arbre final est une fonction constante par morceaux: la prédiction est la même pour toutes les observations situées dans la même région ; elle ne peut varier qu’entre régions.\nIllustration, et représentation graphique (sous forme d’arbre et de graphique).\n\n\n\n\nLes arbres CART présentent plusieurs avantages: leur principe est simple, ils sont aisément interprétables et peuvent faire l’objet de représentations graphiques intuitives. Par ailleurs, la flexibilité offerte par le partitionnement récursif assure que les arbres obtenus reflètent les corrélations observées dans les données d’entraînement.\nIls souffrent néanmoins de deux limites. D’une part, les arbres CART ont souvent un pouvoir prédictif faible qui en limite l’usage. D’autre part, ils sont peu robustes et instables: on dit qu’ils présentent une variance élevée. Ainsi, un léger changement dans les données (par exemple l’ajout ou la suppression de quelques observations) peut entraîner des modifications significatives dans la structure de l’arbre et dans la définition des régions utilisées pour la prédiction (feuilles). Les arbres CART sont notamment sensibles aux valeurs extrêmes, aux points aberrants et au bruit statistique. De plus, les prédictions des arbres CART sont sensibles à de petites fluctuations des données d’échantillonnage: celles-ci peuvent aboutir à ce qu’une partie des observations change brutalement de feuille et donc de valeur prédite.\nCes limites motivent l’utilisation des deux familles de méthodes ensemblistes présentées dans la suite (le bagging, dont la random forests, et le gradient boosting), qui s’appuient sur un grand nombre d’arbres pour accroître à la fois la précision et la stabilité des prédictions. La différence essentielle entre ces deux familles portent sur la façon dont les arbres sont entraînés.\n\n\n\n\n\n\nLes familles de méthodes ensemblistes\n\n\n\nLes méthodes ensemblistes basées sur des arbres de décision se répartissent en deux grandes familles, qui se distinguent selon la manière dont les modèles de base sont construits. Lorsque les modèles de base sont entraînés en parallèle et indépendamment les uns des autres, on parle de bagging (Bootstrap Aggregating). La forêt aléatoire (random forest) est une variante particulièrement performante du bagging. Lorsque les modèles de base sont entraînés de manière séquentielle, chaque modèle visant à corriger les erreurs des modèles précédents, on parle de boosting. Ce document aborde essentiellement le gradient boosting, qui est l’approche de boosting la plus utilisée actuellement.\n\n\n\n\n\n\n\n\nLe bagging (Bootstrap Aggregating) est une méthode ensembliste qui repose sur l’agrégation des prédictions de plusieurs modèles individuels, entraînés indépendamment les uns des autres, pour construire un modèle global plus performant (Breiman (1996)). Cette approche constitue également le socle des forêts aléatoires, qui en sont une version améliorée.\nLe bagging offre deux avantages majeurs par rapport aux arbres de décision CART : une meilleure capacité prédictive et une plus grande stabilité des prédictions. Cette amélioration découle de la stratégie d’entraînement. Au lieu d’entraîner un seul modèle sur l’ensemble des données, le bagging procède en trois étapes principales:\n\nTirage de sous-échantillons aléatoires: À partir du jeu de données initial, plusieurs sous-échantillons sont générés par échantillonnage aléatoire avec remise (bootstrapping). Chaque sous-échantillon a la même taille que le jeu de données original, mais peut contenir des observations répétées, tandis que d’autres peuvent être omises.\nEntraînement parallèle: Un arbre est entraîné sur chaque sous-échantillon de manière indépendante. Ces arbres sont habituellement assez complexes et profonds.\nAgrégation des prédictions: Les prédictions des modèles sont combinées pour produire le résultat final. En classification, la prédiction finale est souvent déterminée par un vote majoritaire, tandis qu’en régression, elle correspond généralement à la moyenne des prédictions.\n\n\n\n\n\n\n\nFigure 1: Représentation schématique d’un algorithme de bagging\n\n\n\nLa 1 propose une représentation schématique du bagging: d’abord, des sous-échantillons sont générés aléatoires avec remise à partir du jeu de données d’entraînement. Ensuite, des arbres de décision sont entraînés indépendamment sur ces sous-échantillons. Enfin, leurs prédictions sont agrégées pour obtenir les prédictions finales. On procède généralement au vote majoritaire (la classe prédite majoritairement par les arbres) dans un problème de classification, et à la moyenne dans un problème de régression.\nL’efficacité du bagging provient de la réduction de la variance qui est permise par l’agrégation des prédictions. Chaque arbre est entraîné sur un sous-échantillon légèrement différent, sujet à des fluctuations aléatoires. L’agrégation des prédictions (par moyenne ou vote majoritaire) de tous les arbres réduit la sensibilité du modèle final aux fluctuations des données d’entraînement. Le modèle final est ainsi plus robuste et plus précis que chacun des arbres pris individuellement.\n\nIllustration avec un cas d’usage de classification en deux dimensions.\n\nMalgré ses avantages, le bagging souffre d’une limite importante qui provient de la corrélation entre les arbres. En effet, malgré le tirage aléatoire des sous-échantillons, les arbres présentent souvent des structures similaires, car les règles de décision sous-jacentes restent généralement assez proches. Cette corrélation réduit l’efficacité de l’agrégation et limite les gains en performance.\nPour réduire cette corrélation entre arbres, les forêts aléatoires introduisent une étape supplémentaire de randomisation. Leur supériorité prédictive explique pourquoi le bagging seul est rarement utilisé en pratique. Néanmoins, les forêts aléatoires tirent leur efficacité des principes fondamentaux du bagging.\n\n\n\nLes forêts aléatoires (random forests, Breiman (2001)) sont une variante du bagging qui vise à produire des modèles très performants en conciliant deux objectifs: maximiser le pouvoir prédictif des arbres pris isolément, et minimiser la corrélation entre ces arbres (le problème inhérent au bagging).\nPour atteindre ce second objectif, la forêt aléatoire introduit une nouvelle source de randomisation: la sélection aléatoire de variables. Lors de la construction de chaque arbre, au lieu d’utiliser toutes les variables disponibles pour déterminer la meilleure séparation à chaque nœud, un sous-ensemble aléatoire de variables est sélectionné. En limitant la quantité d’information à laquelle chaque arbre a accès au moment de chaque nouvelle division, cette étape supplémentaire contraint mécaniquement les arbres à être plus diversifiés (car deux arbres ne pourront plus nécessairement choisir les mêmes variables pour les mêmes séparations). Cela réduit significativement la corrélation entre les arbres, améliorant ainsi l’efficacité de l’agrégation. L’ensemble des prédictions devient ainsi plus précis et moins sujet aux fluctuations aléatoires.\n\n\n\n\n\n\nFigure 2: Représentation schématique d’un algorithme de forêt aléatoire\n\n\n\nLa figure 2 propose une représentation schématique d’une forêt aléatoire. La logique d’ensemble reste la même que celle du bagging. L’échantillonnage bootstrap est inchangé, mais l’étape de construction de chaque arbre est modifiée pour n’utiliser, à chaque nouvelle division, qu’un sous-ensemble aléatoire de variables. L’agrégation des prédictions se fait ensuite de la même manière que pour le bagging.\n\nLe principal enjeu de l’entraînement d’une forêt aléatoire est de trouver le bon arbitrage entre puissance prédictive des arbres individuels (que l’on souhaite maximiser) et corrélation entre les arbres (que l’on souhaite minimiser). L’optimisation des hyper-paramètres des forêts aléatoires (dont le plus important est le nombre de variables sélectionnées à chaque noeud) vise précisément à choisir le meilleur compromis possible entre pouvoir prédictif invividuel et diversité des arbres.\nLes forêts aléatoires sont très populaires car elles sont faciles à implémenter, peu sensibles aux hyperparamètres (elles fonctionnent bien avec les valeurs par défaut de la plupart des implémentations proposées en R ou en Python), et offrent de très bonnes performances dans de nombreux cas. Cependant, comme toute méthode d’apprentissage automatique, elles restent sujettes au surapprentissage (voir encadré), bien que dans une moindre mesure par rapport à d’autres techniques comme le gradient boosting.\n\n\n\n\n\n\n\nQu’est-ce que le surapprentissage?\n\n\n\nLe surapprentissage (overfitting) est un phénomène fréquent en machine learning où un modèle apprend non seulement les relations sous-jacentes entre la variable cible et les variables explicatives, mais également le bruit présent dans les données d’entraînement. En capturant ces fluctuations aléatoires plutôt que les tendances générales, le modèle affiche une performance excellente mais trompeuse sur les données d’entraînement, et s’avère médiocre sur des données nouvelles ou de test, car il ne parvient pas à généraliser efficacement.\n\n\n\n\n\n\n\n\nContrairement aux forêts aléatoires qui combinent des arbres de décision complexes et indépendants, le gradient boosting construit un ensemble d’arbres plus simples et entraînés de manière séquentielle. Chaque arbre vise à corriger les erreurs commises par les arbres précédents, améliorant progressivement la précision du modèle global. Cette approche repose sur des fondements théoriques très différents de ceux du bagging.\nLa logique du gradient boosting est illustrée par la figure 3:\n\n\n\n\n\n\nFigure 3: Représentation schématique d’un algorithme de gradient boosting\n\n\n\n\nUn premier modèle simple et peu performant est entraîné sur les données.\nUn deuxième modèle est entraîné de façon à corriger les erreurs du premier modèle (par exemple en pondérant davantage les observations mal prédites);\nCe processus est répété en ajoutant des modèles simples, chaque modèle corrigeant les erreurs commises par l’ensemble des modèles précédents;\nTous ces modèles sont finalement combinés (souvent par une somme pondérée) pour obtenir un modèle complexe et performant.\n\nLe gradient boosting offre des performances élevées mais exige une attention particulière portée sur la configuration des hyperparamètres et sur la prévention du surapprentissage. En particulier, les hyperparamètres sont nombreux et, contrairement aux forêts aléatoires, nécessitent un ajustement minutieux pour obtenir des résultats optimaux. Une mauvaise configuration peut conduire à des performances médiocres ou à un surapprentissage. L’utilisation du gradient boosting nécessite donc une bonne connaissance du fonctionnement des algorithmes. En outre, les algorithmes de gradient boosting peuvent être sensibles au bruit dans les données et aux erreurs dans la variable cible. Un prétraitement rigoureux des données est donc essentiel. Enfin, une validation rigoureuse sur un jeu de données de test indépendant (non utilisé pendant l’entraînement) est indispensable pour évaluer la qualité du modèle obtenu par gradient boosting.",
    "crumbs": [
      "Survol des méthodes ensemblistes",
      "Aperçu des méthodes ensemblistes"
    ]
  },
  {
    "objectID": "chapters/chapter1/1-survol.html#que-sont-les-méthodes-ensemblistes",
    "href": "chapters/chapter1/1-survol.html#que-sont-les-méthodes-ensemblistes",
    "title": "1 Aperçu des méthodes ensemblistes",
    "section": "",
    "text": "Les méthodes ensemblistes sont des techniques d’apprentissage supervisé en machine learning développées depuis le début des années 1990. Leur objectif est de prédire une variable-cible \\(y\\) (appelée target) à partir d’un ensemble de variables prédictives \\(\\mathbf{X}\\) (appelées features), que ce soit pour des tâches de classification (prédire une catégorie) ou de régression (prédire une valeur numérique). Elles peuvent par exemple être utilisées pour prédire le salaire d’un salarié, la probabilité de réponse dans une enquête, le niveau de diplôme…\nPlutôt que de s’appuyer sur un seul modèle complexe, les méthodes ensemblistes se caractérisent par la combinaison des prédictions de plusieurs modèles plus simples, appelés “apprenants faibles” (weak learner ou base learner), pour créer un modèle performant, dit “apprenant fort” (strong learner).\nLe choix de ces modèles de base, ainsi que la manière dont leurs prédictions sont combinées, sont des facteurs déterminants de la performance finale. Le présent document se concentre sur les méthodes à base d’arbres de décisions, qui sont parmi les plus utilisées en pratique. Nous allons examiner les fondements de ces méthodes, leurs avantages et inconvénients, ainsi que les algorithmes les plus populaires.",
    "crumbs": [
      "Survol des méthodes ensemblistes",
      "Aperçu des méthodes ensemblistes"
    ]
  },
  {
    "objectID": "chapters/chapter1/1-survol.html#pourquoi-utiliser-des-méthodes-ensemblistes",
    "href": "chapters/chapter1/1-survol.html#pourquoi-utiliser-des-méthodes-ensemblistes",
    "title": "1 Aperçu des méthodes ensemblistes",
    "section": "",
    "text": "Les méthodes ensemblistes sont particulièrement bien adaptées à de nombreux cas d’usage de la statistique publique, pour deux raisons. D’une part, elles sont conçues pour s’appliquer à des données tabulaires (enregistrements en lignes, variables en colonnes), structure de données omniprésente dans la statistique publique. D’autre part, elles peuvent être mobilisées dans toutes les situations où le statisticien mobilise une régression linéaire ou une régression logistisque (imputation, repondération…).\nLes méthodes ensemblistes présentent trois avantages par rapport aux méthodes économétriques traditionnelles (régression linéaire et régression logistique):\n\nElles ont une puissance prédictive supérieure: alors que les méthodes traditionnelles supposent fréquemment l’existence d’une relation linéaire ou log-linéaire entre \\(y\\) et \\(\\mathbf{X}\\), les méthodes ensemblistes ne font quasiment aucune hypothèse sur la relation entre \\(y\\) et \\(\\mathbf{X}\\), et se contentent d’approximer le mieux possible cette relation à partir des données disponibles. En particulier, les modèles ensemblistes peuvent facilement modéliser des non-linéarités de la relation entre \\(y\\) et \\(\\mathbf{X}\\) et des interactions entre variables explicatives sans avoir à les spécifier explicitement au préalable, alors que les méthodes traditionnelles supposent fréquemment l’existence d’une relation linéaire ou log-linéaire entre \\(y\\) et \\(\\mathbf{X}\\).\nElles nécessitent moins de préparation des données: elles ne requièrent pas de normalisation des variables explicatives et peuvent s’accommoder des valeurs manquantes (selon des techniques variables selon les algorithmes).\nElles sont généralement moins sensibles aux valeurs extrêmes et à l’hétéroscédasticité des variables explicatives que les approches traditionnelles.\n\nElles présentent par ailleurs deux inconvénients rapport aux méthodes économétriques traditionnelles. Premièrement, bien qu’il existe désormais de multiples approches permettent d’interpétrer partiellement les modèles ensemblistes, leur interprétabilité reste moindre que celle d’une régression linéaire ou logistique. Deuxièmement, les modèles ensemblistes sont plus complexes que les approches traditionnelles, et leurs hyperparamètres doivent faire l’objet d’une optimisation, par exemple au travers d’une validation croisée. Ce processus d’optimisation est généralement plus complexe et plus long que l’estimation d’une régression linéaire ou logistique. En revanche, les méthodes ensemblistes sont relativement simples à prendre en main, et ne requièrent pas nécessairement une puissance de calcul importante.\n\n\n\n\n\n\nEt par rapport au deep learning?\n\n\n\nSi les approches de deep learning sont sans conteste très performantes pour le traitement du langage naturel, des images et du son, leur supériorité n’est pas établie pour les applications reposant sur des données tabulaires. Les comparaisons disponibles dans la littérature concluent en effet que les méthodes ensemblistes à base d’arbres sont soit plus performantes que les approches de deep learning (Grinsztajn, Oyallon, and Varoquaux (2022), Shwartz-Ziv and Armon (2022)), soit font jeu égal avec elles (McElfresh et al. (2024)). Ces études ont identifié trois avantages des méthodes ensemblistes: elles sont peu sensibles aux variables explicatives non pertinentes, robustes aux valeurs extrêmes des variables explicatives, et capables d’approximer des fonctions très irrégulières. De plus, dans la pratique les méthodes ensemblistes sont souvent plus rapides à entraîner et moins gourmandes en ressources informatiques, et l’optimisation des hyperparamètres s’avère souvent moins complexe (Shwartz-Ziv and Armon (2022)).",
    "crumbs": [
      "Survol des méthodes ensemblistes",
      "Aperçu des méthodes ensemblistes"
    ]
  },
  {
    "objectID": "chapters/chapter1/1-survol.html#comment-fonctionnent-les-méthodes-ensemblistes",
    "href": "chapters/chapter1/1-survol.html#comment-fonctionnent-les-méthodes-ensemblistes",
    "title": "1 Aperçu des méthodes ensemblistes",
    "section": "",
    "text": "Ce paragraphe présente d’abord le modèle de base sur lesquelles sont construites les méthodes ensemblistes à base d’arbres: l’arbre de classification et de régression (CART) (Section 1.3.1). Bien que simples et intuitifs, les arbres CART sont souvent insuffisants en termes de performance lorsqu’ils sont utilisés isolément.\nElle introduit ensuite les deux grandes familles de méthodes ensemblistes décrites dans ce document: le bagging et les forêts aléatoires (Section 1.3.2), et le gradient boosting (Section 1.3.3).\n\n\n\n\nLe modèle de base des méthodes ensemblistes est souvent un arbre de classification et de régression (CART, Breiman et al. (1984)). Un arbre CART est un algorithme prédictif qui traite un problème de prédiction complexe en le décomposant en une série de décisions simples, organisées de manière hiérarchique. Ces décisions permettent de segmenter progressivement les données en régions homogènes au sein desquelles il est plus simple de faire des prédictions. Il s’agit d’un outil puissant pour explorer les relations entre les variables explicatives et la variable cible, sans recourir à des hypothèses a priori sur la forme de cette relation.\nTrois caractéristiques essentielles définissent un arbre CART :\n\nL’arbre partitionne l’espace des variables explicatives \\(X\\) en régions (appelées feuilles ou leaves) les plus homogènes possible, au sens d’une mesure de l’hétérogénéité (par exemple, l’entropie ou l’erreur quadratique moyenne). Ces divisions vont permettre de regrouper des observations similaires pour faciliter la prédiction;\nChaque région est définie par un ensemble de conditions, appelées régles de décision (splitting rules), appliquées successivement sur les variables explicatives. Par exemple, une première règle pourrait poser la question : “L’individu est-il en emploi ?”, et subdiviser les données en deux groupes (oui/non). Une deuxième règle pourrait alors affiner la segmentation en posant la question : “L’individu est-il diplômé du supérieur ?”. Une région spécifique serait ainsi définie par la condition combinée : “l’individu est en emploi et est diplômé du supérieur”.\nUne fois l’arbre construit, chaque feuille produit une prédiction en se basant sur les données de la région correspondante. En classification, la prédiction est généralement la classe la plus fréquente parmi les observations de la région. En régression, la prédiction est souvent la moyenne des valeurs observées dans la région.\n\nDeux conséquences importantes découlent de cette construction : - L’algorithme CART ne fait aucune hypothèse a priori sur la relation entre les variables explicatives \\(\\mathbf{X}\\) et la variable cible \\(y\\). C’est une différence majeure avec les modèles économétriques standard, tels que la régression linéaire qui suppose une relation linéaire de la forme \\(E(y) = \\mathbf{X \\beta}\\). - L’arbre final est une fonction constante par morceaux: la prédiction est la même pour toutes les observations situées dans la même région ; elle ne peut varier qu’entre régions.\nIllustration, et représentation graphique (sous forme d’arbre et de graphique).\n\n\n\n\nLes arbres CART présentent plusieurs avantages: leur principe est simple, ils sont aisément interprétables et peuvent faire l’objet de représentations graphiques intuitives. Par ailleurs, la flexibilité offerte par le partitionnement récursif assure que les arbres obtenus reflètent les corrélations observées dans les données d’entraînement.\nIls souffrent néanmoins de deux limites. D’une part, les arbres CART ont souvent un pouvoir prédictif faible qui en limite l’usage. D’autre part, ils sont peu robustes et instables: on dit qu’ils présentent une variance élevée. Ainsi, un léger changement dans les données (par exemple l’ajout ou la suppression de quelques observations) peut entraîner des modifications significatives dans la structure de l’arbre et dans la définition des régions utilisées pour la prédiction (feuilles). Les arbres CART sont notamment sensibles aux valeurs extrêmes, aux points aberrants et au bruit statistique. De plus, les prédictions des arbres CART sont sensibles à de petites fluctuations des données d’échantillonnage: celles-ci peuvent aboutir à ce qu’une partie des observations change brutalement de feuille et donc de valeur prédite.\nCes limites motivent l’utilisation des deux familles de méthodes ensemblistes présentées dans la suite (le bagging, dont la random forests, et le gradient boosting), qui s’appuient sur un grand nombre d’arbres pour accroître à la fois la précision et la stabilité des prédictions. La différence essentielle entre ces deux familles portent sur la façon dont les arbres sont entraînés.\n\n\n\n\n\n\nLes familles de méthodes ensemblistes\n\n\n\nLes méthodes ensemblistes basées sur des arbres de décision se répartissent en deux grandes familles, qui se distinguent selon la manière dont les modèles de base sont construits. Lorsque les modèles de base sont entraînés en parallèle et indépendamment les uns des autres, on parle de bagging (Bootstrap Aggregating). La forêt aléatoire (random forest) est une variante particulièrement performante du bagging. Lorsque les modèles de base sont entraînés de manière séquentielle, chaque modèle visant à corriger les erreurs des modèles précédents, on parle de boosting. Ce document aborde essentiellement le gradient boosting, qui est l’approche de boosting la plus utilisée actuellement.\n\n\n\n\n\n\n\n\nLe bagging (Bootstrap Aggregating) est une méthode ensembliste qui repose sur l’agrégation des prédictions de plusieurs modèles individuels, entraînés indépendamment les uns des autres, pour construire un modèle global plus performant (Breiman (1996)). Cette approche constitue également le socle des forêts aléatoires, qui en sont une version améliorée.\nLe bagging offre deux avantages majeurs par rapport aux arbres de décision CART : une meilleure capacité prédictive et une plus grande stabilité des prédictions. Cette amélioration découle de la stratégie d’entraînement. Au lieu d’entraîner un seul modèle sur l’ensemble des données, le bagging procède en trois étapes principales:\n\nTirage de sous-échantillons aléatoires: À partir du jeu de données initial, plusieurs sous-échantillons sont générés par échantillonnage aléatoire avec remise (bootstrapping). Chaque sous-échantillon a la même taille que le jeu de données original, mais peut contenir des observations répétées, tandis que d’autres peuvent être omises.\nEntraînement parallèle: Un arbre est entraîné sur chaque sous-échantillon de manière indépendante. Ces arbres sont habituellement assez complexes et profonds.\nAgrégation des prédictions: Les prédictions des modèles sont combinées pour produire le résultat final. En classification, la prédiction finale est souvent déterminée par un vote majoritaire, tandis qu’en régression, elle correspond généralement à la moyenne des prédictions.\n\n\n\n\n\n\n\nFigure 1: Représentation schématique d’un algorithme de bagging\n\n\n\nLa 1 propose une représentation schématique du bagging: d’abord, des sous-échantillons sont générés aléatoires avec remise à partir du jeu de données d’entraînement. Ensuite, des arbres de décision sont entraînés indépendamment sur ces sous-échantillons. Enfin, leurs prédictions sont agrégées pour obtenir les prédictions finales. On procède généralement au vote majoritaire (la classe prédite majoritairement par les arbres) dans un problème de classification, et à la moyenne dans un problème de régression.\nL’efficacité du bagging provient de la réduction de la variance qui est permise par l’agrégation des prédictions. Chaque arbre est entraîné sur un sous-échantillon légèrement différent, sujet à des fluctuations aléatoires. L’agrégation des prédictions (par moyenne ou vote majoritaire) de tous les arbres réduit la sensibilité du modèle final aux fluctuations des données d’entraînement. Le modèle final est ainsi plus robuste et plus précis que chacun des arbres pris individuellement.\n\nIllustration avec un cas d’usage de classification en deux dimensions.\n\nMalgré ses avantages, le bagging souffre d’une limite importante qui provient de la corrélation entre les arbres. En effet, malgré le tirage aléatoire des sous-échantillons, les arbres présentent souvent des structures similaires, car les règles de décision sous-jacentes restent généralement assez proches. Cette corrélation réduit l’efficacité de l’agrégation et limite les gains en performance.\nPour réduire cette corrélation entre arbres, les forêts aléatoires introduisent une étape supplémentaire de randomisation. Leur supériorité prédictive explique pourquoi le bagging seul est rarement utilisé en pratique. Néanmoins, les forêts aléatoires tirent leur efficacité des principes fondamentaux du bagging.\n\n\n\nLes forêts aléatoires (random forests, Breiman (2001)) sont une variante du bagging qui vise à produire des modèles très performants en conciliant deux objectifs: maximiser le pouvoir prédictif des arbres pris isolément, et minimiser la corrélation entre ces arbres (le problème inhérent au bagging).\nPour atteindre ce second objectif, la forêt aléatoire introduit une nouvelle source de randomisation: la sélection aléatoire de variables. Lors de la construction de chaque arbre, au lieu d’utiliser toutes les variables disponibles pour déterminer la meilleure séparation à chaque nœud, un sous-ensemble aléatoire de variables est sélectionné. En limitant la quantité d’information à laquelle chaque arbre a accès au moment de chaque nouvelle division, cette étape supplémentaire contraint mécaniquement les arbres à être plus diversifiés (car deux arbres ne pourront plus nécessairement choisir les mêmes variables pour les mêmes séparations). Cela réduit significativement la corrélation entre les arbres, améliorant ainsi l’efficacité de l’agrégation. L’ensemble des prédictions devient ainsi plus précis et moins sujet aux fluctuations aléatoires.\n\n\n\n\n\n\nFigure 2: Représentation schématique d’un algorithme de forêt aléatoire\n\n\n\nLa figure 2 propose une représentation schématique d’une forêt aléatoire. La logique d’ensemble reste la même que celle du bagging. L’échantillonnage bootstrap est inchangé, mais l’étape de construction de chaque arbre est modifiée pour n’utiliser, à chaque nouvelle division, qu’un sous-ensemble aléatoire de variables. L’agrégation des prédictions se fait ensuite de la même manière que pour le bagging.\n\nLe principal enjeu de l’entraînement d’une forêt aléatoire est de trouver le bon arbitrage entre puissance prédictive des arbres individuels (que l’on souhaite maximiser) et corrélation entre les arbres (que l’on souhaite minimiser). L’optimisation des hyper-paramètres des forêts aléatoires (dont le plus important est le nombre de variables sélectionnées à chaque noeud) vise précisément à choisir le meilleur compromis possible entre pouvoir prédictif invividuel et diversité des arbres.\nLes forêts aléatoires sont très populaires car elles sont faciles à implémenter, peu sensibles aux hyperparamètres (elles fonctionnent bien avec les valeurs par défaut de la plupart des implémentations proposées en R ou en Python), et offrent de très bonnes performances dans de nombreux cas. Cependant, comme toute méthode d’apprentissage automatique, elles restent sujettes au surapprentissage (voir encadré), bien que dans une moindre mesure par rapport à d’autres techniques comme le gradient boosting.\n\n\n\n\n\n\n\nQu’est-ce que le surapprentissage?\n\n\n\nLe surapprentissage (overfitting) est un phénomène fréquent en machine learning où un modèle apprend non seulement les relations sous-jacentes entre la variable cible et les variables explicatives, mais également le bruit présent dans les données d’entraînement. En capturant ces fluctuations aléatoires plutôt que les tendances générales, le modèle affiche une performance excellente mais trompeuse sur les données d’entraînement, et s’avère médiocre sur des données nouvelles ou de test, car il ne parvient pas à généraliser efficacement.\n\n\n\n\n\n\n\n\nContrairement aux forêts aléatoires qui combinent des arbres de décision complexes et indépendants, le gradient boosting construit un ensemble d’arbres plus simples et entraînés de manière séquentielle. Chaque arbre vise à corriger les erreurs commises par les arbres précédents, améliorant progressivement la précision du modèle global. Cette approche repose sur des fondements théoriques très différents de ceux du bagging.\nLa logique du gradient boosting est illustrée par la figure 3:\n\n\n\n\n\n\nFigure 3: Représentation schématique d’un algorithme de gradient boosting\n\n\n\n\nUn premier modèle simple et peu performant est entraîné sur les données.\nUn deuxième modèle est entraîné de façon à corriger les erreurs du premier modèle (par exemple en pondérant davantage les observations mal prédites);\nCe processus est répété en ajoutant des modèles simples, chaque modèle corrigeant les erreurs commises par l’ensemble des modèles précédents;\nTous ces modèles sont finalement combinés (souvent par une somme pondérée) pour obtenir un modèle complexe et performant.\n\nLe gradient boosting offre des performances élevées mais exige une attention particulière portée sur la configuration des hyperparamètres et sur la prévention du surapprentissage. En particulier, les hyperparamètres sont nombreux et, contrairement aux forêts aléatoires, nécessitent un ajustement minutieux pour obtenir des résultats optimaux. Une mauvaise configuration peut conduire à des performances médiocres ou à un surapprentissage. L’utilisation du gradient boosting nécessite donc une bonne connaissance du fonctionnement des algorithmes. En outre, les algorithmes de gradient boosting peuvent être sensibles au bruit dans les données et aux erreurs dans la variable cible. Un prétraitement rigoureux des données est donc essentiel. Enfin, une validation rigoureuse sur un jeu de données de test indépendant (non utilisé pendant l’entraînement) est indispensable pour évaluer la qualité du modèle obtenu par gradient boosting.",
    "crumbs": [
      "Survol des méthodes ensemblistes",
      "Aperçu des méthodes ensemblistes"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "",
    "text": "1 Introduction\nUne bien belle introduction pour le site et le DT.\nLa version pdf de ce document est disponible ici.",
    "crumbs": [
      "Introduction aux méthodes ensemblistes"
    ]
  },
  {
    "objectID": "chapters/chapter1/2-comparaison_GB_RF.html",
    "href": "chapters/chapter1/2-comparaison_GB_RF.html",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "",
    "text": "Les forêts aléatoires et le gradient boosting paraissent très similaires au premier abord: il s’agit de deux approches ensemblistes, qui construisent des modèles très prédictifs performants en combinant un grand nombre d’arbres de décision. Mais en réalité, ces deux approches présentent plusieurs différences fondamentales:\n\nLes deux approches reposent sur des fondements théoriques différents: la loi des grands nombres pour les forêts aléatoires, la théorie de l’apprentissage statistique pour le boosting.\nLes arbres n’ont pas le même statut dans les deux approches. Dans une forêt aléatoire, les arbres sont entraînés indépendamment les uns des autres et constituent chacun un modèle à part entière, qui peut être utilisé, représenté et interprété isolément. Dans un modèle de boosting, les arbres sont entraînés séquentiellement, ce qui implique que chaque arbre n’a pas de sens indépendamment de l’ensemble des arbres qui l’ont précédé dans l’entraînement. Par ailleurs, les arbres d’une forêt aléatoire sont relativement complexes et profonds (car ce sont des modèles à part entière), alors que dans le boosting les arbres sont plus souvent simples et peu profonds.\nLes points d’attention lors de l’entraînement des algorithmes sont différents: l’enjeu principal de l’entraînement d’une forêt aléatoire est trouver le bon arbitrage entre puissance prédictive des arbres et corrélation entre arbres, tandis que l’entraînement d’un algorithme de gradient boosting porte davantage sur la lutte contre le surapprentissage.\nComplexité d’usage: les forêts aléatoires s’avèrent plus faciles à prendre en main que le gradient boosting, car elles comprennent moins d’hyperparamètres dont l’optimisation est moins complexe.\nConditions d’utilisation: il est possible d’évaluer la qualité d’une forêt aléatoire en utilisant les données sur lesquelles elle a été entraînée grâce à l’approche out-of-bag, alors que c’est impossible avec le gradient boosting, pour lequel il faut impérativement conserver un ensemble de test. Cette différence peut sembler purement technique en apparence, mais elle s’avère importante en pratique dans de nombreuses situations, par exemple lorsque les données disponibles sont de taille restreinte ou lorsque les ressources informatiques disponibles ne sont pas suffisantes pour mener un exercice de validation croisée.\n\n\n\nLe point de départ recommandé est de commencer par entraîner une forêt aléatoire avec les hyperparamètres par défaut.",
    "crumbs": [
      "Survol des méthodes ensemblistes",
      "Comparaison entre forêts aléatoires et _gradient boosting_"
    ]
  },
  {
    "objectID": "chapters/chapter1/2-comparaison_GB_RF.html#comparaison-entre-forêts-aléatoires-et-gradient-boosting",
    "href": "chapters/chapter1/2-comparaison_GB_RF.html#comparaison-entre-forêts-aléatoires-et-gradient-boosting",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "",
    "text": "Les forêts aléatoires et le gradient boosting paraissent très similaires au premier abord: il s’agit de deux approches ensemblistes, qui construisent des modèles très prédictifs performants en combinant un grand nombre d’arbres de décision. Mais en réalité, ces deux approches présentent plusieurs différences fondamentales:\n\nLes deux approches reposent sur des fondements théoriques différents: la loi des grands nombres pour les forêts aléatoires, la théorie de l’apprentissage statistique pour le boosting.\nLes arbres n’ont pas le même statut dans les deux approches. Dans une forêt aléatoire, les arbres sont entraînés indépendamment les uns des autres et constituent chacun un modèle à part entière, qui peut être utilisé, représenté et interprété isolément. Dans un modèle de boosting, les arbres sont entraînés séquentiellement, ce qui implique que chaque arbre n’a pas de sens indépendamment de l’ensemble des arbres qui l’ont précédé dans l’entraînement. Par ailleurs, les arbres d’une forêt aléatoire sont relativement complexes et profonds (car ce sont des modèles à part entière), alors que dans le boosting les arbres sont plus souvent simples et peu profonds.\nLes points d’attention lors de l’entraînement des algorithmes sont différents: l’enjeu principal de l’entraînement d’une forêt aléatoire est trouver le bon arbitrage entre puissance prédictive des arbres et corrélation entre arbres, tandis que l’entraînement d’un algorithme de gradient boosting porte davantage sur la lutte contre le surapprentissage.\nComplexité d’usage: les forêts aléatoires s’avèrent plus faciles à prendre en main que le gradient boosting, car elles comprennent moins d’hyperparamètres dont l’optimisation est moins complexe.\nConditions d’utilisation: il est possible d’évaluer la qualité d’une forêt aléatoire en utilisant les données sur lesquelles elle a été entraînée grâce à l’approche out-of-bag, alors que c’est impossible avec le gradient boosting, pour lequel il faut impérativement conserver un ensemble de test. Cette différence peut sembler purement technique en apparence, mais elle s’avère importante en pratique dans de nombreuses situations, par exemple lorsque les données disponibles sont de taille restreinte ou lorsque les ressources informatiques disponibles ne sont pas suffisantes pour mener un exercice de validation croisée.\n\n\n\nLe point de départ recommandé est de commencer par entraîner une forêt aléatoire avec les hyperparamètres par défaut.",
    "crumbs": [
      "Survol des méthodes ensemblistes",
      "Comparaison entre forêts aléatoires et _gradient boosting_"
    ]
  },
  {
    "objectID": "chapters/chapter2/1-CART.html",
    "href": "chapters/chapter2/1-CART.html",
    "title": "1 La brique élémentaire: l’arbre de décision",
    "section": "",
    "text": "Les arbres de décision sont des outils puissants en apprentissage automatique, utilisés pour des tâches de classification et de régression. Ces algorithmes non paramétriques consistent à diviser l’espace des caractéristiques en sous-ensembles homogènes à l’aide de règles simples, afin de faire des prédictions. Malgré leur simplicité apparente, les arbres de décision sont capable de saisir des relations complexes et non linéaires entre les variables (ou caractéristiques) d’un jeu de données.\n\n\nImaginez que vous souhaitiez prédire le prix d’une maison en fonction de sa superficie et de son nombre de pièces. L’espace des caractéristiques (superficie et nombre de pièces) est vaste, et les prix des maisons (la réponse à prédire) sont très variables. Pour prédire le prix des maisons, l’idée est de diviser cet espace en zones plus petites, où les maisons ont des prix similaires, et d’attribuer une prédiction identique à toutes les maisons situées dans la même zone.\n\n\nL’objectif principal est de trouver la partition de l’espace des caractéristiques qui offre les meilleures prédictions possibles. Cependant, cet objectif se heurte à plusieurs difficultés, et la complexité du problème augmente rapidement avec le nombre de caractéristiques et la taille de l’échantillon:\n\nInfinité des découpages possibles : Il existe une infinité de façons de diviser l’espace des caractéristiques.\nComplexité de la paramétrisation : Il est difficile de représenter tous ces découpages avec un nombre limité de paramètres.\nOptimisation complexe : Même avec une paramétrisation, trouver le meilleur découpage nécessite une optimisation complexe, souvent irréaliste en pratique.\n\n\n\n\nPour surmonter ces difficultés, les méthodes d’arbres de décision, et notamment la plus célèbre, l’algorithme CART (Classication And Regression Tree, Breiman et al. (1984)), adoptent deux approches clés :\n\nSimplification du partitionnement de l’espace\n\nAu lieu d’explorer tous les découpages possibles, les arbres de décision partitionnent l’espace des caractéristiques en plusieurs régions distinctes (non chevauchantes) en appliquant des règles de décision simples. Les règles suivantes sont communément adoptées:\n\nDécoupages binaires simples : À chaque étape, l’algorithme divise une région de l’espace en deux sous-régions en se basant sur une seule caractéristique (ou variable) et en définissant un seul seuil (ou critère) pour cette segmentation. Concrètement, cela revient à poser une question du type : “La valeur de la caractéristique X dépasse-t-elle un certain seuil ?” Par exemple : “La superficie de la maison est-elle supérieure à 100 m² ?”. Les deux réponses possibles (“Oui” ou “Non”) génèrent deux nouvelles sous-régions distinctes de l’espace, chacune correspondant à un sous-ensemble de données plus homogène.\nPrédictions locales : Lorsque l’algorithme s’arrête, une prédiction simple est faite dans chaque région. Il s’agit souvent de la moyenne des valeurs cibles dans cette région (régression) ou de la classe majoritaire (classification).\n\nCes règles de découpage rendent le problème d’optimisation plus simple mais également plus interprétable.\n\nOptimisation gloutonne (greedy)\n\nPlutôt que d’optimiser toutes les divisions simultanément, les arbres de décision utilisent une approche simplifiée, récursive et séquentielle :\n\nDivision étape par étape : À chaque étape, l’arbre choisit la meilleure division possible sur la base d’un critère de réduction de l’hétérogénéité intra-région. En revanche, il ne prend pas en compte les étapes d’optimisation futures.\nCritère local : La décision est basée sur la réduction immédiate de l’impureté ou de l’erreur de prédiction (par exemple, la réduction de la variance pour la régression). Ce processus est répété pour chaque sous-région, ce qui permet d’affiner progressivement la partition de l’espace en fonction des caractéristiques les plus discriminantes.\n\nCette méthode dite “gloutonne” (greedy) s’avère efficace pour construire un partitionnement de l’espace des caractéristiques, car elle décompose un problème d’optimisation complexe en une succession de problèmes plus simples et plus rapides à résoudre. Le résultat obtenu n’est pas nécessairement un optimum global, mais il s’en approche raisonnablement et surtout rapidement.\nLe terme “arbre de décision” provient de la structure descendante en forme d’arbre inversé qui émerge lorsqu’on utilise un algorithme glouton pour découper l’espace des caractéristiques en sous-ensemble de réponses homogènes de manière récursive. A chaque étape, deux nouvelles branches sont créées et forment une nouvelle partition de l’espace des caractéristiques.\nUne fois entraîné, un arbre de décision est une fonction constante par morceaux défini sur l’espace des caractéristiques. En raison de leur nature non-continue et non-différentiable, il est impossible d’utiliser des méthodes d’optimisation classiques reposant sur le calcul de gradients.\n\n\n\nNous présentons la structure d’un arbre de décision et les principaux éléments qui le composent.\n\nNœud Racine (Root Node) : Le nœud racine est le point de départ de l’arbre de décision, il est situé au sommet de l’arbre. Il contient l’ensemble des données d’entraînement avant toute division. À ce niveau, l’algorithme cherche la caractéristique la plus discriminante, c’est-à-dire celle qui permet de diviser les données de manière à optimiser une fonction de perte (comme l’indice de Gini pour la classification ou la variance pour la régression).\nNœuds Internes (Internal Nodes) : Les nœuds internes sont les points intermédiaires où l’algorithme CART applique des règles de décision pour diviser les données en sous-ensembles plus petits. Chaque nœud interne représente une question ou condition basée sur une caractéristique particulière (par exemple, “La superficie de la maison est-elle supérieure à 100 m² ?”). À chaque étape, une seule caractéristique (la superficie) et un seul seuil (supérieur à 100) sont utilisés pour faire la division.\nBranches: Les branches sont les connexions entre les nœuds, elles illustrent le chemin que les données suivent en fonction des réponses aux questions posées dans les nœuds internes. Chaque branche correspond à une décision binaire, “Oui” ou “Non”, qui oriente les observations vers une nouvelle subdivision de l’espace des caractéristiques.\nNœuds Terminaux ou Feuilles (Leaf Nodes ou Terminal Nodes) : Les nœuds terminaux, situés à l’extrémité des branches, sont les points où le processus de division s’arrête. Ils fournissent la prédiction finale.\n\nEn classification, chaque feuille correspond à une classe prédite (par exemple, “Oui” ou “Non”).\nEn régression, chaque feuille fournit une valeur numérique prédite (comme le prix estimé d’une maison).\n\n\nFigure illustrative : Une représentation visuelle de la structure de l’arbre peut être utile ici pour illustrer les concepts de nœuds, branches et feuilles.\n\n\n\nSupposons que nous souhaitions prédire le prix d’une maison en fonction de sa superficie et de son nombre de pièces. Un arbre de décision pourrait procéder ainsi :\n\nPremière division : “La superficie de la maison est-elle supérieure à 100 m² ?”\n\nOui : Aller à la branche de gauche.\nNon : Aller à la branche de droite.\n\nDeuxième division (branche de gauche) : “Le nombre de pièces est-il supérieur à 4 ?”\n\nOui : Prix élevé (par exemple, plus de 300 000 €).\nNon : Prix moyen (par exemple, entre 200 000 € et 300 000 €).\n\nDeuxième division (branche de droite) : “Le nombre de pièces est-il supérieur à 2 ?”\n\nOui : Prix moyen (par exemple, entre 150 000 € et 200 000 €).\nNon : Prix bas (par exemple, moins de 150 000 €).\n\n\nCet arbre utilise des règles simples pour diviser l’espace des caractéristiques (superficie et nombre de pièces) en sous-groupes homogènes et fournir une prédiction (estimer le prix d’une maison).\nFigure illustrative\n\n\n\n\nL’algorithme CART (Classification and Regression Trees) proposé par Breiman et al. (1984) est une méthode utilisée pour construire des arbres de décision, que ce soit pour des tâches de classification ou de régression. L’algorithme CART fonctionne en partitionnant l’espace des caractéristiques en sous-ensembles de manière récursive, en suivant une logique de décisions binaires à chaque étape. Ce processus est itératif et suit plusieurs étapes clés.\n\n\nLa fonction d’impureté est une mesure locale utilisée dans la construction des arbres de décision pour évaluer la qualité des divisions à chaque nœud. Elle quantifie le degré d’hétérogénéité des observations dans un nœud par rapport à la variable cible (classe pour la classification, ou valeur continue pour la régression). Plus précisément, une mesure d’impureté est conçue pour croître avec la dispersion dans un nœud. Un nœud est dit pur lorsque toutes les observations qu’il contient appartiennent à la même classe (classification) ou présentent des valeurs similaires/identiques (régression).\nL’algorithme CART utilise ce type de mesure pour choisir les divisions qui créent des sous-ensembles plus homogènes que le nœud parent. À chaque étape de construction, l’algorithme sélectionne la division qui réduit le plus l’impureté, afin de garantir des nœuds de plus en plus homogènes au fur et à mesure que l’arbre se développe.\nLe choix de la fonction d’impureté dépend du type de problème :\n\nClassification : L’indice de Gini ou l’entropie sont très souvent utilisées pour évaluer la dispersion des classes dans chaque nœud.\nRégression : La somme des erreurs quadratiques (SSE) est souvent utilisée pour mesurer la variance des valeurs cibles dans chaque nœud.\n\n\n\nDans le cadre de la classification, l’objectif est de partitionner les données de manière à ce que chaque sous-ensemble (ou région) soit le plus homogène possible en termes de classe prédite. Plusieurs mesures d’impureté sont couramment utilisées pour évaluer la qualité des divisions.\nPropriété-définition d’une mesure d’impureté\nPour un nœud \\(t\\) contenant \\(K\\) classes, une mesure d’impureté \\(I(t)\\) est une fonction qui quantifie l’hétérogénéité des classes dans ce nœud. Elle doit satisfaire les propriétés suivantes :\n\nPureté maximale : Lorsque toutes les observations du nœud appartiennent à une seule classe, c’est-à-dire que la proportion \\(p_k = 1\\) pour une classe \\(k\\) et \\(p_j = 0\\) pour toutes les autres classes \\(j \\neq k\\), l’impureté est minimale et \\(I(t) = 0\\). Cela indique que le nœud est entièrement pur, ou homogène.\nImpureté maximale : Lorsque les observations sont réparties de manière uniforme entre toutes les classes, c’est-à-dire que \\(p_k = \\frac{1}{K}\\) pour chaque classe \\(k\\), l’impureté atteint son maximum. Cette situation reflète une impureté élevée, car le nœud est très hétérogène et contient une forte incertitude sur la classe des observations.\n\n1. L’indice de Gini\nL’indice de Gini est l’une des fonctions de perte les plus couramment utilisées pour la classification. Il mesure la probabilité qu’un individu sélectionné au hasard dans un nœud soit mal classé si on lui attribue une classe au hasard, en fonction de la distribution des classes dans ce nœud.\nPour un nœud \\(t\\) contenant \\(K\\) classes, l’indice de Gini \\(G(t)\\) est donné par :\n\\[\nG(t) = 1 - \\sum_{k=1}^{K} p_k^2\n\\]\noù \\(p_k\\) est la proportion d’observations appartenant à la classe \\(k\\) dans le nœud \\(t\\).\n\nCritère de choix : L’indice de Gini est souvent utilisé parce qu’il est simple à calculer et capture bien l’homogénéité des classes au sein d’un nœud. Il privilégie les partitions où une classe domine fortement dans chaque sous-ensemble.\n2. L’entropie (ou entropie de Shannon)\nL’entropie est une autre mesure de l’impureté utilisée dans les arbres de décision. Elle mesure la quantité d’incertitude ou de désordre dans un nœud, en s’appuyant sur la théorie de l’information.\nPour un nœud \\(t\\) contenant \\(K\\) classes, l’entropie \\(E(t)\\) est définie par :\n\\[\nE(t) = - \\sum_{k=1}^{K} p_k \\log(p_k)\n\\]\noù \\(p_k\\) est la proportion d’observations de la classe \\(k\\) dans le nœud \\(t\\).\n\nCritère de choix : L’entropie a tendance à être plus sensible aux changements dans les distributions des classes que l’indice de Gini, car elle attribut un poids plus élevé aux événements rares (valeurs de \\(p_k\\) très faibles). Elle est souvent utilisée lorsque l’erreur de classification des classes minoritaires est particulièrement importante.\n3. Taux d’erreur\nLe taux d’erreur est une autre mesure de l’impureté parfois utilisée dans les arbres de décision. Il représente la proportion d’observations mal classées dans un nœud.\nPour un nœud \\(t\\), le taux d’erreur \\(\\text{TE}(t)\\) est donné par :\n\\[\n\\text{TE}(t) = 1 - \\max(p_k)\n\\]\noù \\(\\max(p_k)\\) est la proportion d’observations appartenant à la classe majoritaire dans le nœud.\n\nCritère de choix : Bien que le taux d’erreur soit simple à comprendre, il est moins souvent utilisé dans la construction des arbres de décision parce qu’il est moins sensible que l’indice de Gini ou l’entropie aux petits changements dans la distribution des classes.\n\n\n\nDans les problèmes de régression, l’objectif est de partitionner les données de manière à réduire au maximum la variabilité des valeurs au sein de chaque sous-ensemble. Pour mesurer cette variabilité, la somme des erreurs quadratiques (SSE) est la fonction d’impureté la plus couramment employée. Elle évalue l’impureté d’une région en quantifiant à quel point les valeurs de cette région s’écartent de la moyenne locale.\n1.Somme des erreurs quadratiques (SSE) ou variance\nLa somme des erreurs quadratiques (ou SSE, pour Sum of Squared Errors) est une mesure qui quantifie la dispersion des valeurs dans un nœud par rapport à la moyenne des valeurs dans ce nœud.\nFormule : Pour un nœud \\(t\\), contenant \\(N\\) observations avec des valeurs \\(y_i\\), la SSE est donnée par :\n\\[\n\\text{SSE}(t) = \\sum_{i=1}^{N} (y_i - \\hat{y})^2\n\\]\noù \\(\\hat{y}\\) est la moyenne des valeurs \\(y_i\\) dans le nœud \\(t\\).\nPropriété :\n\nSi toutes les valeurs de \\(y_i\\) dans un nœud sont proches de la moyenne \\(\\hat{y}\\), la SSE sera faible, indiquant une homogénéité élevée dans le nœud.\nEn revanche, une SSE élevée indique une grande variabilité dans les valeurs, donc un nœud impur.\n\nCritère de choix : La somme des erreurs quadratiques (SSE) est particulièrement sensible aux écarts élevés entre les valeurs observées et la moyenne prédite. En cherchant à minimiser la SSE, les modèles visent à former des nœuds dans lesquels les valeurs des observations sont aussi proches que possible de la moyenne locale.\n\n\n\n\nUne fois la mesure d’impureté définie, l’algorithme CART examine toutes les divisions binaires possibles de l’espace des caractéristiques. À chaque nœud, et pour chaque caractéristique, il cherche à identifier le seuil optimal, c’est-à-dire le seuil qui minimise le plus efficacement l’impureté des deux sous-ensembles générés. L’algorithme compare ensuite toutes les divisions potentielles (caractéristiques et seuils optimaux associés à chaque nœud) et sélectionne celle qui entraîne la réduction maximale de l’impureté.\nPrenons l’exemple d’une caractéristique continue, telle que la superficie d’une maison :\n\nSi l’algorithme teste la règle “Superficie &gt; 100 m²”, il calcule la fonction de perte pour les deux sous-ensembles générés par cette règle (“Oui” et “Non”).\nCe processus est répété pour différentes valeurs seuils afin de trouver la partition qui minimise le plus efficacement l’impureté au sein des sous-ensembles.\n\n\n\n\nL’algorithme CART poursuit le partitionnement de l’espace des caractéristiques en appliquant de manière récursive les mêmes étapes : identification de la caractéristique et du seuil optimal pour chaque nœud, puis sélection du partitionnement binaire qui maximise la réduction de l’impureté. Ce processus est répété jusqu’à ce qu’un critère d’arrêt soit atteint, par exemple :\n\nProfondeur maximale de l’arbre : Limiter le nombre de divisions successives pour éviter un arbre trop complexe.\nNombre minimum d’observations par feuille : Empêcher la création de feuilles contenant très peu d’observations, ce qui réduirait la capacité du modèle à généraliser.\nRéduction minimale de l’impureté à chaque étape\n\n\n\n\n\n\n\nUne fois l’arbre construit, la prédiction pour une nouvelle observation s’effectue en suivant les branches de l’arbre, en partant du nœud racine jusqu’à un nœud terminal (ou feuille). À chaque nœud interne, une décision est prise en fonction des valeurs des caractéristiques de l’observation, ce qui détermine la direction à suivre vers l’un des sous-ensembles. Ce cheminement se poursuit jusqu’à ce que l’observation atteigne une feuille, où la prédiction finale est effectuée.\n\nEn classification, la classe attribuée est celle majoritaire dans la feuille atteinte.\nEn régression, la valeur prédite est généralement la moyenne des valeurs cibles des observations dans la feuille.\n\n\n\n\nPour améliorer la performance de l’arbre, on peut ajuster les hyperparamètres tels que la profondeur maximale ou le nombre minimum d’observations dans une feuille. De plus, des techniques comme la prédiction avec arbres multiples (bagging, forêts aléatoires) permettent de surmonter les limites des arbres individuels, souvent sujets au surapprentissage.\n\n\n\n\n\n\n\nInterprétabilité : Les arbres de décision sont faciles à comprendre et à visualiser.\nSimplicité : Pas besoin de transformations complexes des données.\nFlexibilité : Ils peuvent gérer des caractéristiques numériques et catégorielles, ainsi que les valeurs manquantes.\nGestion des interactions : Modèles non paramétriques, pas d’hypothèses sur les lois par les variables. Ils capturent naturellement les interactions entre les caractéristiques.\n\n\n\n\n\nSurapprentissage : Les arbres trop profonds peuvent surapprendre les données d’entraînement.\nOptimisation locale : L’approche gloutonne peut conduire à des solutions sous-optimales globalement (optimum local).\nStabilité : De petits changements dans les données peuvent entraîner des changements significatifs dans la structure de l’arbre (manque de robustesse).",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "La brique élémentaire: l'arbre de décision"
    ]
  },
  {
    "objectID": "chapters/chapter2/1-CART.html#le-principe-fondamental-partitionner-pour-prédire",
    "href": "chapters/chapter2/1-CART.html#le-principe-fondamental-partitionner-pour-prédire",
    "title": "1 La brique élémentaire: l’arbre de décision",
    "section": "",
    "text": "Imaginez que vous souhaitiez prédire le prix d’une maison en fonction de sa superficie et de son nombre de pièces. L’espace des caractéristiques (superficie et nombre de pièces) est vaste, et les prix des maisons (la réponse à prédire) sont très variables. Pour prédire le prix des maisons, l’idée est de diviser cet espace en zones plus petites, où les maisons ont des prix similaires, et d’attribuer une prédiction identique à toutes les maisons situées dans la même zone.\n\n\nL’objectif principal est de trouver la partition de l’espace des caractéristiques qui offre les meilleures prédictions possibles. Cependant, cet objectif se heurte à plusieurs difficultés, et la complexité du problème augmente rapidement avec le nombre de caractéristiques et la taille de l’échantillon:\n\nInfinité des découpages possibles : Il existe une infinité de façons de diviser l’espace des caractéristiques.\nComplexité de la paramétrisation : Il est difficile de représenter tous ces découpages avec un nombre limité de paramètres.\nOptimisation complexe : Même avec une paramétrisation, trouver le meilleur découpage nécessite une optimisation complexe, souvent irréaliste en pratique.\n\n\n\n\nPour surmonter ces difficultés, les méthodes d’arbres de décision, et notamment la plus célèbre, l’algorithme CART (Classication And Regression Tree, Breiman et al. (1984)), adoptent deux approches clés :\n\nSimplification du partitionnement de l’espace\n\nAu lieu d’explorer tous les découpages possibles, les arbres de décision partitionnent l’espace des caractéristiques en plusieurs régions distinctes (non chevauchantes) en appliquant des règles de décision simples. Les règles suivantes sont communément adoptées:\n\nDécoupages binaires simples : À chaque étape, l’algorithme divise une région de l’espace en deux sous-régions en se basant sur une seule caractéristique (ou variable) et en définissant un seul seuil (ou critère) pour cette segmentation. Concrètement, cela revient à poser une question du type : “La valeur de la caractéristique X dépasse-t-elle un certain seuil ?” Par exemple : “La superficie de la maison est-elle supérieure à 100 m² ?”. Les deux réponses possibles (“Oui” ou “Non”) génèrent deux nouvelles sous-régions distinctes de l’espace, chacune correspondant à un sous-ensemble de données plus homogène.\nPrédictions locales : Lorsque l’algorithme s’arrête, une prédiction simple est faite dans chaque région. Il s’agit souvent de la moyenne des valeurs cibles dans cette région (régression) ou de la classe majoritaire (classification).\n\nCes règles de découpage rendent le problème d’optimisation plus simple mais également plus interprétable.\n\nOptimisation gloutonne (greedy)\n\nPlutôt que d’optimiser toutes les divisions simultanément, les arbres de décision utilisent une approche simplifiée, récursive et séquentielle :\n\nDivision étape par étape : À chaque étape, l’arbre choisit la meilleure division possible sur la base d’un critère de réduction de l’hétérogénéité intra-région. En revanche, il ne prend pas en compte les étapes d’optimisation futures.\nCritère local : La décision est basée sur la réduction immédiate de l’impureté ou de l’erreur de prédiction (par exemple, la réduction de la variance pour la régression). Ce processus est répété pour chaque sous-région, ce qui permet d’affiner progressivement la partition de l’espace en fonction des caractéristiques les plus discriminantes.\n\nCette méthode dite “gloutonne” (greedy) s’avère efficace pour construire un partitionnement de l’espace des caractéristiques, car elle décompose un problème d’optimisation complexe en une succession de problèmes plus simples et plus rapides à résoudre. Le résultat obtenu n’est pas nécessairement un optimum global, mais il s’en approche raisonnablement et surtout rapidement.\nLe terme “arbre de décision” provient de la structure descendante en forme d’arbre inversé qui émerge lorsqu’on utilise un algorithme glouton pour découper l’espace des caractéristiques en sous-ensemble de réponses homogènes de manière récursive. A chaque étape, deux nouvelles branches sont créées et forment une nouvelle partition de l’espace des caractéristiques.\nUne fois entraîné, un arbre de décision est une fonction constante par morceaux défini sur l’espace des caractéristiques. En raison de leur nature non-continue et non-différentiable, il est impossible d’utiliser des méthodes d’optimisation classiques reposant sur le calcul de gradients.\n\n\n\nNous présentons la structure d’un arbre de décision et les principaux éléments qui le composent.\n\nNœud Racine (Root Node) : Le nœud racine est le point de départ de l’arbre de décision, il est situé au sommet de l’arbre. Il contient l’ensemble des données d’entraînement avant toute division. À ce niveau, l’algorithme cherche la caractéristique la plus discriminante, c’est-à-dire celle qui permet de diviser les données de manière à optimiser une fonction de perte (comme l’indice de Gini pour la classification ou la variance pour la régression).\nNœuds Internes (Internal Nodes) : Les nœuds internes sont les points intermédiaires où l’algorithme CART applique des règles de décision pour diviser les données en sous-ensembles plus petits. Chaque nœud interne représente une question ou condition basée sur une caractéristique particulière (par exemple, “La superficie de la maison est-elle supérieure à 100 m² ?”). À chaque étape, une seule caractéristique (la superficie) et un seul seuil (supérieur à 100) sont utilisés pour faire la division.\nBranches: Les branches sont les connexions entre les nœuds, elles illustrent le chemin que les données suivent en fonction des réponses aux questions posées dans les nœuds internes. Chaque branche correspond à une décision binaire, “Oui” ou “Non”, qui oriente les observations vers une nouvelle subdivision de l’espace des caractéristiques.\nNœuds Terminaux ou Feuilles (Leaf Nodes ou Terminal Nodes) : Les nœuds terminaux, situés à l’extrémité des branches, sont les points où le processus de division s’arrête. Ils fournissent la prédiction finale.\n\nEn classification, chaque feuille correspond à une classe prédite (par exemple, “Oui” ou “Non”).\nEn régression, chaque feuille fournit une valeur numérique prédite (comme le prix estimé d’une maison).\n\n\nFigure illustrative : Une représentation visuelle de la structure de l’arbre peut être utile ici pour illustrer les concepts de nœuds, branches et feuilles.\n\n\n\nSupposons que nous souhaitions prédire le prix d’une maison en fonction de sa superficie et de son nombre de pièces. Un arbre de décision pourrait procéder ainsi :\n\nPremière division : “La superficie de la maison est-elle supérieure à 100 m² ?”\n\nOui : Aller à la branche de gauche.\nNon : Aller à la branche de droite.\n\nDeuxième division (branche de gauche) : “Le nombre de pièces est-il supérieur à 4 ?”\n\nOui : Prix élevé (par exemple, plus de 300 000 €).\nNon : Prix moyen (par exemple, entre 200 000 € et 300 000 €).\n\nDeuxième division (branche de droite) : “Le nombre de pièces est-il supérieur à 2 ?”\n\nOui : Prix moyen (par exemple, entre 150 000 € et 200 000 €).\nNon : Prix bas (par exemple, moins de 150 000 €).\n\n\nCet arbre utilise des règles simples pour diviser l’espace des caractéristiques (superficie et nombre de pièces) en sous-groupes homogènes et fournir une prédiction (estimer le prix d’une maison).\nFigure illustrative",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "La brique élémentaire: l'arbre de décision"
    ]
  },
  {
    "objectID": "chapters/chapter2/1-CART.html#lalgorithme-cart-un-partitionnement-binaire-récursif",
    "href": "chapters/chapter2/1-CART.html#lalgorithme-cart-un-partitionnement-binaire-récursif",
    "title": "1 La brique élémentaire: l’arbre de décision",
    "section": "",
    "text": "L’algorithme CART (Classification and Regression Trees) proposé par Breiman et al. (1984) est une méthode utilisée pour construire des arbres de décision, que ce soit pour des tâches de classification ou de régression. L’algorithme CART fonctionne en partitionnant l’espace des caractéristiques en sous-ensembles de manière récursive, en suivant une logique de décisions binaires à chaque étape. Ce processus est itératif et suit plusieurs étapes clés.\n\n\nLa fonction d’impureté est une mesure locale utilisée dans la construction des arbres de décision pour évaluer la qualité des divisions à chaque nœud. Elle quantifie le degré d’hétérogénéité des observations dans un nœud par rapport à la variable cible (classe pour la classification, ou valeur continue pour la régression). Plus précisément, une mesure d’impureté est conçue pour croître avec la dispersion dans un nœud. Un nœud est dit pur lorsque toutes les observations qu’il contient appartiennent à la même classe (classification) ou présentent des valeurs similaires/identiques (régression).\nL’algorithme CART utilise ce type de mesure pour choisir les divisions qui créent des sous-ensembles plus homogènes que le nœud parent. À chaque étape de construction, l’algorithme sélectionne la division qui réduit le plus l’impureté, afin de garantir des nœuds de plus en plus homogènes au fur et à mesure que l’arbre se développe.\nLe choix de la fonction d’impureté dépend du type de problème :\n\nClassification : L’indice de Gini ou l’entropie sont très souvent utilisées pour évaluer la dispersion des classes dans chaque nœud.\nRégression : La somme des erreurs quadratiques (SSE) est souvent utilisée pour mesurer la variance des valeurs cibles dans chaque nœud.\n\n\n\nDans le cadre de la classification, l’objectif est de partitionner les données de manière à ce que chaque sous-ensemble (ou région) soit le plus homogène possible en termes de classe prédite. Plusieurs mesures d’impureté sont couramment utilisées pour évaluer la qualité des divisions.\nPropriété-définition d’une mesure d’impureté\nPour un nœud \\(t\\) contenant \\(K\\) classes, une mesure d’impureté \\(I(t)\\) est une fonction qui quantifie l’hétérogénéité des classes dans ce nœud. Elle doit satisfaire les propriétés suivantes :\n\nPureté maximale : Lorsque toutes les observations du nœud appartiennent à une seule classe, c’est-à-dire que la proportion \\(p_k = 1\\) pour une classe \\(k\\) et \\(p_j = 0\\) pour toutes les autres classes \\(j \\neq k\\), l’impureté est minimale et \\(I(t) = 0\\). Cela indique que le nœud est entièrement pur, ou homogène.\nImpureté maximale : Lorsque les observations sont réparties de manière uniforme entre toutes les classes, c’est-à-dire que \\(p_k = \\frac{1}{K}\\) pour chaque classe \\(k\\), l’impureté atteint son maximum. Cette situation reflète une impureté élevée, car le nœud est très hétérogène et contient une forte incertitude sur la classe des observations.\n\n1. L’indice de Gini\nL’indice de Gini est l’une des fonctions de perte les plus couramment utilisées pour la classification. Il mesure la probabilité qu’un individu sélectionné au hasard dans un nœud soit mal classé si on lui attribue une classe au hasard, en fonction de la distribution des classes dans ce nœud.\nPour un nœud \\(t\\) contenant \\(K\\) classes, l’indice de Gini \\(G(t)\\) est donné par :\n\\[\nG(t) = 1 - \\sum_{k=1}^{K} p_k^2\n\\]\noù \\(p_k\\) est la proportion d’observations appartenant à la classe \\(k\\) dans le nœud \\(t\\).\n\nCritère de choix : L’indice de Gini est souvent utilisé parce qu’il est simple à calculer et capture bien l’homogénéité des classes au sein d’un nœud. Il privilégie les partitions où une classe domine fortement dans chaque sous-ensemble.\n2. L’entropie (ou entropie de Shannon)\nL’entropie est une autre mesure de l’impureté utilisée dans les arbres de décision. Elle mesure la quantité d’incertitude ou de désordre dans un nœud, en s’appuyant sur la théorie de l’information.\nPour un nœud \\(t\\) contenant \\(K\\) classes, l’entropie \\(E(t)\\) est définie par :\n\\[\nE(t) = - \\sum_{k=1}^{K} p_k \\log(p_k)\n\\]\noù \\(p_k\\) est la proportion d’observations de la classe \\(k\\) dans le nœud \\(t\\).\n\nCritère de choix : L’entropie a tendance à être plus sensible aux changements dans les distributions des classes que l’indice de Gini, car elle attribut un poids plus élevé aux événements rares (valeurs de \\(p_k\\) très faibles). Elle est souvent utilisée lorsque l’erreur de classification des classes minoritaires est particulièrement importante.\n3. Taux d’erreur\nLe taux d’erreur est une autre mesure de l’impureté parfois utilisée dans les arbres de décision. Il représente la proportion d’observations mal classées dans un nœud.\nPour un nœud \\(t\\), le taux d’erreur \\(\\text{TE}(t)\\) est donné par :\n\\[\n\\text{TE}(t) = 1 - \\max(p_k)\n\\]\noù \\(\\max(p_k)\\) est la proportion d’observations appartenant à la classe majoritaire dans le nœud.\n\nCritère de choix : Bien que le taux d’erreur soit simple à comprendre, il est moins souvent utilisé dans la construction des arbres de décision parce qu’il est moins sensible que l’indice de Gini ou l’entropie aux petits changements dans la distribution des classes.\n\n\n\nDans les problèmes de régression, l’objectif est de partitionner les données de manière à réduire au maximum la variabilité des valeurs au sein de chaque sous-ensemble. Pour mesurer cette variabilité, la somme des erreurs quadratiques (SSE) est la fonction d’impureté la plus couramment employée. Elle évalue l’impureté d’une région en quantifiant à quel point les valeurs de cette région s’écartent de la moyenne locale.\n1.Somme des erreurs quadratiques (SSE) ou variance\nLa somme des erreurs quadratiques (ou SSE, pour Sum of Squared Errors) est une mesure qui quantifie la dispersion des valeurs dans un nœud par rapport à la moyenne des valeurs dans ce nœud.\nFormule : Pour un nœud \\(t\\), contenant \\(N\\) observations avec des valeurs \\(y_i\\), la SSE est donnée par :\n\\[\n\\text{SSE}(t) = \\sum_{i=1}^{N} (y_i - \\hat{y})^2\n\\]\noù \\(\\hat{y}\\) est la moyenne des valeurs \\(y_i\\) dans le nœud \\(t\\).\nPropriété :\n\nSi toutes les valeurs de \\(y_i\\) dans un nœud sont proches de la moyenne \\(\\hat{y}\\), la SSE sera faible, indiquant une homogénéité élevée dans le nœud.\nEn revanche, une SSE élevée indique une grande variabilité dans les valeurs, donc un nœud impur.\n\nCritère de choix : La somme des erreurs quadratiques (SSE) est particulièrement sensible aux écarts élevés entre les valeurs observées et la moyenne prédite. En cherchant à minimiser la SSE, les modèles visent à former des nœuds dans lesquels les valeurs des observations sont aussi proches que possible de la moyenne locale.\n\n\n\n\nUne fois la mesure d’impureté définie, l’algorithme CART examine toutes les divisions binaires possibles de l’espace des caractéristiques. À chaque nœud, et pour chaque caractéristique, il cherche à identifier le seuil optimal, c’est-à-dire le seuil qui minimise le plus efficacement l’impureté des deux sous-ensembles générés. L’algorithme compare ensuite toutes les divisions potentielles (caractéristiques et seuils optimaux associés à chaque nœud) et sélectionne celle qui entraîne la réduction maximale de l’impureté.\nPrenons l’exemple d’une caractéristique continue, telle que la superficie d’une maison :\n\nSi l’algorithme teste la règle “Superficie &gt; 100 m²”, il calcule la fonction de perte pour les deux sous-ensembles générés par cette règle (“Oui” et “Non”).\nCe processus est répété pour différentes valeurs seuils afin de trouver la partition qui minimise le plus efficacement l’impureté au sein des sous-ensembles.\n\n\n\n\nL’algorithme CART poursuit le partitionnement de l’espace des caractéristiques en appliquant de manière récursive les mêmes étapes : identification de la caractéristique et du seuil optimal pour chaque nœud, puis sélection du partitionnement binaire qui maximise la réduction de l’impureté. Ce processus est répété jusqu’à ce qu’un critère d’arrêt soit atteint, par exemple :\n\nProfondeur maximale de l’arbre : Limiter le nombre de divisions successives pour éviter un arbre trop complexe.\nNombre minimum d’observations par feuille : Empêcher la création de feuilles contenant très peu d’observations, ce qui réduirait la capacité du modèle à généraliser.\nRéduction minimale de l’impureté à chaque étape\n\n\n\n\n\n\n\nUne fois l’arbre construit, la prédiction pour une nouvelle observation s’effectue en suivant les branches de l’arbre, en partant du nœud racine jusqu’à un nœud terminal (ou feuille). À chaque nœud interne, une décision est prise en fonction des valeurs des caractéristiques de l’observation, ce qui détermine la direction à suivre vers l’un des sous-ensembles. Ce cheminement se poursuit jusqu’à ce que l’observation atteigne une feuille, où la prédiction finale est effectuée.\n\nEn classification, la classe attribuée est celle majoritaire dans la feuille atteinte.\nEn régression, la valeur prédite est généralement la moyenne des valeurs cibles des observations dans la feuille.\n\n\n\n\nPour améliorer la performance de l’arbre, on peut ajuster les hyperparamètres tels que la profondeur maximale ou le nombre minimum d’observations dans une feuille. De plus, des techniques comme la prédiction avec arbres multiples (bagging, forêts aléatoires) permettent de surmonter les limites des arbres individuels, souvent sujets au surapprentissage.",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "La brique élémentaire: l'arbre de décision"
    ]
  },
  {
    "objectID": "chapters/chapter2/1-CART.html#avantages-et-limites-de-cette-approche",
    "href": "chapters/chapter2/1-CART.html#avantages-et-limites-de-cette-approche",
    "title": "1 La brique élémentaire: l’arbre de décision",
    "section": "",
    "text": "Interprétabilité : Les arbres de décision sont faciles à comprendre et à visualiser.\nSimplicité : Pas besoin de transformations complexes des données.\nFlexibilité : Ils peuvent gérer des caractéristiques numériques et catégorielles, ainsi que les valeurs manquantes.\nGestion des interactions : Modèles non paramétriques, pas d’hypothèses sur les lois par les variables. Ils capturent naturellement les interactions entre les caractéristiques.\n\n\n\n\n\nSurapprentissage : Les arbres trop profonds peuvent surapprendre les données d’entraînement.\nOptimisation locale : L’approche gloutonne peut conduire à des solutions sous-optimales globalement (optimum local).\nStabilité : De petits changements dans les données peuvent entraîner des changements significatifs dans la structure de l’arbre (manque de robustesse).",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "La brique élémentaire: l'arbre de décision"
    ]
  },
  {
    "objectID": "chapters/chapter2/3-random_forest.html",
    "href": "chapters/chapter2/3-random_forest.html",
    "title": "1 La forêt aléatoire",
    "section": "",
    "text": "La forêt aléatoire (random forests) est une méthode ensembliste puissante, largement utilisée pour les tâches de classification et de régression. Elle combine la simplicité des arbres de décision et l’échantillonnage des observations et des variables avec la puissance de l’agrégation pour améliorer les performances prédictives et réduire le risque de surapprentissage (overfitting).\n\n\n\nLa forêt aléatoire est une extension du bagging, présenté dans la section ?@sec-bagging-detail. Elle introduit un niveau supplémentaire de randomisation dans la construction des arbres, puisqu’à chaque nouvelle division (noeud), le critère de séparation est choisi en considérant uniquement un sous-ensemble de variables sélectionné aléatoirement. Cette randomisation supplémentaire réduit la corrélation entre les arbres, ce qui permet de diminuer la variance des prédiction du modèle agrégé.\nLes forêts aléatoires reposent sur quatre éléments essentiels:\n\nLes arbres CART: Les modèles élémentaires sont des arbres CART non élagués, c’est-à-dire autorisés à pousser jusqu’à l’atteinte d’un critère d’arrêt défini en amont.\nL’échantillonnage bootstrap: Chaque arbre est construit à partir d’un échantillon aléatoire du jeu de données d’entraînement tiré avec remise (ou parfois sans remise).\nLa sélection aléatoire de variables : Lors de la construction d’un arbre, à chaque nœud de celui-ci, un sous-ensemble aléatoire de variables est sélectionné. La meilleure division est ensuite choisie parmi ces caractéristiques aléatoires.\nL’agrégation des prédictions : Comme pour le bagging, les prédictions de tous les arbres sont combinées. On procède généralement à la moyenne (ou à la médiane) des prédictions dans le cas de la régression, et au vote majoritaire (ou à la moyenne des probabilités prédites pour chaque classe) dans le cas de la classification.\n\n\n\n\nL’entraînement d’une forêt aléatoire est très similaire à celui du bagging et se résume comme suit:\n\nLe nombre d’arbres à construire est défini a priori.\nPour chaque arbre, on effectue les étapes suivantes:\n\nGénérer un échantillon bootstrap de taille fixe à partir des données d’entraînement.\nConstruire récursivement un arbre de décision à partir de cet échantillon:\n\nÀ chaque nœud de l’arbre, un sous-ensemble de features est sélectionné aléatoirement.\nDéterminer quel couple (variable, valeur) définit la règle de décision qui divise la population du nœud en deux sous-groupes les plus homogènes possibles.\nCréer les deux nœuds-enfants à partir de cette règle de décision.\nArrêter la croissance de l’arbre selon des critères d’arrêt fixés a priori.\n\n\n\nPour construire la prédiction de la forêt aléatoire une fois celle-ci entraînée, on agrège les arbres selon une méthode qui dépend du problème modélisé:\n\nRégression: la prédiction finale est la moyenne des prédictions de tous les arbres.\nClassification: chaque arbre vote pour une classe, et la classe majoritaire est retenue.\n\nLes principaux hyper-paramètres des forêts aléatoires (détaillés dans la section ?@sec-guide-rf) sont les suivants: le nombre d’arbres, la méthode et le taux d’échantillonnage, le nombre (ou la proportion) de variables considérées à chaque nœud, le critère de division des nœuds (ou mesure d’hétérogénéité), et les critères d’arrêt (notamment la profondeur de l’arbre, le nombre minimal d’observations dans une feuille terminale, et le nombre minimal d’observations qu’un nœud doit comprendre pour être divisé en deux).\n\n\n\n\nLes propriétés théoriques des forêts aléatoires permettent de comprendre pourquoi (et dans quelles situations) elles sont particulièrement robustes et performantes.\n\n\nL’agrégation de plusieurs arbres permet de réduire la variance globale du modèle, ce qui améliore la stabilité des prédictions. Lorsque les estimateurs sont (faiblement) biaisés mais caractérisés par une variance élevée, l’agrégation permet d’obtenir un estimateur avec un biais similaire mais une variance réduite. La démonstration est identique à celle présentée dans la section ?@sec-bagging-detail.\n\n\n\nBien qu’elle s’avèrent très performantes en pratique, il n’est pas prouvé à ce stade que les forêts aléatoires convergent vers une solution optimale lorsque la taille de l’échantillon tend vers l’infini (Louppe (2014)). Plusieurs travaux théoriques ont toutefois fourni des preuves de convergence pour des versions simplifiées de l’algorithme (par exemple, Biau (2012)).\nPar ailleurs, une propriété importante des forêts aléatoires démontrée par Breiman (2001) est que leur erreur de généralisation, c’est-à-dire l’écart entre les prédictions du modèle et les résultats attendus sur des données jamais vues (donc hors de l’échantillon d’entraînement), diminue à mesure que le nombre d’arbres augmente et converge vers une valeur constante. Autrement dit, la forêt aléatoire ne souffre pas d’un surapprentissage croissant avec le nombre d’arbres. La conséquence pratique de ce résultat est qu’inclure un (trop) grand nombre d’arbres dans le modèle n’en dégrade pas la qualité, ce qui contribue à la rendre particulièrement robuste. En revanche, une forêt aléatoire peut souffrir de surapprentissage si ses autres hyperparamètres sont mal choisis (des arbres trop profonds par exemple).\n\n\n\nL’erreur de généralisation des forêts aléatoires est influencée par deux facteurs principaux :\n\nLa puissance prédictrice des arbres individuels : Les arbres doivent être suffisamment prédictifs pour contribuer positivement à l’ensemble, et idéalement sans biais.\nLa corrélation entre les arbres : Moins les arbres sont corrélés, plus la variance de l’ensemble est réduite, car leurs erreurs tendront à se compenser. Inversement, des arbres fortement corrélés auront tendance à faire des erreurs similaires, donc agréger un grand nombre d’arbres n’apportera pas grand chose.\n\nOn peut mettre en évidence ces deux facteurs dans le cas d’une forêt aléatoire utilisée pour une tâche de régression (où l’objectif est de minimiser l’erreur quadratique moyenne). Dans ce cas, la variance de la prédiction du modèle peut être décomposée de la façon suivante:\n\\[\n\\text{Var}(\\hat{f}(x)) = \\rho(x) \\sigma(x)^2 + \\frac{1 - \\rho(x)}{M} \\sigma(x)^2\n\\]\noù \\(\\rho(x)\\) est le coefficient de corrélation moyen entre les arbres individuels, \\(\\sigma(x)^2\\) est la variance d’un arbre individuel, \\(M\\) est le nombre d’arbres dans la forêt. Cette décomposition fait apparaître l’influence de la corrélation entre les arbres sur les performance de la forêt aléatoire:\n\nSi \\(\\rho(x)\\) est proche de 1 (forte corrélation entre les arbres) : la première composante \\(\\rho \\sigma^2\\) domine et la réduction de variance est moindre lorsque le nombre d’arbres augmente.\nSi \\(\\rho(x)\\) est proche de 0 (faible corrélation entre les arbres) : la seconde composante \\(\\frac{1 - \\rho}{M} \\sigma^2\\) et la variance est davantage réduite avec l’augmentation du nombre d’arbres \\(M\\).\n\nL’objectif de l’entraînement des forêts aléatoires est donc de minimiser la corrélation entre les arbres tout en maximisant leur capacité à prédire correctement, ce qui permet de réduire la variance globale sans augmenter excessivement le biais. La sélection aléatoires des caractéristiques (features) à chaque nœud joue un rôle majeur dans cet arbitrage entre puissance prédictive des arbres et corrélation entre arbres.\n\n\n\n\n\nLa forêt aléatoire présente une particularité intéressante et très utile en pratique: il est possible d’évaluer les performances d’une forêt aléatoire directement à partir des données d’entraînement, grâce à l’estimation de l’erreur Out-of-Bag (OOB). Cette technique repose sur le fait que chaque arbre est construit à partir d’un échantillon bootstrap, c’est-à-dire un échantillon tiré avec remise. Cela implique qu’une part conséquente des observations ne sont pas utilisées pour entraîner un arbre donné. Ces observations laissées de côté forment un échantillon dit out-of-bag, que l’on peut utiliser pour évaluer la performance de chaque arbre. On peut donc construire pour chaque observation du jeu d’entraînement une prédiction qui agrège uniquement les prédictions des arbres pour lesquels cette observation est out-of-bag; cette prédiction n’est pas affectée par le surapprentissage (puisque cette observation n’a jamais été utilisée pour entraîner ces arbres). De cette façon, il est possible d’évaluer correctement la performance de la forêt aléatoire en comparant ces prédictions avec la variable-cible à l’aide d’une métrique bien choisie.\nLa procédure d’estimation de l’erreur OOB se déroule comme ceci:\n\nEntraînement de la forêt aléatoire: la forêt aléatoire est entraînée sur les données d’entraînement selon la procédure détaillée ci-dessus.\nPrédiction out-of-bag : Pour chaque observation \\((x_i, y_i)\\) des données d’entraînement, on calcule la prédiction de tous les arbres pour lesquels elle fait partie de l’échantillon out-of-bag.\nAgrégation des prédictions : La prédiction finale est obtenue en agrégeant les prédictions selon la procédure standard détaillée ci-dessus (moyenne pour la régression, vote majoritaire pour la classification).\nCalcul de l’erreur OOB : L’erreur OOB est ensuite calculée en comparant les prédictions avec la variable-cible \\(y\\) sur toutes les observations, à l’aide d’une métrique (précision, rappel, AUC, erreur quadratique moyenne, score de Brier…).\n\nL’utilisation de l’erreur OOB présente de multiples avantages:\n\nApproximation de l’erreur de généralisation: L’erreur OOB est en général considérée comme une bonne approximation de l’erreur de généralisation, comparable à celle obtenue par une validation croisée.\nPas besoin de jeu de validation séparé : L’un des principaux avantages de l’erreur OOB est qu’elle ne nécessite pas de réserver une partie des données pour la validation. Cela est particulièrement utile lorsque la taille du jeu de données est limitée, car toutes les données peuvent être utilisées pour l’entraînement tout en ayant une estimation fiable de la performance. Ceci dit, il est malgré tout recommandé de conserver un ensemble de test si la taille des données le permet, car il arrive que l’erreur OOB sous\nGain de temps : Contrairement à la validation croisée qui requiert de réentraîner plusieurs fois le modèle pour un jeu donné d’hyperparamètres, l’erreur OOB ne nécessite qu’un seul entraînement du modèle. Cela induit un gain de temps appréciable lors de l’optimisation des hyperparamètres.\n\n\n\n\n\nLes forêts aléatoires sont des modèles d’apprentissage performants, mais leur complexité interne les rend difficiles à interpréter, ce qui leur vaut souvent le qualificatif de “boîtes noires”. Comprendre l’influence des variables explicatives sur les prédictions est crucial pour interpréter les résutlats et être en mesure d’extraire des connaissances.\nL’objectif des méthodes d’interprétabilité (ou d’importance des variables) est d’identifier les variables les plus influentes sur la variable cible, de comprendre les mécanismes prédictifs sous-jacents, et potentiellement d’extraire des règles de décision simples et transparentes. Plusieurs méthodes d’importance des variables existent, mais il est important de comprendre leurs forces et faiblesses.\n\n\n\nRéduction moyenne de l’impureté (Mean Decrease in Impurity - MDI) : Cette méthode quantifie l’importance d’une variable par la somme des réductions d’impureté qu’elle induit dans tous les arbres de la forêt. Plus spécifiquement, pour chaque variable, on s’intéresse à la moyenne des réductions d’impureté qu’elle a engendrées dans tous les nœuds de tous les arbres où elle est impliquée. Les variables présentant la réduction moyenne d’impureté la plus élevée sont considérées comme les prédicteurs les plus importants.\n\nLa MDI présente des biais importants. Elle est notamment sensible aux variables catégorielles avec de nombreuses modalités, qui peuvent apparaître artificiellement importantes (même si leur influence réelle est faible), ainsi qu’aux variables avec une échelle de valeurs plus étendues, qui obtiennent des scores plus élevés, indépendamment de leur importance réelle. Elle est également fortement biaisée en présence de variables explicatives corrélées, ce qui conduit à surestimer l’importance de variables redondantes. Les interactions entre variables ne sont pas non plus prises en compte de manière adéquate.\n\nImportance par permutation (Mean Decrease Accuracy - MDA) : Cette méthode évalue l’importance d’une variable en mesurant la diminution de précision du modèle après permutation aléatoire de ses valeurs. Plus spécifiquement, pour chaque variable, les performances du modèle sont comparées avant et après la permutation de ses valeurs. La différence moyenne de performance correspond à la MDA. L’idée est que si l’on permute aléatoirement les valeurs d’une variable (cassant ainsi sa relation avec la cible), une variable importante entraînera une hausse significative de l’erreur de généralisation.\n\nComme la MDI, la MDA présente des biais lorsque les variables sont corrélées. En particulier, la MDA peut surévaluer l’importance de variables qui sont corrélées à d’autres variables importantes, même si elles n’ont pas d’influence directe sur la cible (Bénard, Da Veiga, and Scornet (2022)).\nPlusieurs stratégies peuvent aider à réduire les biais d’interprétation :\n\nPrétraitement des variables: Standardisation des variables, regroupement des modalités rares des variables catégorielles, réduction de la cardinalité des variables catégorielles.\nAnalyse des corrélations: Identification et gestion des variables fortement corrélées, qui peuvent fausser les mesures d’importance.\nChoix de méthodes robustes: Privilégier les méthodes moins sensibles aux biais, comme les CIF ou la Sobol-MDA, et, le cas échéant, SHAFF pour les valeurs de Shapley. Ces méthodes sont présentées dans la sectio suivante.\n\n\n\n\nPour pallier les limites des méthodes traditionnelles, des approches plus sophistiquées ont été développées.\n\nValeurs de Shapley: Les valeurs de Shapley permettent de quantifier la contribution de chaque variable explicative à la variance expliquée de la variable cible, en tenant compte des interactions entre les variables. Elles attribuent à chaque variable une contribution marginale moyenne à la performance du modèle, en considérant toutes les combinaisons possibles de sous-ensembles de variables. Cependant, l’estimation des valeurs de Shapley est computationnellement coûteuse (complexité exponentielle avec le nombre de variables). Des méthodes approximatives existent, mais peuvent introduire des biais. L’algorithme SHAFF (Bénard et al. (2022)) propose une solution rapide et précise à ce problème, en tirant parti des propriétés des forêts aléatoires.\nConditional Inference Forests (CIF): Les CIF (Strobl et al. (2007)), implémentées dans le package party de R (cforest), corrigent certains biais de la MDI en utilisant des tests statistiques conditionnels pour sélectionner les variables et les seuils de coupure dans les arbres. Elles sont particulièrement robustes face aux variables hétérogènes et aux corrélations entre variables. Couplées à un échantillonnage sans remise, les CIF fournissent des mesures d’importance plus fiables.\nSobol-MDA: La Sobol-MDA combine l’idée de la MDA avec une approche basée sur les indices de Sobol, permettant de gérer efficacement les variables dépendantes. Au lieu de permuter les valeurs, elle projette la partition des arbres sur le sous-espace excluant la variable dont on souhaite mesurer l’importance, simulant ainsi son absence. Elle est plus efficace en calcul que les méthodes MDA classiques tout en fournissant une mesure d’importance cohérente, convergeant vers l’indice de Sobol total (la mesure appropriée pour identifier les covariables les plus influentes, même avec des dépendances) (Bénard, Da Veiga, and Scornet (2022)).",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "La forêt aléatoire"
    ]
  },
  {
    "objectID": "chapters/chapter2/3-random_forest.html#principe-de-la-forêt-aléatoire",
    "href": "chapters/chapter2/3-random_forest.html#principe-de-la-forêt-aléatoire",
    "title": "1 La forêt aléatoire",
    "section": "",
    "text": "La forêt aléatoire est une extension du bagging, présenté dans la section ?@sec-bagging-detail. Elle introduit un niveau supplémentaire de randomisation dans la construction des arbres, puisqu’à chaque nouvelle division (noeud), le critère de séparation est choisi en considérant uniquement un sous-ensemble de variables sélectionné aléatoirement. Cette randomisation supplémentaire réduit la corrélation entre les arbres, ce qui permet de diminuer la variance des prédiction du modèle agrégé.\nLes forêts aléatoires reposent sur quatre éléments essentiels:\n\nLes arbres CART: Les modèles élémentaires sont des arbres CART non élagués, c’est-à-dire autorisés à pousser jusqu’à l’atteinte d’un critère d’arrêt défini en amont.\nL’échantillonnage bootstrap: Chaque arbre est construit à partir d’un échantillon aléatoire du jeu de données d’entraînement tiré avec remise (ou parfois sans remise).\nLa sélection aléatoire de variables : Lors de la construction d’un arbre, à chaque nœud de celui-ci, un sous-ensemble aléatoire de variables est sélectionné. La meilleure division est ensuite choisie parmi ces caractéristiques aléatoires.\nL’agrégation des prédictions : Comme pour le bagging, les prédictions de tous les arbres sont combinées. On procède généralement à la moyenne (ou à la médiane) des prédictions dans le cas de la régression, et au vote majoritaire (ou à la moyenne des probabilités prédites pour chaque classe) dans le cas de la classification.",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "La forêt aléatoire"
    ]
  },
  {
    "objectID": "chapters/chapter2/3-random_forest.html#comment-construit-on-une-forêt-aléatoire",
    "href": "chapters/chapter2/3-random_forest.html#comment-construit-on-une-forêt-aléatoire",
    "title": "1 La forêt aléatoire",
    "section": "",
    "text": "L’entraînement d’une forêt aléatoire est très similaire à celui du bagging et se résume comme suit:\n\nLe nombre d’arbres à construire est défini a priori.\nPour chaque arbre, on effectue les étapes suivantes:\n\nGénérer un échantillon bootstrap de taille fixe à partir des données d’entraînement.\nConstruire récursivement un arbre de décision à partir de cet échantillon:\n\nÀ chaque nœud de l’arbre, un sous-ensemble de features est sélectionné aléatoirement.\nDéterminer quel couple (variable, valeur) définit la règle de décision qui divise la population du nœud en deux sous-groupes les plus homogènes possibles.\nCréer les deux nœuds-enfants à partir de cette règle de décision.\nArrêter la croissance de l’arbre selon des critères d’arrêt fixés a priori.\n\n\n\nPour construire la prédiction de la forêt aléatoire une fois celle-ci entraînée, on agrège les arbres selon une méthode qui dépend du problème modélisé:\n\nRégression: la prédiction finale est la moyenne des prédictions de tous les arbres.\nClassification: chaque arbre vote pour une classe, et la classe majoritaire est retenue.\n\nLes principaux hyper-paramètres des forêts aléatoires (détaillés dans la section ?@sec-guide-rf) sont les suivants: le nombre d’arbres, la méthode et le taux d’échantillonnage, le nombre (ou la proportion) de variables considérées à chaque nœud, le critère de division des nœuds (ou mesure d’hétérogénéité), et les critères d’arrêt (notamment la profondeur de l’arbre, le nombre minimal d’observations dans une feuille terminale, et le nombre minimal d’observations qu’un nœud doit comprendre pour être divisé en deux).",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "La forêt aléatoire"
    ]
  },
  {
    "objectID": "chapters/chapter2/3-random_forest.html#pourquoi-les-forêts-aléatoires-sont-elles-performantes",
    "href": "chapters/chapter2/3-random_forest.html#pourquoi-les-forêts-aléatoires-sont-elles-performantes",
    "title": "1 La forêt aléatoire",
    "section": "",
    "text": "Les propriétés théoriques des forêts aléatoires permettent de comprendre pourquoi (et dans quelles situations) elles sont particulièrement robustes et performantes.\n\n\nL’agrégation de plusieurs arbres permet de réduire la variance globale du modèle, ce qui améliore la stabilité des prédictions. Lorsque les estimateurs sont (faiblement) biaisés mais caractérisés par une variance élevée, l’agrégation permet d’obtenir un estimateur avec un biais similaire mais une variance réduite. La démonstration est identique à celle présentée dans la section ?@sec-bagging-detail.\n\n\n\nBien qu’elle s’avèrent très performantes en pratique, il n’est pas prouvé à ce stade que les forêts aléatoires convergent vers une solution optimale lorsque la taille de l’échantillon tend vers l’infini (Louppe (2014)). Plusieurs travaux théoriques ont toutefois fourni des preuves de convergence pour des versions simplifiées de l’algorithme (par exemple, Biau (2012)).\nPar ailleurs, une propriété importante des forêts aléatoires démontrée par Breiman (2001) est que leur erreur de généralisation, c’est-à-dire l’écart entre les prédictions du modèle et les résultats attendus sur des données jamais vues (donc hors de l’échantillon d’entraînement), diminue à mesure que le nombre d’arbres augmente et converge vers une valeur constante. Autrement dit, la forêt aléatoire ne souffre pas d’un surapprentissage croissant avec le nombre d’arbres. La conséquence pratique de ce résultat est qu’inclure un (trop) grand nombre d’arbres dans le modèle n’en dégrade pas la qualité, ce qui contribue à la rendre particulièrement robuste. En revanche, une forêt aléatoire peut souffrir de surapprentissage si ses autres hyperparamètres sont mal choisis (des arbres trop profonds par exemple).\n\n\n\nL’erreur de généralisation des forêts aléatoires est influencée par deux facteurs principaux :\n\nLa puissance prédictrice des arbres individuels : Les arbres doivent être suffisamment prédictifs pour contribuer positivement à l’ensemble, et idéalement sans biais.\nLa corrélation entre les arbres : Moins les arbres sont corrélés, plus la variance de l’ensemble est réduite, car leurs erreurs tendront à se compenser. Inversement, des arbres fortement corrélés auront tendance à faire des erreurs similaires, donc agréger un grand nombre d’arbres n’apportera pas grand chose.\n\nOn peut mettre en évidence ces deux facteurs dans le cas d’une forêt aléatoire utilisée pour une tâche de régression (où l’objectif est de minimiser l’erreur quadratique moyenne). Dans ce cas, la variance de la prédiction du modèle peut être décomposée de la façon suivante:\n\\[\n\\text{Var}(\\hat{f}(x)) = \\rho(x) \\sigma(x)^2 + \\frac{1 - \\rho(x)}{M} \\sigma(x)^2\n\\]\noù \\(\\rho(x)\\) est le coefficient de corrélation moyen entre les arbres individuels, \\(\\sigma(x)^2\\) est la variance d’un arbre individuel, \\(M\\) est le nombre d’arbres dans la forêt. Cette décomposition fait apparaître l’influence de la corrélation entre les arbres sur les performance de la forêt aléatoire:\n\nSi \\(\\rho(x)\\) est proche de 1 (forte corrélation entre les arbres) : la première composante \\(\\rho \\sigma^2\\) domine et la réduction de variance est moindre lorsque le nombre d’arbres augmente.\nSi \\(\\rho(x)\\) est proche de 0 (faible corrélation entre les arbres) : la seconde composante \\(\\frac{1 - \\rho}{M} \\sigma^2\\) et la variance est davantage réduite avec l’augmentation du nombre d’arbres \\(M\\).\n\nL’objectif de l’entraînement des forêts aléatoires est donc de minimiser la corrélation entre les arbres tout en maximisant leur capacité à prédire correctement, ce qui permet de réduire la variance globale sans augmenter excessivement le biais. La sélection aléatoires des caractéristiques (features) à chaque nœud joue un rôle majeur dans cet arbitrage entre puissance prédictive des arbres et corrélation entre arbres.",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "La forêt aléatoire"
    ]
  },
  {
    "objectID": "chapters/chapter2/3-random_forest.html#sec-rf-oob",
    "href": "chapters/chapter2/3-random_forest.html#sec-rf-oob",
    "title": "1 La forêt aléatoire",
    "section": "",
    "text": "La forêt aléatoire présente une particularité intéressante et très utile en pratique: il est possible d’évaluer les performances d’une forêt aléatoire directement à partir des données d’entraînement, grâce à l’estimation de l’erreur Out-of-Bag (OOB). Cette technique repose sur le fait que chaque arbre est construit à partir d’un échantillon bootstrap, c’est-à-dire un échantillon tiré avec remise. Cela implique qu’une part conséquente des observations ne sont pas utilisées pour entraîner un arbre donné. Ces observations laissées de côté forment un échantillon dit out-of-bag, que l’on peut utiliser pour évaluer la performance de chaque arbre. On peut donc construire pour chaque observation du jeu d’entraînement une prédiction qui agrège uniquement les prédictions des arbres pour lesquels cette observation est out-of-bag; cette prédiction n’est pas affectée par le surapprentissage (puisque cette observation n’a jamais été utilisée pour entraîner ces arbres). De cette façon, il est possible d’évaluer correctement la performance de la forêt aléatoire en comparant ces prédictions avec la variable-cible à l’aide d’une métrique bien choisie.\nLa procédure d’estimation de l’erreur OOB se déroule comme ceci:\n\nEntraînement de la forêt aléatoire: la forêt aléatoire est entraînée sur les données d’entraînement selon la procédure détaillée ci-dessus.\nPrédiction out-of-bag : Pour chaque observation \\((x_i, y_i)\\) des données d’entraînement, on calcule la prédiction de tous les arbres pour lesquels elle fait partie de l’échantillon out-of-bag.\nAgrégation des prédictions : La prédiction finale est obtenue en agrégeant les prédictions selon la procédure standard détaillée ci-dessus (moyenne pour la régression, vote majoritaire pour la classification).\nCalcul de l’erreur OOB : L’erreur OOB est ensuite calculée en comparant les prédictions avec la variable-cible \\(y\\) sur toutes les observations, à l’aide d’une métrique (précision, rappel, AUC, erreur quadratique moyenne, score de Brier…).\n\nL’utilisation de l’erreur OOB présente de multiples avantages:\n\nApproximation de l’erreur de généralisation: L’erreur OOB est en général considérée comme une bonne approximation de l’erreur de généralisation, comparable à celle obtenue par une validation croisée.\nPas besoin de jeu de validation séparé : L’un des principaux avantages de l’erreur OOB est qu’elle ne nécessite pas de réserver une partie des données pour la validation. Cela est particulièrement utile lorsque la taille du jeu de données est limitée, car toutes les données peuvent être utilisées pour l’entraînement tout en ayant une estimation fiable de la performance. Ceci dit, il est malgré tout recommandé de conserver un ensemble de test si la taille des données le permet, car il arrive que l’erreur OOB sous\nGain de temps : Contrairement à la validation croisée qui requiert de réentraîner plusieurs fois le modèle pour un jeu donné d’hyperparamètres, l’erreur OOB ne nécessite qu’un seul entraînement du modèle. Cela induit un gain de temps appréciable lors de l’optimisation des hyperparamètres.",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "La forêt aléatoire"
    ]
  },
  {
    "objectID": "chapters/chapter2/3-random_forest.html#interprétation-et-importance-des-variables",
    "href": "chapters/chapter2/3-random_forest.html#interprétation-et-importance-des-variables",
    "title": "1 La forêt aléatoire",
    "section": "",
    "text": "Les forêts aléatoires sont des modèles d’apprentissage performants, mais leur complexité interne les rend difficiles à interpréter, ce qui leur vaut souvent le qualificatif de “boîtes noires”. Comprendre l’influence des variables explicatives sur les prédictions est crucial pour interpréter les résutlats et être en mesure d’extraire des connaissances.\nL’objectif des méthodes d’interprétabilité (ou d’importance des variables) est d’identifier les variables les plus influentes sur la variable cible, de comprendre les mécanismes prédictifs sous-jacents, et potentiellement d’extraire des règles de décision simples et transparentes. Plusieurs méthodes d’importance des variables existent, mais il est important de comprendre leurs forces et faiblesses.\n\n\n\nRéduction moyenne de l’impureté (Mean Decrease in Impurity - MDI) : Cette méthode quantifie l’importance d’une variable par la somme des réductions d’impureté qu’elle induit dans tous les arbres de la forêt. Plus spécifiquement, pour chaque variable, on s’intéresse à la moyenne des réductions d’impureté qu’elle a engendrées dans tous les nœuds de tous les arbres où elle est impliquée. Les variables présentant la réduction moyenne d’impureté la plus élevée sont considérées comme les prédicteurs les plus importants.\n\nLa MDI présente des biais importants. Elle est notamment sensible aux variables catégorielles avec de nombreuses modalités, qui peuvent apparaître artificiellement importantes (même si leur influence réelle est faible), ainsi qu’aux variables avec une échelle de valeurs plus étendues, qui obtiennent des scores plus élevés, indépendamment de leur importance réelle. Elle est également fortement biaisée en présence de variables explicatives corrélées, ce qui conduit à surestimer l’importance de variables redondantes. Les interactions entre variables ne sont pas non plus prises en compte de manière adéquate.\n\nImportance par permutation (Mean Decrease Accuracy - MDA) : Cette méthode évalue l’importance d’une variable en mesurant la diminution de précision du modèle après permutation aléatoire de ses valeurs. Plus spécifiquement, pour chaque variable, les performances du modèle sont comparées avant et après la permutation de ses valeurs. La différence moyenne de performance correspond à la MDA. L’idée est que si l’on permute aléatoirement les valeurs d’une variable (cassant ainsi sa relation avec la cible), une variable importante entraînera une hausse significative de l’erreur de généralisation.\n\nComme la MDI, la MDA présente des biais lorsque les variables sont corrélées. En particulier, la MDA peut surévaluer l’importance de variables qui sont corrélées à d’autres variables importantes, même si elles n’ont pas d’influence directe sur la cible (Bénard, Da Veiga, and Scornet (2022)).\nPlusieurs stratégies peuvent aider à réduire les biais d’interprétation :\n\nPrétraitement des variables: Standardisation des variables, regroupement des modalités rares des variables catégorielles, réduction de la cardinalité des variables catégorielles.\nAnalyse des corrélations: Identification et gestion des variables fortement corrélées, qui peuvent fausser les mesures d’importance.\nChoix de méthodes robustes: Privilégier les méthodes moins sensibles aux biais, comme les CIF ou la Sobol-MDA, et, le cas échéant, SHAFF pour les valeurs de Shapley. Ces méthodes sont présentées dans la sectio suivante.\n\n\n\n\nPour pallier les limites des méthodes traditionnelles, des approches plus sophistiquées ont été développées.\n\nValeurs de Shapley: Les valeurs de Shapley permettent de quantifier la contribution de chaque variable explicative à la variance expliquée de la variable cible, en tenant compte des interactions entre les variables. Elles attribuent à chaque variable une contribution marginale moyenne à la performance du modèle, en considérant toutes les combinaisons possibles de sous-ensembles de variables. Cependant, l’estimation des valeurs de Shapley est computationnellement coûteuse (complexité exponentielle avec le nombre de variables). Des méthodes approximatives existent, mais peuvent introduire des biais. L’algorithme SHAFF (Bénard et al. (2022)) propose une solution rapide et précise à ce problème, en tirant parti des propriétés des forêts aléatoires.\nConditional Inference Forests (CIF): Les CIF (Strobl et al. (2007)), implémentées dans le package party de R (cforest), corrigent certains biais de la MDI en utilisant des tests statistiques conditionnels pour sélectionner les variables et les seuils de coupure dans les arbres. Elles sont particulièrement robustes face aux variables hétérogènes et aux corrélations entre variables. Couplées à un échantillonnage sans remise, les CIF fournissent des mesures d’importance plus fiables.\nSobol-MDA: La Sobol-MDA combine l’idée de la MDA avec une approche basée sur les indices de Sobol, permettant de gérer efficacement les variables dépendantes. Au lieu de permuter les valeurs, elle projette la partition des arbres sur le sous-espace excluant la variable dont on souhaite mesurer l’importance, simulant ainsi son absence. Elle est plus efficace en calcul que les méthodes MDA classiques tout en fournissant une mesure d’importance cohérente, convergeant vers l’indice de Sobol total (la mesure appropriée pour identifier les covariables les plus influentes, même avec des dépendances) (Bénard, Da Veiga, and Scornet (2022)).",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "La forêt aléatoire"
    ]
  },
  {
    "objectID": "chapters/chapter2/ajouts_boosting.html",
    "href": "chapters/chapter2/ajouts_boosting.html",
    "title": "1 Le Shrinkage en Apprentissage Automatique",
    "section": "",
    "text": "Choses importantes à mettre en avant:\n\nLe boosting est fondamentalement différent des forêts aléatoires. See ESL, chapitre 10.\nLa mécanique du gradient boosting est entièrement indépendante de la nature du problème considéré (régression, classification, classement…) et de la fonction de perte choisie1. L’approche de gradient boosting est donc particulièrement flexible et peut être adaptée à des problèmes variés.\n\n\nA la différence des forêts aléatoires, l’approche de gradient boosting ne contient en elle-même aucune limite au surapprentissage, bien au contraire: le gradient boosting est un algorithme conçu pour approximer le plus précisément possible la relation entre \\(X\\) et \\(y\\) telle qu’elle apparaît dans les données d’entraînement, qu’il s’agisse d’un signal pertinent ou d’un bruit statistique, ce qui le rend particulièrement vulnérable au surapprentissage. Par conséquent, la lutte contre l’overfitting est un élément essentiel de l’usage des algorithmes de gradient boosting.\nLes termes de régularisation sont directement intégrées à la mécanique du gradient boosting.\n\nInterprétation intuitive: \\(\\gamma\\) est le gain minimal nécessaire pour diviser un noeud.\n\nComment on interprète le gradient et la hessienne: cas avec une fonction de perte quadratique.",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "Sujets avancés"
    ]
  },
  {
    "objectID": "chapters/chapter2/ajouts_boosting.html#explication-du-shrinkage-dans-le-contexte-du-gradient-boosting",
    "href": "chapters/chapter2/ajouts_boosting.html#explication-du-shrinkage-dans-le-contexte-du-gradient-boosting",
    "title": "1 Le Shrinkage en Apprentissage Automatique",
    "section": "1.1 Explication du Shrinkage dans le Contexte du Gradient Boosting",
    "text": "1.1 Explication du Shrinkage dans le Contexte du Gradient Boosting\n\n1.1.1 1. But\nDans les méthodes de boosting, à chaque itération, un nouveau modèle (souvent un arbre de décision) est ajouté pour corriger les erreurs du modèle précédent. Le shrinkage permet de réduire l’impact de chaque nouvel arbre ajouté, ce qui peut aider à éviter le sur-apprentissage (overfitting) en ralentissant l’ajustement du modèle aux données d’entraînement.\n\n\n1.1.2 2. Comment ça fonctionne\n\nAprès chaque itération, au lieu d’ajouter directement le nouvel arbre au modèle existant, on applique un facteur de réduction ( ) (souvent appelé taux d’apprentissage ou learning rate) qui détermine l’ampleur de l’ajustement du modèle.\nPar exemple, si l’arbre nouvellement appris améliore la prédiction de ( f(x) ), au lieu de l’ajouter directement à la prédiction, on ajoute ( f(x) ), où ( ) est un petit nombre (généralement entre 0 et 1).\n\n\n\n1.1.3 3. Avantages\n\nRéduction du sur-apprentissage : En limitant l’impact de chaque arbre, le shrinkage aide à éviter que le modèle s’ajuste trop précisément aux bruits ou aux fluctuations des données d’entraînement.\nAmélioration de la généralisation : Bien qu’il ralentisse l’entraînement, le shrinkage améliore souvent les performances du modèle sur des données de test (généralisation).\n\n\n\n1.1.4 4. Inconvénients\n\nTemps d’entraînement plus long : En raison de la réduction de l’impact de chaque arbre, il peut être nécessaire de former plus d’arbres pour atteindre une performance similaire à celle d’un modèle sans shrinkage.",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "Sujets avancés"
    ]
  },
  {
    "objectID": "chapters/chapter2/ajouts_boosting.html#conclusion",
    "href": "chapters/chapter2/ajouts_boosting.html#conclusion",
    "title": "1 Le Shrinkage en Apprentissage Automatique",
    "section": "1.2 Conclusion",
    "text": "1.2 Conclusion\nEn résumé, le shrinkage dans le contexte du gradient boosting consiste à appliquer un facteur de réduction à chaque nouvel arbre pour modérer ses effets sur le modèle final, afin d’améliorer la stabilité et la capacité de généralisation du modèle.",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "Sujets avancés"
    ]
  },
  {
    "objectID": "chapters/chapter2/ajouts_boosting.html#footnotes",
    "href": "chapters/chapter2/ajouts_boosting.html#footnotes",
    "title": "1 Le Shrinkage en Apprentissage Automatique",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nla fonction de perte doit uniquement vérifier quelques conditions mathématiques peu contraignantes en pratique.↩︎\nIl est bien sûr possible de n’encoder que les modalités les plus fréquentes, et de regrouper toutes les autres dans une seule variable binaire.↩︎\nLe target encoding utilisé par CatBoost est présenté en détail dans Prokhorenkova et al. (2018) et sur ce billet de blog.↩︎\nVoir cet exemple dans la documentation de scikit-learn.↩︎\nIl s’agit de l’équation donnant le poids optimal d’une feuille terminale, avec \\(\\lambda = 0\\).↩︎\nCet exemple s’appuie sur la documentation de scikit-learn.↩︎",
    "crumbs": [
      "Présentation formelle des algorithmes",
      "Sujets avancés"
    ]
  },
  {
    "objectID": "chapters/chapter3/1-preparation_donnees.html",
    "href": "chapters/chapter3/1-preparation_donnees.html",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "",
    "text": "Valeurs manquantes : Les forêts aléatoires peuvent gérer les données manquantes, mais une imputation préalable peut améliorer les performances.\nVariables catégorielles : Utiliser un encodage adapté (one-hot encoding, ordinal encoding) en fonction de la nature des données. Convertir en variables continues dès que c’est possible.\nÉchelle des Variables : Pas nécessaire de normaliser, les arbres sont invariants aux transformations monotones.\n\n\n\n\n\n\n\nPas indispensable pour RF, mais souhaitable. Indispensable pour GB.",
    "crumbs": [
      "Comment bien utiliser les algorithmes?",
      "Préparation des données"
    ]
  },
  {
    "objectID": "chapters/chapter3/1-preparation_donnees.html#préparation-des-données",
    "href": "chapters/chapter3/1-preparation_donnees.html#préparation-des-données",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "",
    "text": "Valeurs manquantes : Les forêts aléatoires peuvent gérer les données manquantes, mais une imputation préalable peut améliorer les performances.\nVariables catégorielles : Utiliser un encodage adapté (one-hot encoding, ordinal encoding) en fonction de la nature des données. Convertir en variables continues dès que c’est possible.\nÉchelle des Variables : Pas nécessaire de normaliser, les arbres sont invariants aux transformations monotones.\n\n\n\n\n\n\n\nPas indispensable pour RF, mais souhaitable. Indispensable pour GB.",
    "crumbs": [
      "Comment bien utiliser les algorithmes?",
      "Préparation des données"
    ]
  },
  {
    "objectID": "chapters/chapter3/1-preparation_donnees.html#evaluation-des-performances-du-modèle-et-optimisation-des-hyper-paramètres",
    "href": "chapters/chapter3/1-preparation_donnees.html#evaluation-des-performances-du-modèle-et-optimisation-des-hyper-paramètres",
    "title": "Introduction aux méthodes ensemblistes",
    "section": "2 Evaluation des performances du modèle et optimisation des hyper-paramètres",
    "text": "2 Evaluation des performances du modèle et optimisation des hyper-paramètres\n\n2.1 Estimation de l’erreur par validation croisée\nLa validation croisée est une méthode d’évaluation couramment utilisée en apprentissage automatique pour estimer la capacité d’un modèle à généraliser les prédictions à de nouvelles données. Bien que l’évaluation par l’erreur Out-of-Bag (OOB) soit généralement suffisante pour les forêts aléatoires, la validation croisée permet d’obtenir une évaluation plus robuste, car moins sensible à l’échantillon d’entraînement, notamment sur des jeux de données de petite taille.\nConcrètement, le jeu de donné est divisé en \\(k\\) sous-ensembles, un modèle est entraîné sur \\(k-1\\) sous-ensembles et testé sur le sous-ensemble restant. L’opération est répétée \\(k\\) fois de manière à ce que chaque observation apparaisse au moins une fois dans l’échantillon test. L’erreur est ensuite moyennée sur l’ensemble des échantillons test.\nProcédure de validation croisée:\nLa validation croisée la plus courante est la validation croisée en k sous-échantillons (k-fold cross-validation):\n\nDivision des données : Le jeu de données est divisé en k sous-échantillons égaux, appelés folds. Typiquement, k est choisi entre 5 et 10, mais il peut être ajusté en fonction de la taille des données.\nEntraînement et test : Le modèle est entraîné sur k - 1 sous-échantillons et testé sur le sous-échantillon restant. Cette opération est répétée k fois, chaque sous-échantillon jouant à tour de rôle le rôle de jeu de test.\nCalcul de la performance : Les k performances obtenues (par exemple, l’erreur quadratique moyenne pour une régression, ou l’accuracy (exactitude) pour une classification) sont moyennées pour obtenir une estimation finale de la performance du modèle.\n\nAvantages de la validation croisée:\n\nUtilisation optimale des données : En particulier lorsque les données sont limitées, la validation croisée maximise l’utilisation de l’ensemble des données en permettant à chaque échantillon de contribuer à la fois à l’entraînement et au test.\nRéduction de la variance : En utilisant plusieurs divisions des données, on obtient une estimation de la performance moins sensible aux particularités d’une seule division.\n\nBien que plus coûteuse en termes de calcul, la validation croisée est souvent préférée lorsque les données sont limitées ou lorsque l’on souhaite évaluer différents modèles ou hyperparamètres avec précision.\nLeave-One-Out Cross-Validation (LOOCV) : Il s’agit d’un cas particulier où le nombre de sous-échantillons est égal à la taille du jeu de données. En d’autres termes, chaque échantillon est utilisé une fois comme jeu de test, et tous les autres échantillons pour l’entraînement. LOOCV fournit une estimation très précise de la performance, mais est très coûteuse en temps de calcul, surtout pour de grands jeux de données.\n\n\n2.2 Choix des hyper-paramètres du modèle\nL’estimation Out-of-Bag (OOB) et la validation croisée sont deux méthodes clés pour optimiser les hyper-paramètres d’une forêt aléatoire. Les deux approches permettent de comparer les performances obtenues pour différentes combinaisons d’hyper-paramètres et de sélectionner celles qui maximisent les performances prédictives, l’OOB étant souvent plus rapide et moins coûteuse, tandis que la validation croisée est plus fiable dans des situations où le surapprentissage est un risque important (Probst, Wright, and Boulesteix (2019)).\nIl convient de définir une stratégie d’optimisation des hyperparamètres pour ne pas perdre de temps à tester trop de jeux d’hyperparamètres. Plusieurs stratégies existent pour y parvenir, les principales sont exposées dans la section ?@sec-guide-rf. Les implémentations des forêts aléatoires disponibles en R et en Python permettent d’optimiser aisément les principaux hyper-paramètres des forêts aléatoires.\n\n2.2.1 Méthodes de recherche exhaustives\n\nRecherche sur grille (Grid Search): Cette approche simple explore toutes les combinaisons possibles d’hyperparamètres définis sur une grille. Les paramètres continus doivent être discrétisés au préalable. La méthode est exhaustive mais coûteuse en calcul, surtout pour un grand nombre d’hyperparamètres.\nRecherche aléatoire (Random Search): Plus efficace que la recherche sur grille, cette méthode échantillonne aléatoirement les valeurs des hyperparamètres dans un espace défini. Bergstra et Bengio (2012) ont démontré sa supériorité pour les réseaux neuronaux, et elle est également pertinente pour les forêts aléatoires. La distribution d’échantillonnage est souvent uniforme.\n\n\n\n2.2.2 Optimisation séquentielle/itérative basée sur un modèle (SMBO)\nLa méthode SMBO (Sequential model-based optimization) est une approche plus efficace que les précédentes car elle s’appuie sur les résultats des évaluations déjà effectuées pour guider la recherche des prochains hyper-paramètres à tester (Probst, Wright, and Boulesteix (2019)).\nVoici les étapes clés de cette méthode:\n\nDéfinition du problème: On spécifie une mesure d’évaluation (ex: AUC pour la classification, MSE pour la régression), une stratégie d’évaluation (ex: validation croisée k-fold), et l’espace des hyperparamètres à explorer.\nInitialisation: échantillonner aléatoirement des points dans l’espace des hyperparamètres et évaluer leurs performances.\nBoucle itérative :\n\nConstruction d’un modèle de substitution (surrogate model): un modèle de régression (ex: krigeage ou une forêt aléatoire) est ajusté aux données déjà observées. Ce modèle prédit la performance en fonction des hyperparamètres.\nSélection d’un nouvel hyperparamètre: un critère basé sur le modèle de substitution sélectionne le prochain ensemble d’hyperparamètres à évaluer. Ce critère vise à explorer des régions prometteuses de l’espace des hyperparamètres qui n’ont pas encore été suffisamment explorées.\nÉvaluer les points proposés et les ajouter à l’ensemble déjà exploré: la performance du nouvel ensemble d’hyperparamètres est évaluée et ajoutée à l’ensemble des données d’apprentissage du modèle de substitution afin d’orienter les recherches vers de nouveaux hyper-paramètres prometteurs.",
    "crumbs": [
      "Comment bien utiliser les algorithmes?",
      "Préparation des données"
    ]
  },
  {
    "objectID": "chapters/chapter3/3-guide_usage_GB.html",
    "href": "chapters/chapter3/3-guide_usage_GB.html",
    "title": "1 Guide d’usage du gradient boosting",
    "section": "",
    "text": "Ce guide propose des recommandations sur l’usage des algorithmes de gradient boosting disponibles dans la littérature, notamment Bentéjac, Csörgő, and Martínez-Muñoz (2021).\nContrairement aux forêts aléatoires, la littérature méthodologique sur l’usages des algorithmes de gradient boosting est assez limitée et relativement peu conclusive.\n. Ce guide comporte un certain nombre de choix méthodologiques forts, comme les implémentations recommandées ou la procédure d’entraînement proposée, et d’autres choix pertinents sont évidemment possibles. C’est pourquoi les recommandations de ce guide doivent être considérées comme un point de départ raisonnable, pas comme un ensemble de règles devant être respectées à tout prix.\n\n\nIl existe quatre implémentations du gradient boosting: XGBoost, LightGBM, CatBoost et scikit-learn. Elles sont toutes des variantes optimisées de l’algorithme de Friedman (2001) et ne diffèrent que sur des points mineurs. De multiples publications les ont comparées, à la fois en matière de pouvoir prédictif et de rapidité d’entraînement (voir notamment Bentéjac, Csörgő, and Martínez-Muñoz (2021) et Alshari, Saleh, and Odabaş (2021)). Cette littérature a abouti à trois conclusions. Premièrement, les différentes implémentations présentent des performances très proches (le classement exact variant d’une publication à l’autre). Deuxièmement, bien optimiser les hyperparamètres est nettement plus important que le choix de l’implémentation. Troisièmement, le temps d’entraînement varie beaucoup d’une implémentation à l’autre, et LightGBM est sensiblement plus rapide que les autres. Dans la mesure où l’optimisation des hyperparamètres est une étape à la fois essentielle et intense en calcul, l’efficacité computationnelle apparaît comme un critère majeur de choix de l’implémentation. C’est pourquoi le présent document décrit et recommande l’usage de LightGBM. Ceci étant, les trois autres implémentations peuvent également être utilisées, notamment si les données sont de taille limitée.\nPar ailleurs, chacune de ces implémentations propose une interface de haut niveau compatible avec scikit-learn. Il est vivement recommandé d’utiliser cette interface car elle minimise les risques d’erreur, facilite la construction de modèles reproductibles et permet d’utiliser l’ensemble des outils proposés par scikit-learn.\n\n\n\nCette section décrit en détail les principaux hyperparamètres des algorithmes de gradient boosting listés dans le tableau 1. Les noms des hyperparamètres sont ceux utilisés dans LightGBM. Les hyperparamètres portent généralement le même nom dans les autres implémentations; si ce n’est pas le cas, il est facile de s’y retrouver en lisant attentivement la documentation.\n\n\n\nTable 1: Les principaux hyperparamètres de LightGBM\n\n\n\n\n\n\n\n\n\n\nHyperparamètre\nDescription\nValeur par défaut\n\n\n\n\nobjective\nFonction de perte utilisée\nVariable\n\n\nn_estimators ou num_trees\nNombre d’arbres\n100\n\n\nlearning_rate ou eta\nTaux d’apprentissage\n0.1\n\n\nmax_depth\nProfondeur maximale des arbres\n-1 (pas de limite)\n\n\nnum_leaves\nNombre de feuilles terminales des arbres\n31\n\n\nmin_child_samples\nNombre minimal d’observations qu’une feuille terminale doit contenir\n20\n\n\nmin_child_weight\nPoids minimal qu’une feuille terminale doit contenir\n0.001\n\n\nlambda ou lambda_l2\nPénalisation quadratique sur la valeur des feuilles terminales\n0\n\n\nreg_alpha ou lambda_l1\nPénalisation absolue (L1) sur la valeur des feuilles terminales\n0\n\n\nmin_split_gain\nGain minimal nécessaire pour diviser un noeud\n0\n\n\nbagging_fraction\nTaux d’échantillonnage des données d’entraînement\n1\n\n\nfeature_fraction\nTaux d’échantillonnage des colonnes par arbre\n1\n\n\nfeature_fraction_bynode\nTaux d’échantillonnage des colonnes par noeud\n1\n\n\nmax_bin\nNombre de bins utilisés pour discrétiser les variables continues\n255\n\n\nmax_cat_to_onehot\nNombre de modalités en-deça duquel LightGBM utilise le one-hot-encoding\n4\n\n\nmax_cat_threshold\nNombre maximal de splits considérés  dans le traitement des variables catégorielles\n32\n\n\nsample_weight\nPondération des observations dans les données d’entraînement\n1\n\n\nscale_pos_weight\nPoids des observations de la classe positive (classification binaire uniquement)\nAucun\n\n\nclass_weight\nPoids des observations de chaque classe (classification multiclasse uniquement)\nAucun\n\n\n\n\n\n\n\n\n\n\n\n\nAttention aux alias!\n\n\n\nIl arrive fréquemment que les hyperparamètres des algorithmes de gradient boosting portent plusieurs noms. Par exemple dans LightGBM, le nombre d’arbres porte les noms suivants: num_iterations, num_iteration, n_iter, num_tree, num_trees, num_round, num_rounds, nrounds, num_boost_round, n_estimators et max_iter (ouf!). C’est une source récurrente de confusion, mais il est facile de s’y retrouver en consultant la page de la documentation sur les hyperparamètres, qui liste les alias:\n\nhyperparamètres de LightGBM;\nhyperparamètres de XGBoost;\nhyperparamètres de CatBoost;\nhyperparamètres de scikit-learn.\n\n\n\nVoici une présentation des principaux hyperparamètres et de leurs effets sur les performances sur le modèle de gradient boosting:\n\nLa mécanique du gradient boosting est contrôlée par seulement trois hyperparamètres (tous les autres hyperparamètres portant sur la construction des arbres pris isolément):\n\nL’hyperparamètre objective définit à la fois la nature du problème modélisé (régression, classification…) et la fonction de perte utilisée lors de l’entraînement du modèle. Valeur par défaut différente selon les cas, regression_l2 en cas de régression, binary_log_loss pour la classification binaire, LIEN PARTIE AVANCE. A COMPLETER.\nle nombre d’arbres contrôle la complexité générale de l’algorithme. Le point essentiel est que, contrairement aux forêts aléatoires, la performance du gradient boosting sur les données d’entraînement croît continûment avec le nombre d’arbres sans jamais se stabiliser. Le choix du nombre d’arbres est essentiel, et doit viser un équilibre entre augmentation du pouvoir prédictif du modèle (si les arbres supplémentaires permettent au modèle de corriger les erreurs résiduelles), et lutte contre le surajustement (si les arbres supplémentaires captent uniquement les bruits statistiques et les fluctuations spécifiques des données d’entraînement). Par ailleurs, Le choix du nombre d’arbres est très lié à celui du taux d’apprentissage, et il est nécessaire de les optimiser conjointement.\nle taux d’apprentissage (learning rate) contrôle l’influence de chaque arbre sur le modèle global; il s’agit de \\(\\eta\\) dans l’équation REFERENCE PARTIE OVERFITTING. Un taux d’apprentissage faible réduit la contribution de chaque arbre, rendant l’apprentissage plus progressif; cela évite qu’un arbre donné ait une influence trop importante sur le modèle global et contribue donc à réduire le surajustement, mais cela nécessite un plus grand nombre d’arbres pour converger vers une solution optimale. Inversement, un taux d’apprentissage élevé accélère l’entraînement mais peut rendre le modèle instable (car trop sensible à un arbre donné), entraîner un surajustement et/ou aboutir à un modèle sous-optimal. La règle générale est de privilégier un taux d’apprentissage faible (entre 0.01 ou 0.3). Le choix du taux d’apprentissage est très lié à celui du nombre d’arbres: plus le taux d’apprentissage sera faible, plus le nombre d’arbres nécessaires pour converger vers une solution optimale sera élevé. Ces deux hyperparamètres doivent donc être optimisés conjointement.\n\nLa complexité des arbres: la profondeur maximale des arbres, le nombre de feuilles terminales et le nombre minimal d’observations par feuille terminale contrôlent la complexité des weak learners: une profondeur élevée, un grand nombre de feuilles et un faible nombre d’observations par feuille terminale aboutissent à des arbres complexes au pouvoir prédictif plus élevé, mais induisent un risque de surajustement. Par ailleurs, de tels arbres sont plus longs à entraîner que des arbres peu profonds avec un nombre limité de feuilles. Il est à noter que le nombre de feuilles terminales a un effet linéaire sur la complexité des arbres, tandis que la profondeur maximale a un effet exponentiel: un arbre pleinement développé de profondeur \\(k\\) comprend \\(2^k\\) feuilles terminales et \\(2^k - 1\\) splits. Augmenter la profondeur d’une unité a donc pour effet de doubler le temps d’entraînement de chaque arbre.\nLa lutte contre le surajustement: ces hyperparamètres de régularisation jouent un rôle important dans le contrôle de la complexité des weak learners et contribuent à éviter le surajustement:\n\nLes pénalisations tendent à réduire le poids \\(w_j\\) des feuilles terminales: la pénalisation quadratique réduit la valeur absolue des poids sans les annuler (il s’agit de \\(\\lambda\\) dans l’équation donnant le poids optimal d’une feuille terminale), tandis que la pénalisation absolue élevée pousse certains poids à être nuls. La pénalisation quadratique est la plus utilisée, notamment parce qu’elle permet d’amoindrir l’influence des points aberrants.\nLe gain minimal définit la quantité minimale de réduction de la perte requise pour qu’un nœud soit divisé (il s’agit du paramètre \\(\\gamma\\) dans l’équation donnant le gain potentiel d’un split); une valeur plus élevée contribue à réduire la complexité des arbres et à limiter le surajustement en empêchant l’algorithme de créer des splits dont l’apport est très faible et potentiellement dû à des variations non significatives des données d’entraînement.\n\nLes hyperparamètres d’échantillonnage:\n\nle taux d’échantillonnage des données d’entraînement et le taux d’échantillonnage des colonnes par noeud jouent exactement le même rôle que sample.fraction ou max_samples, et mtry dans la forêt aléatoire: échantillonner les données d’entraînement accélère l’entraînement, et échantillonner les colonnes au niveau de chaque noeud aboutit à des arbres plus variés. Il est à noter que l’échantillonnage des données se fait systématiquement sans remise dans les algorithmes de gradient boosting. Comme pour la forêt aléatoire, la valeur optimale du taux d’échantillonnage des colonnes par noeud dépend du nombre de variables réellement pertinentes dans les données, et une valeur plus élevée est préférable si les données comprennent un grand nombre de variables binaires issues du one-hot-encoding des variables catégorielles.\nL’échantillonnage des colonnes par arbre sert essentiellement à accélérer l’entraînement. Si les colonnes sont échantillonnées par arbre et par noeud, alors le taux d’échantillonnage final est le produit des deux taux.\n\nLes réglages relatifs au retraitement des colonnes:\n\nle nombre de bins utilisés pour discrétiser les variables continues (voir partie PREPROCESSING pour le détail): un faible de bins contribue à accélérer l’entraînement (car le nombre de splits potentiels est faible), mais peut dégrader le pouvoir prédictif si de faibles variations de la variable continue ont un impact notable sur la variable-cible. Inversement, une valeur élevée permet de conserver davantage d’information sur la distribution de la variable continue, mais peut ralentir l’entraînement.\nle nombre de modalités en-deça duquel les variables catégorielles font l’objet d’un one-hot-encoding et le nombre maximal de splits considérés dans le traitement des variables catégorielles définissent la méthode utilisée pour traiter les variables catégorielles (voir partie PREPROCESSING pour le détail).\n\nLes pondérations:\n\nla pondération des observations sert à pondérer les données d’entraînement (voir PARTIE USAGE AVANCE).\nle poids des observations de la classe positive sert à rééquilibrer les données d’entraînement lorsque la classe positive est sous-représentée. Cet hyperparamètre ne sert que pour la classification binaire. Par défaut les deux classes ont le même poids.\nle poids des observations de chaque classe sert à rééquilibrer les données d’entraînement lorsque la part des différentes classes est hétérogène. Cet hyperparamètre ne sert que pour la classification binaire multiclasse. Par défaut toutes les classes ont le même poids.\n\n\n\n\n\n\n\n\nUne différence entre LightGBM et XGBoost\n\n\n\nUne différence notable entre les versions initiales de LightGBM et XGBoost tient à la méthode de construction des arbres:\n\nLightGBM construit les arbres selon une approche par feuille (dite leaf-wise): l’arbre est construit feuille par feuille, et c’est le split avec le gain le plus élevé qui est retenu à chaque étape, et ce quelle que soit sa position dans l’arbre. L’approche leaf-wise est très efficace pour minimiser la fonction de perte, car elle privilégie les splits les plus porteurs de gain, mais elle peut aboutir à un surajustement et à des arbres complexes, déséquilibrés et très profonds. L’hyperparamètre-clé de cette approche est le nombre maximal de feuilles terminales (num_leaves).\nXGBoost construit les arbres selon une approche par niveau (dite depth-wise): l’arbre est construit niveau par niveau, en divisant tous les nœuds du même niveau avant de passer au niveau suivant. L’approche depth-wise n’est pas optimale pour minimiser la fonction de perte, car elle ne recherche pas systématiquement le split le plus performant, mais elle permet d’obtenir des arbres équilibrés et de profondeur limitée. L’hyperparamètre-clé de cette approche est la profondeur maximale des arbres (max_depth).\n\nIl se trouve que l’approche leaf-wise a été ajoutée par la suite à XGBoost, via l’hyperparamètre grow_policy qui peut prendre les valeurs depthwise (valeur par défaut) et lossguide (approche leaf-wise).\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\nProposer une procédure pour l’optimisation des hyperparamètres s’avère plus délicat pour les algorithmes de gradient boosting que pour les forêts aléatoires, car ces algorithmes comprennent un nombre beaucoup plus élevé d’hyperparamètres, et la littérature méthodologique sur leur usage pratique reste assez limitée et peu conclusive (en-dehors des nombreux tutoriels introductifs disponibles sur internet). Trois constats sont néanmoins bien établis. Premièrement, bien optimiser les hyperparamètres est essentiel pour la performance du modèle final. Deuxièmement, cette optimisation est complexe et longue, il faut donc la mener de façon rigoureuse et organisée pour ne pas perdre de temps. Troisièmement, contrairement aux forêts aléatoires, les valeurs par défaut des hyperparamètres des implémentations ne constituent pas un point de départ raisonnable (Bentéjac, Csörgő, and Martínez-Muñoz (2021)), en particulier pour les hyperparamètres de régularisation dont la valeur par défaut est souvent nulle.\n\n\n\nDéfinir des valeurs de départ raisonnables pour les hyperpararamètres. Comme il est impossible d’optimiser conjointement tous les hyperparamètres, il est nécessaire de mener cette optimisation de façon itérative, en optimisant certains hyperparamètres conditionnellement aux valeurs des autres. Il est donc essentiel de retenir des valeurs de départ raisonnables pour les hyperpararamètres qui ne sont pas optimisés en premier. Ce choix prend du temps et doit reposer sur une bonne compréhension du fonctionnement de l’algorithme et sur une connaissance approfondie des données utilisées. Voici quelques suggestions de valeurs de départ (voir notamment Bentéjac, Csörgő, and Martínez-Muñoz (2021)); il est tout à fait possible de s’en écarter lorsqu’on pense que le problème modélisé le justifie:\n\nmax_depth: entre 4 et 10;\nnum_leaves: entre 30 et 255;\nmin_split_gain: valeur strictement positive, commencer entre 0.1 et 1;\nlambda: valeur strictement positive; commencer avec une valeur entre 0.5 et 2; choisir une valeur plus élevée s’il y a des valeurs aberrantes sur \\(y\\) ou de clairs signes de surajustement;\nbagging_fraction : valeur strictement inférieure à 1, commencer entre 0.6 et 0.8;\nfeature_fraction_bynode : valeur strictement inférieure à 1, commencer entre 0.5 et 0.7; choisir une valeur plus élevée si les données comprennent un grand nombre de variables binaires issues d’un one-hot-encoding;\nmax_bin : garder la valeur par défaut; choisir éventuellement une valeur plus élevée si la la valeur par défaut ne suffit pas à refléter la distribution des variables continues;\nmax_cat_to_onehot : garder la valeur par défaut;\nmax_cat_threshold : garder la valeur par défaut.\n\nChoisir la méthode d’optimisation des hyperparamètres: validation croisée ou ensemble de validation.\n\n\n\n\nVoici une procédure simple pour optimiser les hyperparamètres d’un algorithme de gradient boosting. Elle ne garantit pas l’obtention d’un modèle optimal, mais elle est lisible et permet d’obtenir rapidement un modèle raisonnablement performant.\n\nAjuster conjointement le nombre d’arbres et le taux d’apprentissage.\nAjuster la complexité des arbres.\nAjuster les hyperparamètres de lutte contre le surajustement.\nEntraîner du modèle final: entraîner une forêt aléatoire avec les hyperparamètres optimisés déduits des étapes précédentes.\nÉvaluer du modèle final: mesurer la performance du modèle final sur un ensemble de test.\n\n\n\n\n\n\n\nParfois, une forêt aléatoire suffit…\n\n\n\nAvant de se lancer dans le gradient boosting, il peut être utile d’entraîner une forêt aléatoire selon la procédure décrite dans la section ?@sec-procedure-training-rf.__ Ce modèle servira de point de comparaison pour la suite, et permettra notamment de voir si le gradient boosting offre des gains de performances qui justifient le temps passé à l’optimisation des hyperparamètres.",
    "crumbs": [
      "Comment bien utiliser les algorithmes?",
      "Guide d'usage du _gradient boosting_"
    ]
  },
  {
    "objectID": "chapters/chapter3/3-guide_usage_GB.html#sec-implementation-gb",
    "href": "chapters/chapter3/3-guide_usage_GB.html#sec-implementation-gb",
    "title": "1 Guide d’usage du gradient boosting",
    "section": "",
    "text": "Il existe quatre implémentations du gradient boosting: XGBoost, LightGBM, CatBoost et scikit-learn. Elles sont toutes des variantes optimisées de l’algorithme de Friedman (2001) et ne diffèrent que sur des points mineurs. De multiples publications les ont comparées, à la fois en matière de pouvoir prédictif et de rapidité d’entraînement (voir notamment Bentéjac, Csörgő, and Martínez-Muñoz (2021) et Alshari, Saleh, and Odabaş (2021)). Cette littérature a abouti à trois conclusions. Premièrement, les différentes implémentations présentent des performances très proches (le classement exact variant d’une publication à l’autre). Deuxièmement, bien optimiser les hyperparamètres est nettement plus important que le choix de l’implémentation. Troisièmement, le temps d’entraînement varie beaucoup d’une implémentation à l’autre, et LightGBM est sensiblement plus rapide que les autres. Dans la mesure où l’optimisation des hyperparamètres est une étape à la fois essentielle et intense en calcul, l’efficacité computationnelle apparaît comme un critère majeur de choix de l’implémentation. C’est pourquoi le présent document décrit et recommande l’usage de LightGBM. Ceci étant, les trois autres implémentations peuvent également être utilisées, notamment si les données sont de taille limitée.\nPar ailleurs, chacune de ces implémentations propose une interface de haut niveau compatible avec scikit-learn. Il est vivement recommandé d’utiliser cette interface car elle minimise les risques d’erreur, facilite la construction de modèles reproductibles et permet d’utiliser l’ensemble des outils proposés par scikit-learn.",
    "crumbs": [
      "Comment bien utiliser les algorithmes?",
      "Guide d'usage du _gradient boosting_"
    ]
  },
  {
    "objectID": "chapters/chapter3/3-guide_usage_GB.html#sec-hyperparam-gb",
    "href": "chapters/chapter3/3-guide_usage_GB.html#sec-hyperparam-gb",
    "title": "1 Guide d’usage du gradient boosting",
    "section": "",
    "text": "Cette section décrit en détail les principaux hyperparamètres des algorithmes de gradient boosting listés dans le tableau 1. Les noms des hyperparamètres sont ceux utilisés dans LightGBM. Les hyperparamètres portent généralement le même nom dans les autres implémentations; si ce n’est pas le cas, il est facile de s’y retrouver en lisant attentivement la documentation.\n\n\n\nTable 1: Les principaux hyperparamètres de LightGBM\n\n\n\n\n\n\n\n\n\n\nHyperparamètre\nDescription\nValeur par défaut\n\n\n\n\nobjective\nFonction de perte utilisée\nVariable\n\n\nn_estimators ou num_trees\nNombre d’arbres\n100\n\n\nlearning_rate ou eta\nTaux d’apprentissage\n0.1\n\n\nmax_depth\nProfondeur maximale des arbres\n-1 (pas de limite)\n\n\nnum_leaves\nNombre de feuilles terminales des arbres\n31\n\n\nmin_child_samples\nNombre minimal d’observations qu’une feuille terminale doit contenir\n20\n\n\nmin_child_weight\nPoids minimal qu’une feuille terminale doit contenir\n0.001\n\n\nlambda ou lambda_l2\nPénalisation quadratique sur la valeur des feuilles terminales\n0\n\n\nreg_alpha ou lambda_l1\nPénalisation absolue (L1) sur la valeur des feuilles terminales\n0\n\n\nmin_split_gain\nGain minimal nécessaire pour diviser un noeud\n0\n\n\nbagging_fraction\nTaux d’échantillonnage des données d’entraînement\n1\n\n\nfeature_fraction\nTaux d’échantillonnage des colonnes par arbre\n1\n\n\nfeature_fraction_bynode\nTaux d’échantillonnage des colonnes par noeud\n1\n\n\nmax_bin\nNombre de bins utilisés pour discrétiser les variables continues\n255\n\n\nmax_cat_to_onehot\nNombre de modalités en-deça duquel LightGBM utilise le one-hot-encoding\n4\n\n\nmax_cat_threshold\nNombre maximal de splits considérés  dans le traitement des variables catégorielles\n32\n\n\nsample_weight\nPondération des observations dans les données d’entraînement\n1\n\n\nscale_pos_weight\nPoids des observations de la classe positive (classification binaire uniquement)\nAucun\n\n\nclass_weight\nPoids des observations de chaque classe (classification multiclasse uniquement)\nAucun\n\n\n\n\n\n\n\n\n\n\n\n\nAttention aux alias!\n\n\n\nIl arrive fréquemment que les hyperparamètres des algorithmes de gradient boosting portent plusieurs noms. Par exemple dans LightGBM, le nombre d’arbres porte les noms suivants: num_iterations, num_iteration, n_iter, num_tree, num_trees, num_round, num_rounds, nrounds, num_boost_round, n_estimators et max_iter (ouf!). C’est une source récurrente de confusion, mais il est facile de s’y retrouver en consultant la page de la documentation sur les hyperparamètres, qui liste les alias:\n\nhyperparamètres de LightGBM;\nhyperparamètres de XGBoost;\nhyperparamètres de CatBoost;\nhyperparamètres de scikit-learn.\n\n\n\nVoici une présentation des principaux hyperparamètres et de leurs effets sur les performances sur le modèle de gradient boosting:\n\nLa mécanique du gradient boosting est contrôlée par seulement trois hyperparamètres (tous les autres hyperparamètres portant sur la construction des arbres pris isolément):\n\nL’hyperparamètre objective définit à la fois la nature du problème modélisé (régression, classification…) et la fonction de perte utilisée lors de l’entraînement du modèle. Valeur par défaut différente selon les cas, regression_l2 en cas de régression, binary_log_loss pour la classification binaire, LIEN PARTIE AVANCE. A COMPLETER.\nle nombre d’arbres contrôle la complexité générale de l’algorithme. Le point essentiel est que, contrairement aux forêts aléatoires, la performance du gradient boosting sur les données d’entraînement croît continûment avec le nombre d’arbres sans jamais se stabiliser. Le choix du nombre d’arbres est essentiel, et doit viser un équilibre entre augmentation du pouvoir prédictif du modèle (si les arbres supplémentaires permettent au modèle de corriger les erreurs résiduelles), et lutte contre le surajustement (si les arbres supplémentaires captent uniquement les bruits statistiques et les fluctuations spécifiques des données d’entraînement). Par ailleurs, Le choix du nombre d’arbres est très lié à celui du taux d’apprentissage, et il est nécessaire de les optimiser conjointement.\nle taux d’apprentissage (learning rate) contrôle l’influence de chaque arbre sur le modèle global; il s’agit de \\(\\eta\\) dans l’équation REFERENCE PARTIE OVERFITTING. Un taux d’apprentissage faible réduit la contribution de chaque arbre, rendant l’apprentissage plus progressif; cela évite qu’un arbre donné ait une influence trop importante sur le modèle global et contribue donc à réduire le surajustement, mais cela nécessite un plus grand nombre d’arbres pour converger vers une solution optimale. Inversement, un taux d’apprentissage élevé accélère l’entraînement mais peut rendre le modèle instable (car trop sensible à un arbre donné), entraîner un surajustement et/ou aboutir à un modèle sous-optimal. La règle générale est de privilégier un taux d’apprentissage faible (entre 0.01 ou 0.3). Le choix du taux d’apprentissage est très lié à celui du nombre d’arbres: plus le taux d’apprentissage sera faible, plus le nombre d’arbres nécessaires pour converger vers une solution optimale sera élevé. Ces deux hyperparamètres doivent donc être optimisés conjointement.\n\nLa complexité des arbres: la profondeur maximale des arbres, le nombre de feuilles terminales et le nombre minimal d’observations par feuille terminale contrôlent la complexité des weak learners: une profondeur élevée, un grand nombre de feuilles et un faible nombre d’observations par feuille terminale aboutissent à des arbres complexes au pouvoir prédictif plus élevé, mais induisent un risque de surajustement. Par ailleurs, de tels arbres sont plus longs à entraîner que des arbres peu profonds avec un nombre limité de feuilles. Il est à noter que le nombre de feuilles terminales a un effet linéaire sur la complexité des arbres, tandis que la profondeur maximale a un effet exponentiel: un arbre pleinement développé de profondeur \\(k\\) comprend \\(2^k\\) feuilles terminales et \\(2^k - 1\\) splits. Augmenter la profondeur d’une unité a donc pour effet de doubler le temps d’entraînement de chaque arbre.\nLa lutte contre le surajustement: ces hyperparamètres de régularisation jouent un rôle important dans le contrôle de la complexité des weak learners et contribuent à éviter le surajustement:\n\nLes pénalisations tendent à réduire le poids \\(w_j\\) des feuilles terminales: la pénalisation quadratique réduit la valeur absolue des poids sans les annuler (il s’agit de \\(\\lambda\\) dans l’équation donnant le poids optimal d’une feuille terminale), tandis que la pénalisation absolue élevée pousse certains poids à être nuls. La pénalisation quadratique est la plus utilisée, notamment parce qu’elle permet d’amoindrir l’influence des points aberrants.\nLe gain minimal définit la quantité minimale de réduction de la perte requise pour qu’un nœud soit divisé (il s’agit du paramètre \\(\\gamma\\) dans l’équation donnant le gain potentiel d’un split); une valeur plus élevée contribue à réduire la complexité des arbres et à limiter le surajustement en empêchant l’algorithme de créer des splits dont l’apport est très faible et potentiellement dû à des variations non significatives des données d’entraînement.\n\nLes hyperparamètres d’échantillonnage:\n\nle taux d’échantillonnage des données d’entraînement et le taux d’échantillonnage des colonnes par noeud jouent exactement le même rôle que sample.fraction ou max_samples, et mtry dans la forêt aléatoire: échantillonner les données d’entraînement accélère l’entraînement, et échantillonner les colonnes au niveau de chaque noeud aboutit à des arbres plus variés. Il est à noter que l’échantillonnage des données se fait systématiquement sans remise dans les algorithmes de gradient boosting. Comme pour la forêt aléatoire, la valeur optimale du taux d’échantillonnage des colonnes par noeud dépend du nombre de variables réellement pertinentes dans les données, et une valeur plus élevée est préférable si les données comprennent un grand nombre de variables binaires issues du one-hot-encoding des variables catégorielles.\nL’échantillonnage des colonnes par arbre sert essentiellement à accélérer l’entraînement. Si les colonnes sont échantillonnées par arbre et par noeud, alors le taux d’échantillonnage final est le produit des deux taux.\n\nLes réglages relatifs au retraitement des colonnes:\n\nle nombre de bins utilisés pour discrétiser les variables continues (voir partie PREPROCESSING pour le détail): un faible de bins contribue à accélérer l’entraînement (car le nombre de splits potentiels est faible), mais peut dégrader le pouvoir prédictif si de faibles variations de la variable continue ont un impact notable sur la variable-cible. Inversement, une valeur élevée permet de conserver davantage d’information sur la distribution de la variable continue, mais peut ralentir l’entraînement.\nle nombre de modalités en-deça duquel les variables catégorielles font l’objet d’un one-hot-encoding et le nombre maximal de splits considérés dans le traitement des variables catégorielles définissent la méthode utilisée pour traiter les variables catégorielles (voir partie PREPROCESSING pour le détail).\n\nLes pondérations:\n\nla pondération des observations sert à pondérer les données d’entraînement (voir PARTIE USAGE AVANCE).\nle poids des observations de la classe positive sert à rééquilibrer les données d’entraînement lorsque la classe positive est sous-représentée. Cet hyperparamètre ne sert que pour la classification binaire. Par défaut les deux classes ont le même poids.\nle poids des observations de chaque classe sert à rééquilibrer les données d’entraînement lorsque la part des différentes classes est hétérogène. Cet hyperparamètre ne sert que pour la classification binaire multiclasse. Par défaut toutes les classes ont le même poids.\n\n\n\n\n\n\n\n\nUne différence entre LightGBM et XGBoost\n\n\n\nUne différence notable entre les versions initiales de LightGBM et XGBoost tient à la méthode de construction des arbres:\n\nLightGBM construit les arbres selon une approche par feuille (dite leaf-wise): l’arbre est construit feuille par feuille, et c’est le split avec le gain le plus élevé qui est retenu à chaque étape, et ce quelle que soit sa position dans l’arbre. L’approche leaf-wise est très efficace pour minimiser la fonction de perte, car elle privilégie les splits les plus porteurs de gain, mais elle peut aboutir à un surajustement et à des arbres complexes, déséquilibrés et très profonds. L’hyperparamètre-clé de cette approche est le nombre maximal de feuilles terminales (num_leaves).\nXGBoost construit les arbres selon une approche par niveau (dite depth-wise): l’arbre est construit niveau par niveau, en divisant tous les nœuds du même niveau avant de passer au niveau suivant. L’approche depth-wise n’est pas optimale pour minimiser la fonction de perte, car elle ne recherche pas systématiquement le split le plus performant, mais elle permet d’obtenir des arbres équilibrés et de profondeur limitée. L’hyperparamètre-clé de cette approche est la profondeur maximale des arbres (max_depth).\n\nIl se trouve que l’approche leaf-wise a été ajoutée par la suite à XGBoost, via l’hyperparamètre grow_policy qui peut prendre les valeurs depthwise (valeur par défaut) et lossguide (approche leaf-wise).\n\n\n\n\n\n\nFigure 1",
    "crumbs": [
      "Comment bien utiliser les algorithmes?",
      "Guide d'usage du _gradient boosting_"
    ]
  },
  {
    "objectID": "chapters/chapter3/3-guide_usage_GB.html#sec-procedure-training-gb",
    "href": "chapters/chapter3/3-guide_usage_GB.html#sec-procedure-training-gb",
    "title": "1 Guide d’usage du gradient boosting",
    "section": "",
    "text": "Proposer une procédure pour l’optimisation des hyperparamètres s’avère plus délicat pour les algorithmes de gradient boosting que pour les forêts aléatoires, car ces algorithmes comprennent un nombre beaucoup plus élevé d’hyperparamètres, et la littérature méthodologique sur leur usage pratique reste assez limitée et peu conclusive (en-dehors des nombreux tutoriels introductifs disponibles sur internet). Trois constats sont néanmoins bien établis. Premièrement, bien optimiser les hyperparamètres est essentiel pour la performance du modèle final. Deuxièmement, cette optimisation est complexe et longue, il faut donc la mener de façon rigoureuse et organisée pour ne pas perdre de temps. Troisièmement, contrairement aux forêts aléatoires, les valeurs par défaut des hyperparamètres des implémentations ne constituent pas un point de départ raisonnable (Bentéjac, Csörgő, and Martínez-Muñoz (2021)), en particulier pour les hyperparamètres de régularisation dont la valeur par défaut est souvent nulle.\n\n\n\nDéfinir des valeurs de départ raisonnables pour les hyperpararamètres. Comme il est impossible d’optimiser conjointement tous les hyperparamètres, il est nécessaire de mener cette optimisation de façon itérative, en optimisant certains hyperparamètres conditionnellement aux valeurs des autres. Il est donc essentiel de retenir des valeurs de départ raisonnables pour les hyperpararamètres qui ne sont pas optimisés en premier. Ce choix prend du temps et doit reposer sur une bonne compréhension du fonctionnement de l’algorithme et sur une connaissance approfondie des données utilisées. Voici quelques suggestions de valeurs de départ (voir notamment Bentéjac, Csörgő, and Martínez-Muñoz (2021)); il est tout à fait possible de s’en écarter lorsqu’on pense que le problème modélisé le justifie:\n\nmax_depth: entre 4 et 10;\nnum_leaves: entre 30 et 255;\nmin_split_gain: valeur strictement positive, commencer entre 0.1 et 1;\nlambda: valeur strictement positive; commencer avec une valeur entre 0.5 et 2; choisir une valeur plus élevée s’il y a des valeurs aberrantes sur \\(y\\) ou de clairs signes de surajustement;\nbagging_fraction : valeur strictement inférieure à 1, commencer entre 0.6 et 0.8;\nfeature_fraction_bynode : valeur strictement inférieure à 1, commencer entre 0.5 et 0.7; choisir une valeur plus élevée si les données comprennent un grand nombre de variables binaires issues d’un one-hot-encoding;\nmax_bin : garder la valeur par défaut; choisir éventuellement une valeur plus élevée si la la valeur par défaut ne suffit pas à refléter la distribution des variables continues;\nmax_cat_to_onehot : garder la valeur par défaut;\nmax_cat_threshold : garder la valeur par défaut.\n\nChoisir la méthode d’optimisation des hyperparamètres: validation croisée ou ensemble de validation.\n\n\n\n\nVoici une procédure simple pour optimiser les hyperparamètres d’un algorithme de gradient boosting. Elle ne garantit pas l’obtention d’un modèle optimal, mais elle est lisible et permet d’obtenir rapidement un modèle raisonnablement performant.\n\nAjuster conjointement le nombre d’arbres et le taux d’apprentissage.\nAjuster la complexité des arbres.\nAjuster les hyperparamètres de lutte contre le surajustement.\nEntraîner du modèle final: entraîner une forêt aléatoire avec les hyperparamètres optimisés déduits des étapes précédentes.\nÉvaluer du modèle final: mesurer la performance du modèle final sur un ensemble de test.\n\n\n\n\n\n\n\nParfois, une forêt aléatoire suffit…\n\n\n\nAvant de se lancer dans le gradient boosting, il peut être utile d’entraîner une forêt aléatoire selon la procédure décrite dans la section ?@sec-procedure-training-rf.__ Ce modèle servira de point de comparaison pour la suite, et permettra notamment de voir si le gradient boosting offre des gains de performances qui justifient le temps passé à l’optimisation des hyperparamètres.",
    "crumbs": [
      "Comment bien utiliser les algorithmes?",
      "Guide d'usage du _gradient boosting_"
    ]
  }
]