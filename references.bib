
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% Les papiers sur l'intérêt des méthodes ensemblistes sur données tabulaires
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

@article{grinsztajn2022tree,
  title={Why do tree-based models still outperform deep learning on typical tabular data?},
  author={Grinsztajn, L{\'e}o and Oyallon, Edouard and Varoquaux, Ga{\"e}l},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={507--520},
  year={2022}
}
@article{shwartz2022tabular,
  title={Tabular data: Deep learning is not all you need},
  author={Shwartz-Ziv, Ravid and Armon, Amitai},
  journal={Information Fusion},
  volume={81},
  pages={84--90},
  year={2022},
  publisher={Elsevier}
}
@article{mcelfresh2024neural,
  title={When do neural nets outperform boosted trees on tabular data?},
  author={McElfresh, Duncan and Khandagale, Sujay and Valverde, Jonathan and Prasad C, Vishak and Ramakrishnan, Ganesh and Goldblum, Micah and White, Colin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% Les papiers fondateurs des méthodes ensemblistes
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

@book{hastie2009elements,
  title={The elements of statistical learning: data mining, inference, and prediction},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H},
  volume={2},
  year={2009},
  publisher={Springer}
}
@article{fisher1958grouping,
  title={On grouping for maximum homogeneity},
  author={Fisher, Walter D},
  journal={Journal of the American statistical Association},
  volume={53},
  number={284},
  pages={789--798},
  year={1958},
  publisher={Taylor \& Francis}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% Les arbres de décision
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 


@book{quinlan2014c4,
  title={C4. 5: programs for machine learning},
  author={Quinlan, J Ross},
  year={2014},
  publisher={Elsevier}
}

@article{breiman1984cart,
  title={Cart},
  author={Breiman, Leo and Friedman, Jerome and Olshen, Richard and Stone, Charles},
  journal={Classification and regression trees},
  year={1984},
  publisher={Wadsworth and Brooks/Cole Monterey, CA, USA}
}

@article{friedman1991multivariate,
  title={Multivariate adaptive regression splines},
  author={Friedman, Jerome H},
  journal={The annals of statistics},
  volume={19},
  number={1},
  pages={1--67},
  year={1991},
  publisher={Institute of Mathematical Statistics}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% Les random forests
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

@article{breiman1996bagging,
  title={Bagging predictors},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={24},
  pages={123--140},
  year={1996},
  publisher={Springer}
}
@article{breiman2001random,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% Le boosting
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

@article{schapire1990strength,
  title={The strength of weak learnability},
  author={Schapire, Robert E},
  journal={Machine learning},
  volume={5},
  pages={197--227},
  year={1990},
  publisher={Springer}
}

@article{freund1997decision,
  title={A decision-theoretic generalization of on-line learning and an application to boosting},
  author={Freund, Yoav and Schapire, Robert E},
  journal={Journal of computer and system sciences},
  volume={55},
  number={1},
  pages={119--139},
  year={1997},
  publisher={Elsevier}
}
@article{breiman1998rejoinder,
  title={Rejoinder: arcing classifiers},
  author={Breiman, Leo},
  journal={The Annals of Statistics},
  volume={26},
  number={3},
  pages={841--849},
  year={1998},
  publisher={JSTOR}
}
@inproceedings{grove1998boosting,
  title={Boosting in the limit: Maximizing the margin of learned ensembles},
  author={Grove, Adam J and Schuurmans, Dale},
  booktitle={AAAI/IAAI},
  pages={692--699},
  year={1998}
}
@article{friedman2000additive,
  title={Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors)},
  author={Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  journal={The annals of statistics},
  volume={28},
  number={2},
  pages={337--407},
  year={2000},
  publisher={Institute of Mathematical Statistics}
}

@article{friedman2001greedy,
  title={Greedy function approximation: a gradient boosting machine},
  author={Friedman, Jerome H},
  journal={Annals of statistics},
  pages={1189--1232},
  year={2001},
  publisher={JSTOR}
}
@inproceedings{chen2016xgboost,
  title={Xgboost: A scalable tree boosting system},
  author={Chen, Tianqi and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining},
  pages={785--794},
  year={2016}
}
@article{ke2017lightgbm,
  title={Lightgbm: A highly efficient gradient boosting decision tree},
  author={Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{prokhorenkova2018catboost,
  title={CatBoost: unbiased boosting with categorical features},
  author={Prokhorenkova, Liudmila and Gusev, Gleb and Vorobev, Aleksandr and Dorogush, Anna Veronika and Gulin, Andrey},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% Les papiers sur l'usage des méthodes ensemblistes
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

@article{biau2016random,
  title={A random forest guided tour},
  author={Biau, G{\'e}rard and Scornet, Erwan},
  journal={Test},
  volume={25},
  pages={197--227},
  year={2016},
  publisher={Springer}
}
@article{probst2018tune,
  title={To tune or not to tune the number of trees in random forest},
  author={Probst, Philipp and Boulesteix, Anne-Laure},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={181},
  pages={1--18},
  year={2018}
}
@article{probst2019hyperparameters,
  title={Hyperparameters and tuning strategies for random forest},
  author={Probst, Philipp and Wright, Marvin N and Boulesteix, Anne-Laure},
  journal={Wiley Interdisciplinary Reviews: data mining and knowledge discovery},
  volume={9},
  number={3},
  pages={e1301},
  year={2019},
  publisher={Wiley Online Library}
}
@article{bentejac2021comparative,
  title={A comparative analysis of gradient boosting algorithms},
  author={Bent{\'e}jac, Candice and Cs{\"o}rg{\H{o}}, Anna and Mart{\'i}nez-Mu{\~n}oz, Gonzalo},
  journal={Artificial Intelligence Review},
  volume={54},
  pages={1937--1967},
  year={2021},
  publisher={Springer}
}
@article{alshari2021comparison,
  title={Comparison of gradient boosting decision tree algorithms for CPU performance},
  author={Alshari, Haithm and Saleh, Abdulrazak Yahya and Odaba{\c{s}}, Alper},
  journal={Journal of Institue Of Science and Technology},
  volume={37},
  number={1},
  pages={157--168},
  year={2021}
}
@article{florek2023benchmarking,
  title={Benchmarking state-of-the-art gradient boosting algorithms for classification},
  author={Florek, Piotr and Zagda{\'n}ski, Adam},
  journal={arXiv preprint arXiv:2305.17094},
  year={2023}
}
@incollection{biau2021optimization,
  title={Optimization by gradient boosting},
  author={Biau, G{\'e}rard and Cadre, Beno{\^\i}t},
  booktitle={Advances in Contemporary Statistics and Econometrics: Festschrift in Honor of Christine Thomas-Agnan},
  pages={23--44},
  year={2021},
  publisher={Springer}
}

@article{bischl2023hyperparameter,
  title={Hyperparameter optimization: Foundations, algorithms, best practices, and open challenges},
  author={Bischl, Bernd and Binder, Martin and Lang, Michel and Pielok, Tobias and Richter, Jakob and Coors, Stefan and Thomas, Janek and Ullmann, Theresa and Becker, Marc and Boulesteix, Anne-Laure and others},
  journal={Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  volume={13},
  number={2},
  pages={e1484},
  year={2023},
  publisher={Wiley Online Library}
}

@inproceedings{van2018hyperparameter,
  title={Hyperparameter importance across datasets},
  author={Van Rijn, Jan N and Hutter, Frank},
  booktitle={Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={2367--2376},
  year={2018}
}
@article{probst2019tunability,
  title={Tunability: Importance of hyperparameters of machine learning algorithms},
  author={Probst, Philipp and Boulesteix, Anne-Laure and Bischl, Bernd},
  journal={Journal of Machine Learning Research},
  volume={20},
  number={53},
  pages={1--32},
  year={2019}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% Les papiers sur l'interprétabilité des méthodes ensemblistes
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% Références générales sur l'interprétabilité
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

@book{molnar2020interpretable,
  title={Interpretable machine learning, 2nd edition},
  author={Molnar, Christoph},
  year={2022},
  publisher={Lulu.com}
}

@article{arrieta2020explainable,
  title={Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI},
  author={Arrieta, Alejandro Barredo and D{\'\i}az-Rodr{\'\i}guez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garc{\'\i}a, Salvador and Gil-L{\'o}pez, Sergio and Molina, Daniel and Benjamins, Richard and others},
  journal={Information fusion},
  volume={58},
  pages={82--115},
  year={2020},
  publisher={Elsevier}
}

@article{rudin2019stop,
  title={Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
  author={Rudin, Cynthia},
  journal={Nature machine intelligence},
  volume={1},
  number={5},
  pages={206--215},
  year={2019},
  publisher={Nature Publishing Group UK London}
}
@article{sahakyan2021explainable,
  title={Explainable artificial intelligence for tabular data: A survey},
  author={Sahakyan, Maria and Aung, Zeyar and Rahwan, Talal},
  journal={IEEE access},
  volume={9},
  pages={135392--135422},
  year={2021},
  publisher={IEEE}
}
@article{ali2023explainable,
  title={Explainable Artificial Intelligence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence},
  author={Ali, Sajid and Abuhmed, Tamer and El-Sappagh, Shaker and Muhammad, Khan and Alonso-Moral, Jose M and Confalonieri, Roberto and Guidotti, Riccardo and Del Ser, Javier and D{\'\i}az-Rodr{\'\i}guez, Natalia and Herrera, Francisco},
  journal={Information fusion},
  volume={99},
  pages={101805},
  year={2023},
  publisher={Elsevier}
}
@article{lipton2018mythos,
  title={The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery.},
  author={Lipton, Zachary C},
  journal={Queue},
  volume={16},
  number={3},
  pages={31--57},
  year={2018},
  publisher={ACM New York, NY, USA}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% Références centrées sur les méthodes ensemblistes
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

@article{louppe2014understanding,
  title={Understanding random forests: From theory to practice},
  author={Louppe, Gilles},
  journal={arXiv preprint arXiv:1407.7502},
  year={2014}
}

@article{louppe2013understanding,
  title={Understanding variable importances in forests of randomized trees},
  author={Louppe, Gilles and Wehenkel, Louis and Sutera, Antonio and Geurts, Pierre},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{biau2012analysis,
  title={Analysis of a random forests model},
  author={Biau, G{\'e}rard},
  journal={The Journal of Machine Learning Research},
  volume={13},
  number={1},
  pages={1063--1095},
  year={2012},
  publisher={JMLR. org}
}

@article{strobl2007bias,
  title={Bias in random forest variable importance measures: Illustrations, sources and a solution},
  author={Strobl, Carolin and Boulesteix, Anne-Laure and Zeileis, Achim and Hothorn, Torsten},
  journal={BMC bioinformatics},
  volume={8},
  pages={1--21},
  year={2007},
  publisher={Springer}
}

@article{benard2022mda, 
  title={Mean decrease accuracy for random forests: inconsistency, and a practical solution via the Sobol-MDA}, 
  author={Bénard, Clément and Da Veiga, Sébastien and Scornet, Erwan}, 
  journal={Biometrika}, 
  volume={109}, 
  number={4}, 
  pages={881--900}, 
  year={2022}, 
  publisher={Oxford University Press}, 
  doi={10.1093/biomet/asac017} 
}

@inproceedings{benard2022shaff,
  title={SHAFF: Fast and consistent SHApley eFfect estimates via random Forests},
  author={B{\'e}nard, Cl{\'e}ment and Biau, G{\'e}rard and Da Veiga, S{\'e}bastien and Scornet, Erwan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={5563--5582},
  year={2022},
  publisher={PMLR}
}

@article{haddouchi2024survey,
  title={A survey and taxonomy of methods interpreting random forest models},
  author={Haddouchi, Maissae and Berrado, Abdelaziz},
  journal={arXiv preprint arXiv:2407.12759},
  year={2024}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% Utilisation des GAMI pour interpréter les arbres
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

@inproceedings{lou2013accurate,
  title={Accurate intelligible models with pairwise interactions},
  author={Lou, Yin and Caruana, Rich and Gehrke, Johannes and Hooker, Giles},
  booktitle={Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={623--631},
  year={2013}
}
@article{yang2021gami,
  title={GAMI-Net: An explainable neural network based on generalized additive models with structured interactions},
  author={Yang, Zebin and Zhang, Aijun and Sudjianto, Agus},
  journal={Pattern Recognition},
  volume={120},
  pages={108192},
  year={2021},
  publisher={Elsevier}
}

@article{friedman2024function,
  title={Function Trees: Transparent Machine Learning},
  author={Friedman, Jerome H},
  journal={arXiv preprint arXiv:2403.13141},
  year={2024}
}
@article{hu2022using,
  title={Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA Models},
  author={Hu, Linwei and Chen, Jie and Nair, Vijayan N},
  journal={arXiv preprint arXiv:2207.06950},
  year={2022}
}
@article{hu2023monotonetreebasedgamimodels,
      title={Monotone Tree-Based GAMI Models by Adapting XGBoost}, 
      author={Linwei Hu and Soroush Aramideh and Jie Chen and Vijayan N. Nair},
      year={2023},
      eprint={2309.02426},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2309.02426}, 
}
